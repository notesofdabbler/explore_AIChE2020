abstract,sessionid,talkid
"For fine powders, in particular those below 30μm, interparticle cohesion is the dominating force, inducing significant agglomerate formation that delays their dissolution rate1,2. For such fine powders, dry coating with glidants, typically silica nanoparticles, is very effective in reducing cohesion and hence the extent of agglomeration3,4. Previous papers have reported on the impact of dry coating on enhancements in drug powder bulk density, flow, and dissolution5,6,7,8,9. However, those papers did not quantify the extent of agglomerate size reduction and its impact on the dissolution3,4. In addition to presumed agglomerate size reduction after dry coating, the competing relationship with the surface hydrophobicity on the drug dissolution rate was not examined. In this work, the investigation is carried out to fill these gaps and develop a more quantitative understanding of the relationship between cohesion reduction and agglomerate size reduction. The latter objective will be based on proposing a predictive model that is based on the dimensionless parameter that captures the effect of interparticle cohesion, in the form of the granular Bond number. Griseofulvin (d50 of 10 µm) and ibuprofen (d50 of 20 µm) were selected as model drugs. Each drug was dry coated with either hydrophilic (A200) or hydrophobic (R972P) fumed silica via LabRAM, while varying theoretical surface area coverage (SAC) of silica. Agglomerated and primary particle sizes were evaluated by two different particle sizers, a gravity-driven dispersion method, and a compressed air dispersion method, respectively. Following USP <711> protocol, USP IV dissolution testing was conducted in de-ionized water. Surface hydrophilicity was measured via the modified Washburn method10,11. Inverse gas chromatography was employed to measure the surface energy of the dry coated powders. The results show that the dissolution rate significantly influenced by the changed surface hydrophilicity and the reduced agglomerate size. The multi-asperity model was employed to calculate the bond number for this work12, utilizing the measured surface energy of the dry coated drug particles. The applicability of the previously presented mathematical correlation between a bond number to an agglomerate ratio13 was tested by comparing the predicted agglomerate size to the measured agglomerate size keeping in mind that it does not account for the polydispersity of the powders and process shear effect from the dry coating. Overall, the outcomes from this study will help the design of solid oral dosages through the understanding of the impacts of the surface hydrophobicity and the agglomerate size after dry coating on the dissolution of poorly water-soluble drugs.ReferencesYang, J.; Sliva, A.; Banerjee, A.; Dave, R. N.; Pfeffer, R., Dry particle coating for improving the flowability of cohesive powders. Powder Technology 2005, 158 (1-3), 21-33.de Villiers, M. M., Influence of agglomeration of cohesive particles on the dissolution behaviour of furosemide powder. International Journal of Pharmaceutics 1996, 136 (1-2), 175-179.Han, X.; Ghoroi, C.; To, D.; Chen, Y.; Davé, R., Simultaneous micronization and surface modification for improvement of flow and dissolution of drug particles. International Journal of Pharmaceutics 2011, 415 (1-2), 185-195.Han, X.; Ghoroi, C.; Davé, R., Dry coating of micronized API powders for improved dissolution of directly compacted tablets with high drug loading. International Journal of Pharmaceutics 2013, 442 (1-2), 74-85.Jallo, L. J.; Ghoroi, C.; Gurumurthy, L.; Patel, U.; Davé, R. N., Improvement of flow and bulk density of pharmaceutical powders using surface modification. International Journal of Pharmaceutics 2012, 423 (2), 213-225.Huang, Z.; Xiong, W.; Kunnath, K.; Bhaumik, S.; Davé, R. N., Improving blend content uniformity via dry particle coating of micronized drug powders. European Journal of Pharmaceutical Sciences 2017, 104, 344-355.Capece, M.; Huang, Z.; To, D.; Aloia, M.; Muchira, C.; Davé, R. N.; Yu, A. B., Prediction of porosity from particle scale interactions: Surface modification of fine cohesive powders. Powder Technology 2014, 254, 103-113.Chen, L.; Ding, X.; He, Z.; Huang, Z.; Kunnath, K. T.; Zheng, K.; Davé, R. N., Surface engineered excipients: I. improved functional properties of fine grade microcrystalline cellulose. International Journal of Pharmaceutics 2018, 536 (1), 127-137.Kunnath, K.; Huang, Z.; Chen, L.; Zheng, K.; Davé, R., Improved properties of fine active pharmaceutical ingredient powder blends and tablets at high drug loading via dry particle coating. International Journal of Pharmaceutics 2018, 543 (1-2), 288-299.Thakker, M.; Karde, V.; Shah, D. O.; Shukla, P.; Ghoroi, C., Wettability measurement apparatus for porous material using the modified Washburn method. Measurement Science and Technology 2013, 24 (12).Washburn, E. W., The dynamics of capillary flow. Physical Review 1921, 17 (3), 273-283.Chen, Y.; Yang, J.; Dave, R. N.; Pfeffer, R., Fluidization of coated group C powders. AIChE Journal 2008, 54 (1), 104-121.Castellanos, A., The relationship between attractive interparticle forces and bulk behaviour in dry and uncharged fine powders. Advances in Physics 2005, 54 (4), 263-376.",0,0.0
"Direct compaction is the simplest and most preferred process route in the manufacturing of tablets. Unfortunately, direct compaction cannot often be developed without severely restricting drug loading due to the poor flow and compaction properties typical of the active pharmaceutical ingredient (API). Dry-coating processes, which coat API or excipients with glidant (nano-sized silicon dioxide), have been shown effective to improve the manufacturability of formulations containing APIs with poor flowability and tabletability. Accordingly, dry-coating has been considered an enabling technology for direct compaction. Despite the advantages of dry-coating at the lab-scale using a wide variety of equipment such as powder blenders/mixers and mills, manufacturing-scale processes have yet to gain significant acceptance. This may be due to the poor scalability of dry-coating processes or presumably due to the aversion of risk associated with adopting new technology in the pharmaceutical industry.One particularly attractive dry-coating device which has high relevance to the pharmaceutical industry is the conical screen mill (comill). The comill is a continuous process which is frequently employed in the production of tablets as a screening/de-lumping or particle size reduction process. Several lab-scale studies have shown that the comill is capable of dry-coating, but typically not as effective or efficient as batch processing equipment due to the very short residence time.As a major novelty, this study makes a simple and easy modification to the comill screen in order to improve the effectiveness and efficiency of the comill used as a dry-coating device. The effect of screen properties and processing conditions are evaluated for a model cohesive material, Avicel PH 105. It is shown that the flowability of Avicel PH 105 can be improved equally as well as a benchmark batch dry-coating device known as the LabRAM (Resodyn Acoustic Mixer). The comill process with the modified screen is applied to a micronized grade of acetaminophen as well as several in-house APIs. The API’s are formulated into 50% drug loading blends which are all shown to have adequate flowability and tabletability. The industry’s familiarity with the comill combined with its improved effectiveness may allow it to be adopted as a dry-coating process in order to enable direct compaction for high drug loading formulations.   AbbVie funded and participated in the study design, research, data collection, analysis and interpretation of data, as well as writing, reviewing, and approving the publication. Maxx Capece, is an AbbVie employees and may own AbbVie stock/options. Arun Jayaraman and Christian Borchardt are former employee of AbbVie and have no conflicts of interest to report.",0,1.0
"Budesonide powder is an important pharmaceutical ingredient, used to treat asthma or chronic obstructive pulmonary disease by respiratory inhalation. As an important physical property of Budesonide, wettability has a significant influence on dissolution rates and drug release characteristics. Good flowability of Budesonide powders is vital in the re-suspending of powder in inhalers. However, micron-sized Budesonide powders generally exhibit poor flowability as the inter-particle cohesive forces play a dominant role in the bulk powder behavior when the size of individual particles is less than 30 microns.Coating of powder material can be a solution to modify surface properties of individual particles, so as to improve wettability and flowability. Therefore, we experimentally coated the Budesonide powders (size range of 0.1 to 10 µm) with SiO2, TiO2, Al2O3 and PET (Polyethylene terephthalate) in a vibrated fluidized bed reactor. Nano-sized films of SiO2, TiO2, and Al2O3 was coated on the surface of Budesonide particles via atomic layer deposition (ALD, inorganic coatings). Nano-scaled layers of PET was deposited onto Budesonide via molecular layer deposition (MLD, organic coatings). The effects of different coating materials, coating cycles (thickness) and coating process (ALD and MLD) on the two important properties of Budesonide powders, wettability and flowability were investigated.Uncoated Budesonide powder is classified as a very hydrophobic material with poor flowability. Improvements both in the wettability and flowability of Budesonide after the ALD coating process are observed. The coated Budesonide becomes less hydrophobic and its flow property improves; the degree of improvement depends on the coating material. The positive effect can be further enhanced by increasing the number of coating cycles. In particular, Budesonide coated with TiO2 shows the largest increase in wettability (smallest water contact angle). In contrast to ALD coated Budesonide, PET coated budesonide via MLD becomes more hydrophobic compared to the uncoated Budesonide.",0,2.0
"INTRODUCTIONPowder tribo-charging occurs due to the charge exchange between two solid surfaces that are brought into contact and then separated. During powder manufacturing, particles can acquire electrostatic charges from their frequent collisions within the powder and the surfaces of the processing equipment. The attained electrostatic charges can alter powder behaviours and affect its processability, i.e., reducing flow, inducing particle agglomeration and/or segregation, causing adhesion to surfaces, etc. Besides the type of processing operation (e.g., sieving, blending, conveying, etc.) and environmental conditions (i.e., temperature and relative humidity) used during production, the charging propensity of powders also depends on their multiple material attributes1, particularly, their solid-state characteristics (e.g., crystallinity), presence of impurities, etc. For instance, crystalline salbutamol sulphate (SS) has been reported to charge distinctly, compared to its amorphous counterpart2. The presence of surface and bulk impurities can also contribute to powder tribo-electrification3. In this context, the present work aims at evaluating the impact of solid-state, including different amorphous contents and impurity contents, on the tribo-charging tendency of SS powders.MATERIALS AND METHODSMaterialsSalbutamol sulphate (SS) was purchased from Fagron GmbH Co.KG (Barsbüttel, Germany) and engineered to obtain particles with different solid-state properties.MethodsPowder engineering via ball-millingParticle engineering was performed via ball-milling on a PM 100 planetary mill (Retsch, Germany), equipped with a 250 ml agate jar. Milling was performed at 400 rpm for different milling times (i.e., 10, 20, 40, 120 min) at room temperature.Solid-state characterization of the salbutamol sulphate powdersThe crystallinity of SS particles was analysed via wide angle X-ray scattering (WAXS) and attenuated total reflectance-Fourier transformed infrared (ATR-FTIR) spectroscopy. Information on the structural heterogeneity of the samples at nano-scale was obtained by the analysis of the small angle X-ray scattering (SAXS) signal. Karl Fischer (KF) titration was performed to determine the water content of the samples, whereas particle size and specific surface area were characterized by laser diffraction and gas adsorption, respectively. The impurities arising from the milling process were quantified via high performance liquid chromatography (HPLC). Results of the powder characterization of the engineered samples were compared to the starting material (i.e., crystalline powder) and to spray-dried SS (i.e., amorphous powder). For spray-drying process, the same parameters described elsewhere were used4.Charge evaluationThe electrostatic charge density of the milled powders and raw material was measured using the GranuCharge™ (GranuTools, Belgium) apparatus at 22 ± 2 °C and 38 ± 3% RH. Approx. 2 g of powder were used for the analysis by measuring the powder charge before and after flow in contact with the stainless-steel tubes of the GranuCharge™ device and the charge density of the samples expressed as difference between the two charge values. Charge density data was normalized by the specific surface area of the powders to allow a proper comparison between the samples.Statistical analysisPrincipal component analysis (PCA) was selected as multivariate data analysis method with the primary aim of identifying material attributes contributing to powder tribo-electrification. Analysis was performed using Simca 16.0 software (Umetrics, Sweden).RESULTS AND DISCUSSIONThe room temperature ball-milling process induced a progressive amorphization of the SS powders with increasing milling time as confirmed from the changes noticed in both FTIR and WAXS patterns. A completely amorphous sample was obtained after 120 min of milling, as shown by the absence of Bragg peaks in X-ray pattern of the material. Similarly, a broad halo was found for the spray-dried SS, further confirming the complete amorphicity of the 120 min sample. In addition to powder amorphization, the milling energy applied on the material also led to the drug degradation in solid-state and resulted in the appearance of impurities, particularly at longer milling times. All the engineered powders showed similar particle size distribution (i.e., x50~7.2 µm; span~3.7), whereas a narrower distribution was obtained for the starting material (i.e., x50=8.3 µm; span=2.7) and the spray-dried sample (i.e., x50=7.0 µm; span=2.2). Since tribo-charging is particularly sensitive to the presence of water, the water content of the samples was quantified and found to be in the range between 3.6-4.7 wt% for all the produced samples. These amounts were considerably higher, when compared to the starting material (i.e., 0.2 wt%) and the spray-dried sample (i.e., 1.7 wt%). Regarding charge evaluation, all the milled powders and the starting material were found to charge positively before and after contact with the stainless-steel surface of the GranuCharge tubes. However, distinct charging trends were obtained for the different milling times. Charge density was found to initially decrease with milling time, reach a minimum at 20 min and thereafter, progressively increase up to 120 min (i.e., fully amorphous sample). The tribo-charging behaviour of fully crystalline and amorphous SS has also been explored in earlier studies2,5, where, likewise to our current results, amorphous SS was found to charge to a higher extent (compared to its crystalline form). However, the effect of different degree of amorphicity and impurities on the SS tribo-electrification have not been investigated before. Both factors could potentially impact tribo-charging by inducing variations to surface chemical composition of the materials, thus altering their effective work function (i.e., driving force of the tribo-charging phenomenon)2,3,5,6. Statistical analysis of the results presented in this study revealed underlying correlations between the various material attributes (Figure 1). From the arrangement of the powder properties in the loading plot (i.e., plot displaying the variables in the principal components planes), the main source of variability among the samples was attributed to the particle size of the materials and their amorphous and water content. Those variables were in fact located along the first principal component (PC), explaining 53.3% of variability in the data, whereas the surface charge density (Δq), impurities and nano-heterogeneity were contributing to the second principal component of the PCA model. Surprisingly, powder tribo-charging was found to be not only correlated to solid-state properties, such as the amorphous content and impurity content, but a negative correlation was also revealed in respect to the structural heterogeneity of the samples at the nano-scale level. This observation suggests that the presence of heterogeneous domains at the nano-scale might disrupt charge transfer within the material, resulting in charge mitigation.CONCLUSIONThe present study showed that changes in solid-state properties and impurity level of SS powders can alter their tribo-charging behaviour. Statistical analysis revealed a correlation between powder tribo-electrification and the amorphous and impurity contents. Also, a relationship between the structural heterogeneity of the particles at the nano-scale and tribo-charging was found. This suggests an inter-dependence between amorphous and impurity contents and the nano-heterogeneity of materials and its consequent effect on tribo-charging.REFERENCESKaialy, K. On the effects of blending, physicochemical properties, and their interactions on the performance of carrier-based dry powders for inhalation, Adv. Colloid Interface Sci. 235, 70-89 (2016).Wong, J. Effect of crystallinity on electrostatic charging in dry powder inhaler formulations, Pharm Res. 31, 1656-1664 (2014).Mukherjee, R. Effects of particle size on the triboelectrification phenomenon in pharmaceutical excipients: experiments and multi-scale modelling. Asian J. Pharm. Sci. 11, 603-617 (2016).Littringer E. M. Spray Drying of Aqueous Salbutamol Sulfate Solutions Using the Nano Spray Dryer B-90—The Impact of Process Parameters on Particle Size. Dry. Technol. 31, 1346–1353 (2013).Zellnitz S. Tribo-Charging Behaviour of Inhalable Mannitol Blends with Salbutamol Sulphate Pharm Res. 36, 80 (2019).Mirkowska, M. Principal Factors of Contact Charging of Minerals for a Successful Triboelectrostatic Separation Process – a Review. Berg Huettenmaenn Monatsh 161, 359–382 (2016).",0,3.0
"The surface energy of pharmaceutical powders can affect their processing behavior including flowability and miscibility with other powders. Traditionally, surface energies are measured by classical contact angle measurements. However, the surface roughness, the presence of pores, and surface energy gradients of some materials make the contact angle measurements less appropriate for surface energetics determinations [1]. Surface energies has also been reported as the property of pharmaceutical powders that has the most impact on aerosol formation and powder aerosol performance, in inhalation formulations. Therefore, the measurement of these physicochemical properties is considered critical to describe and predict the aerodynamic performance of inhalation products [2].An alternative technique that can be used for surface energy measurements is inverse gas chromatography (IGC). This technique has been previously applied to the measurement of important physicochemical properties of pharmaceutical materials such as powder surface energies, acid/base/polar functionality of surfaces, diffusion kinetics, solubility parameters and surface heterogeneity. In IGC the roles of the stationary (solid) and mobile (gas or vapor) phases are inverted with respect to traditional analytical gas chromatography (GC). This work aims at describing the development and application of a home-made inverse gas chromatograph for the determination of surface properties in different pharmaceutical powders at infinite dilution regimen. In this study, IGC was used to explore different properties of formulations used in dry powder inhalers (DPI) and oral dosage formulations and the results obtained were compared to those obtained using next generation impactor (NGI) and contact angle measurements.Two different case studies where devised to demonstrate the applicability of IGC as a tool for the characterization of pharmaceutical powders during the development stages of formulation design and development. This study was conducted not only to demonstrate the suitability of IGC for surface energetics measurement but also to benchmark the results obtained using the home-made equipment and the results obtained by the well-established classical methodology. For the first case study DPI formulations were used and a correlation between IGC results and emitted dose (ED) and fine particle fraction (FPF) obtained by NGI was evaluated. Lactose carrier-based formulations were prepared with different active pharmaceutical ingredient (API) content (2% 3.5% and 5%) and different proportions of fine lactose (5%, 7.5% and 10%). The aerodynamic performance of each formulation was determined using NGI performed in duplicate for each formulation. The dispersive (apolar) component of the surface energy, as well as the specific (polar) component were measured for each formulation in triplicate. By calculating the specific work of adhesion (WaS) of tetrahydrofuran (slightly basic) and chloroform (slightly acid) to the surface particles it was possible to chemically characterize the surface of the particles for all formulations, regarding their acid/base properties. The ratio between the works of adhesion calculated is a good indicator of the chemical behavior of the surface. The results obtained for the ratio of the work of adhesion varied between 1.63 and 1.71 for all formulations tested, demonstrating that the surface chemistry does not change significantly with changes in the formulation variables. This observation is consistent to the fact that the API content is always much lesser than the Lactose content. The average surface chemistry is not significantly changed by varying the API content between 5% (w/w) and 10%(w/w).The results obtained for the dispersive component of the surface free energy (gd) did not correlate well with the % Emitted Dose (ED) (r2=0.199) neither with the Fine Particle Fraction (FPF) (r2=0.548) measured by NGI. Only when the contribution of the specific interactions (Acid/Base) was accounted for, the determination of the surface interaction energy (SIE) correlation with FPF was observed with good agreement (r2=0.808; Figure 1 a) confirming that the polar contributions of the surface free energy measurements of particles play a crucial role in understanding and determine particle-particle interactions as previously described [3]. It was also observed that the increase in the % of fine lactose present in the formulation increases the SIE (fic. 1 c), which is in agreement with previous observations [2] and counter intuitive to observations made for other materials since higher SIE are typically associated to more particle to particle interaction. However, it is proposed that for DPI formulations it is required some additional energy to ensure that micronized API which is very cohesive to be easily deagglomerated to originate better aerodynamic performance. The fine lactose increases the total surface area of the powder and competes with the API fort the high energy spots present in the coarse lactose therefore the increase in FPF observed when fine lactose is added to the formulation.In addition to the IGC experiments, the traditional liquid wetting angle approach, for surface energy determination was also applied to surface characterization of four excipients commonly used in oral dosage forms, namely two diluents (lactose monohydrate, LM, and microcrystalline cellulose, MCC), one glidant (silicon dioxide, SD) and one lubricant (magnesium stearate, MgSt). Throughout these wetting angle studies, the surface energy, work of cohesion, work of adhesion and spreading coefficient between different excipients were determined.Results showed that both SD and MgSt present a lower work of cohesion between their particles (28 mJ/m2 for MgSt and 31 mJ/m2 for SD) compared to the diluents (120 mJ/m2 for LM and 105 mJ/m2 for MCC). Since MgSt and SD present lower work of cohesion than adhesion to the different excipients (45 mJ/m2 for LC-MgSt, 43 mJ/m2 for LC-SD; 45 mJ/m2 MCC-MgSt and 37 mJ/m2 MCC-SD) they are likely to adhere and spread over both diluents surfaces. Raman imaging confirmed the surface coverage of diluents by both MgSt and SD when physical mixtures between the diluents and glidant/lubricant were prepared. As a result of the ability of MgSt or SD to spread over diluent particles during blending, these additives showed to be able to reduce the angle of repose, cohesion, compressibility and specific energy of the individual diluents.IGC measurements were conducted in the same four excipients to determine the surface energy as well as the work of adhesion and cohesion and correlations between the IGC and liquid wetting angle results were explored, allowing to understand the benefits of each technique and major inconvenient. The work conducted demonstrated that IGC can be used as an alternative methodology to support both oral dosage forms and DPI formulation development. The proposed methodology was successfully used to estimate the aerodynamic performance of a DPI formulation as demonstrated by the good correlation between NGI data and surface energy obtained using IGC. Preliminary work with excipients used in oral dose formulations also allowed identifying that the IGC data can be translated to this field of science, allowing insights on blending efficiency, predicting the impact of using excipients in formulations regarding final blend flowability and performance.References: [1] B. Riedl and P. D. Kamdem, “Estimation of the dispersive component of surface energy of polymer-grafted lignocellulosic fibers with inverse gas chromatography,” J. Adhes. Sci. Technol., vol. 6, no. 9, pp. 1053–1067, 1992.[2] D. Cline and R. Dalby, “Predicting the quality of powders for inhalation from surface energy and area,” Pharm. Res., vol. 19, no. 9, pp. 1274–1277, 2002.[3] D. Traini, P. Rogueda, P. Young, and R. Price, “Surface energy and interparticle forces correlations in model pMDI formulations,” Pharm. Res., vol. 22, no. 5, pp. 816–825, 2005.",0,4.0
"Spherical agglomeration is a size enlargement technique which offers many advantages such as ease of active pharmaceutical ingredient (API) handling and improved tabletability. Consequently, this reduces the need for further downstream processing during the manufacture of pharmaceuticals. In the spherical agglomeration process, an immiscible bridging liquid is added to agglomerate a suspension of primary particles. There are many process and formulation parameters that influence the final agglomerate properties, and the bridging liquid to solid ratio (BSR) is considered to be highly important. Although there has been a considerable amount of research regarding spherical agglomeration, there are a lack of studies that focus specifically on mechanistic understanding and the rate processes involved [1].In this work, experiments have been designed to study the spherical agglomeration mechanism via immersion nucleation, i.e. where the bridging liquid droplets are larger than the primary particles to be agglomerated. Here, particles cover the droplets and subsequently penetrate the droplets forming agglomerate nuclei. A model system using paracetamol crystals suspended in heptane with water as the bridging liquid is used in this study.A range of bridging liquid to solid ratios was investigated by adding the appropriate amount of bridging liquid directly to a stirred suspension of paracetamol crystals in heptane, at different solid loadings using a novel methodology. The product was characterized using image analysis (optical microscopy and digital photography). To establish the kinetics of the process, residence time studies were conducted. Samples were taken from the reactor at different time intervals and analysed. Agglomerate size and shape were determined via image analysis. Agglomerate density was measured using a combination of mass measurement and image analysis. The extent of agglomeration as a function of time was determined using a wet sieving technique. In certain experiments, the bridging liquid was dyed with acid red. This allowed visualization of the droplets, and confirmed that the spherical agglomeration process was via the immersion nucleation mechanism.An optimum BSR range (0.7-0.8) has been identified where 100% agglomeration occurs. Here, stable, dense and highly spherical agglomerates with excellent flow properties are formed. This optimum BSR is independent of solid loading. Below the optimum BSR, a mixture of primary particles and agglomerates is produced. Above the optimum BSR, 100% agglomeration occurs, followed by formation of a paste; the timescales of which are dependent on both the BSR and the solid loading, as are the properties of the agglomerates prior to paste formation.Changes in the extent of agglomeration and agglomerate properties (density, size) as a function of residence time also give insights into the mechanisms of spherical agglomeration via immersion nucleation. These kinetic studies will be used to validate a recently developed immersion nucleation mathematical model [2].[1] K. Pitt et al. Powder Technol. 326 (2018) 327-343[2] O. Arjmandi-Tash et al. Chem. Eng. Sci. X 4 (2019) 100048",0,5.0
"Purpose Punch sticking is a common problem in tablet manufacturing that leads to poor tablet quality, such as tablet weight variation. This work was aimed at overcoming punch sticking problems using spherical crystallization achieved through quasi-emulsion solvent diffusion (QESD). The hypothesis is that a suitable polymer containing both hydrophilic and hydrophobic moieties facilitates the formation of emulsions because of its simultaneous interactions with water and celecoxib (CEL). Such a polymer is expected to remain largely at the water-drug interface, which results in CEL spherulites coated with polymer. Both the size enlargement and surface coating reduce punch sticking propensity by reducing direct contact between CEL and punch during compaction. MethodsCelecoxib, a COX-2 selective nonsteroidal anti-inflammatory drug for treating pain and inflammation associated with osteoarthritis or rheumatoid arthritis, was selected as a model drug because it exhibits high punch sticking propensity. HPMC was used as an emulsion stabilizer. To prepare CEL spherical crystals, a CEL solution in ethyl acetate was added dropwise into HPMC aqueous solutions at different concentrations (0.1 %, 0.3 %, 0.5 %, w/w) while mixed by an overhead stirrer at 600 rpm. The growth process of CEL spherulites was monitored by observing samples under an optical microscope at 0.5, 1, 3, 6, 10, and 15 min. The appropriate concentration of HPMC sufficient to eliminate punch sticking was determined based on the cleanness of punch surface after compressing CEL spherulites at 50 MPa. The phase purity of the CEL spherulite powders was verified by DSC and PXRD. The particle size and shape were analyzed by laser diffraction and microscopy, respectively. Sticking kinetics of direct compression formulations, consisting of 20% of CEL 79.5% Avicel PH102 and 0.5% magnesium stearate, were assessed using a punch with a removable tip on a compaction simulator. The amount of material transferred to punch was determined gravimetrically every 10 compressions up to 50 compactions. Powder flow properties were characterized using a ring shear tester and powder tabletability was determined on a Zwick universal material testing machine. HPMC content was measured via size-exclusion chromatography (SEC). Surface coating of HPMC on CEL spherulites was studied by scanning electronic microscopy (SEM) and X-ray photoelectron spectroscopy (XPS). A solution 1D 1H NMR was carried out to understand the mechanism of the QESD process. ResultsSpherical CEL was successfully prepared through a QESD process. No detectable punch sticking was observed when the HPMC concentration was 0.5 % (w/w), which was selected for further study. The CEL crystals were spherulites when HPMC was used but irregular in shape without using HPMC. Upon adding the CEL ethyl acetate solution into the HPMC aqueous solution, emulsions form with size of drug rich droplets gradually grew and eventually solidified. The PXRD, DSC and TGA of the spherical CEL matched well with the CEL as-received, suggesting the phase purity and negligible residual solvents. The amount of HPMC in the CEL QESD powder was determined 1.1 ± 0.1 % (w/w). Both tabletability and flowability of the CEL spherical crystals were much better than the as-received CEL. The spherical CEL crystals exhibited significantly lower punch sticking propensity compared to that of as-received CEL. SEM images showed that the spherulites were covered by materials with irregular morphology distinct from that of the CEL crystals. The significantly reduced signals of fluorine and nitrogen atoms on the top surface of a compressed tablet than that of a CEL tablet suggested that the spherulites were coated by HPMC. Addition of HPMC into a 3 mg/mL CEL DMSO solution shifted the peaks within 7.50 ~ 7.60 ppm, 7.16 ~ 7.24 ppm and 2.28 ~ 2.36 ppm, to the upfield, corresponding to the H atoms on aromatic ring and methyl group of a CEL molecule. ConclusionThis was the first work that reported polymer assisted QESD process to reduce the punch sticking propensity, attributed to the polymer on the surface of CEL particles, higher bonding strength and enlarged particle size of CEL QESD. Polymer coating was confirmed through: (1) surface observation by SEM, where the rough and irregular surface of QESD instead of sharp and smooth surface of as-received was observed, and (2) elemental composition analysis by XPS, where the peak intensity of feature elements F and N of CEL significant reduced in QESD compared to as-received. Mechanism investigation on polymer coating on SA surface through solution NMR suggested that the strong interactions between the hydrophobic groups on CEL and HPMC played a crucial role on the surface coverage of SA by HPMC but not HPC and PVP, whose intermolecular interactions with CEL were much weaker due to the lack of hydrophobic functional groups. Collectively, QESD is a powerful API crystal engineering approach to enable the successful development of direct compression tablets of sticky drugs.",0,6.0
"The renewable production of hydrogen is an important technology. Current approaches to catalyst discovery for thermo/electro and photochemical methods for hydrogen production have been difficult to optimize. This difficulty arises in part because of the separation of the synthesis and characterization of catalysts from their performance under operating conditions. It is challenging to build models that relate catalyst performance to synthesis conditions. We have taken a different approach to this problem. We use a photo-driven process that couples a photoabsorber with the synthesis of multi-component metal particles and hydrogen production. This approach is readily parallelized in a multi-well plate, enabling us to explore a wide range of synthesis conditions that are directly connected to catalyst performance measurements. We will show how we used this approach to screen thousands of possible catalyst synthesis conditions to identify promising combinations. The high-throughput experiments along, however, do not tell us what metals to test. For this we augment the approach with a parallel high-throughput computation approach where hundreds of thousands of DFT calculations are performed on multi-component alloy surfaces to screen for promising candidates with desired hydrogen and CO adsorption energies. Finally, we combine these efforts to drive focused studies of the most promising candidate synthesis conditions to develop a more conventional understanding of how the synthesis conditions led to the observed activity. We will show how this approach has led to confirmation of some known results from the electrochemical production of hydrogen from water, as well as some new results we may not have discovered with any single approach.",1,0.0
"Non-oxidative propane dehydrogenation (PDH) on intermetallic alloys is a promising technology to convert large amounts of propane available in the shale gas to propylene. However, the high temperatures required for the reaction lead to reduced selectivity towards propylene formation and deactivation of the catalyst surface.[1] Even though Pt and Pd alloys have shown promise in terms of improved selectivity in comparison to pure metals, they ultimately experience loss of reactivity under reaction conditions. This difficulty motivates development of a more fundamental understanding of why Pt and Pd alloys have improved performance compared to pure metals and, further, elucidation of how such insights can be leveraged to find other alloy combinations with simultaneously high selectivity, stability, and activity for PDH.Through our previous analysis on PdIn alloys, we have demonstrated that, as compared to terraces, the step surfaces of 1:1 alloys are highly active under PDH conditions (5 orders of magnitude higher rates). Furthermore, a comprehensive microkinetic analysis pointed to useful descriptors for activity, selectivity and stability.[2] With these descriptors as a starting point, the current study focuses on high-throughput screening of intermetallic alloy structures, including both steps and terraces, to identify reactivity trends across Pt and Pd alloys for PDH and, ultimately, find improved catalysts for this chemistry. First, we deploy a simple automated framework for adsorption and activation energy calculations on large number of chosen terrace and step alloy structures. The Materials Project is used to identify the most stable bulk structure of the alloy composition, while a surface energy analysis points to the most stable surface terminations, and CatKit is employed to identify all the distinct adsorption sites [3]. Subsequently, an in-house algorithm identifies all unique adsorbate configurations and finds the most stable adsorption site. These data are organized with a python-based databasing tool, leading to identification of BEP-type correlations between bond breaking barriers and the binding energies of various adsorbates. Further the binding energies of key descriptors were correlated with electronic and geometric properties of the alloy surfaces, using Random Forest techniques. This, therefore, allowed for a deeper understanding of the surface features that impacts the binding of descriptors the most. References: [1] J.J.H.B. Sattler, J. Ruiz-Martinez, E. Santillan-Jimenez, B.M. Weckhuysen, Catalytic dehydrogenation of light alkanes on metals and metal oxides, Chem. Rev. 114 (2014) 10613–10653. https://doi.org/10.1021/cr5002436.[2] R.R. Seemakurthi, et al. Structure Sensitivity and Microkinetic Analysis of Propane Dehydrogenation on PdIn alloy and Pd, In Preparation.[3] J.R. Boes, O. Mamun, K. Winther, T. Bligaard, Graph Theory Approach to High-Throughput Surface Adsorption Structure Generation, J. Phys. Chem. A. 123 (2019) 2281–2285. https://doi.org/10.1021/acs.jpca.9b00311.",1,1.0
"Chemisorption, the direct chemical bonding of an atomic or molecular species to a solid-state material, is central in the fields of catalysis, corrosion, and electrochemistry, among many others. Despite the fundamental nature and importance of chemisorption, linking the geometry (i.e., the structure and composition) of different materials to their chemisorption properties remains a critical challenge. Developing physically transparent and quantitatively accurate models that can relate the chemisorption strength between an adsorbate and a solid surface to the adsorption site’s geometry is critical to advance our understanding of chemisorption. In this talk, we discuss our efforts to use a theory-guided machine learning approach, which uses an interpretable class of machine learning models called generalized additive models (iGAM models),1 to discover predictive structure-property models that can quantify the chemisorption strength of O, OH, S, and Cl on Pt-metal and Au-metal alloy surfaces subject to various strain- and ligand-induced changes in the local geometric structure. The iGAM models show a strong degree of predictive accuracy, with an average root-mean-square-error (RMSE) of 0.046 eV for samples in the test set. Through quantification of the relative importance of the features used to construct the models, we identified three important geometric features of the adsorption site that impact the relative chemisorption strength on metal alloys: the strain in the surface layer, the number of d-electrons in the ligand metal, and the size of the ligand atom. The interpretable functional form of the iGAM models allows us to analyze the model behavior with respect to each of these geometric features. Comparison between the iGAM chemisorption models and established electronic-structure models shed light on the critical physical concepts that control the chemisorption process on metal surfaces.References(1) Lou, Y.; Caruana, R.; Gehrke, J. Intelligible Models for Classification and Regression. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; KDD ’12; ACM: New York, NY, USA, 2012; pp 150–158. https://doi.org/10.1145/2339530.2339556.",1,2.0
"The catalytic turnovers of a typical reaction pathway are determined by the energies of between 10 to 100 reaction intermediates.[1] Discovering optimally performing catalysts entails maximizing catalytic turnovers across vast spaces of energy and materials. One way to simplify this formidable optimization problem is to parametrize the reaction mechanism using energy correlations.[2] These energy correlations, often linear functions, are mappings between binding energies of reaction intermediates (e.g. CxHyOz*) and one to two catalytic descriptors (e.g. binding energies of CH* and O*). Such energy correlations have enabled the high-throughput screening of metals, oxides, and zeolites. The slopes of these linear correlations (e.g. 0.5 for correlations between OH* and O*) across metals, oxides, and zeolites are determined by the ratios of bond orders of the corresponding species. Recent advances notwithstanding, these correlations are limited by three key features. First, since these correlations do not include catalyst stability metrics, they are unable to describe catalyst dynamics under reaction conditions. Second, within the existing formalism it is difficult to reverse-engineer active sites through property (e.g. rate) → structure (e.g. chemical composition) “inverse design” approaches. Third, their linearity, together with rigid constraints on the slope, imposes severe limitations in the accessible regions of materials space. There has been a concerted effort in the computational catalysis community to bridge these three knowledge gaps using data-driven statistical learning models.[3]-[5] Herein, we illustrate our contributions towards reverse-engineering realistic active ensembles sites that possess targeted catalytic turnovers.[6]-[9] We place a special emphasis on extracting physical insights from regressed parameters within our simple data-driven approach.[6], [10]In the first part, we present accelerated schemes for determining the energies of metal atoms that, in turn, influence catalyst stability.[6] Predicting energies of metal atoms with atomic site specificity enables expeditious evaluations of surface energies, cohesive energies of nanoparticles having generic shapes, and binding energies of metal adsorption sites. We calculate the energies of metal atoms by melding quadratic and linear interpolations across the space of coordination numbers, composition, and external strain.[7] These new interpolations shrink the training set size for 12 fcc p- and d-block metals from > 204 to as few as 24 Density Functional Theory (DFT) calculations, without sacrificing accuracy on the test set.[6] We will specifically highlight physical insights regarding metal-metal bonding that are obtained from the regressed parameters. As a proof of concept, we employ this method to reverse engineer thermodynamically stable active site motifs of nanoparticles that catalyze C1 conversion.In the second part, we investigate the existence of energy correlations on supported nanoparticles using Au nanorods on doped MgO supports as a prototypical model.[10] Using > 1700 DFT calculated binding energies, we show that although DFT-derived energy correlations between 31 reaction intermediates of CO oxidation, water gas shift, and methanol synthesis do exist, their slopes depart from bond order conservation constraints. This departure from bond order conservation at bifunctional metal/oxide interfaces enhances the flexibility in materials space during catalyst optimization. We build a conceptual framework using the multipole expansion in electrostatics to semi-quantitatively explain trends within the slopes. Through these trends, we elucidate the electrostatic origin of the departure in slopes from bond order conservation.Taken together, these two studies reveal new functional forms for energy correlations that determine metal nanoparticle stabilities, and reactivity trends at bifunctional interfaces. More crucially, we show that parameters of simple data-driven models contain a wealth of intriguing physical insights that enhance model interpretability.ReferencesNorskov et al., Proc. Natal. Acad. Sci. USA, (2011), 108, 937.Abild-Pedersen et al., Phys. Rev. Lett. (2007), 99, 016105.Tran et al., Nature Catal. (2018), 1, 696.O’Connor et al., Nature Catal. (2018), 1, 531.Goldsmith et al., AICHE J. (2018), 64, 2311.Choksi, Streibel, and Abild-Pedersen., J. Chem. Phys. (2020), 152, 094702.Streibel, Choksi and Abild-Pedersen. J. Chem. Phys. (2020), 152, 094701.Choksi et al. J. Phys. Chem. Lett. (2019), 10, 1852.Roling, Choksi, and Abild-Pedersen. Nanoscale (2019), 11, 4438.Choksi et al. Angew. Chem. Int. Ed. (2018), 57, 15410.",1,3.0
"Nanoparticles (NPs) have received tremendous attention as catalysts due to their high specific surface area and abundance of unique active sites. When designing such a catalyst, the Sabatier principle tells us that it must be tuned to adsorb key intermediates neither too strongly nor too weakly. For this reason, it is imperative to screen candidate NPs for their adsorption behavior. This task is conventionally accomplished either through time-consuming experiments, or computationally-expensive ab-initio calculations, which are primarily limited to highly idealized periodic surfaces. This presents a bottleneck in materials discovery, one which can be overcome through the development of rapid methods to predict adsorption behavior. To this end, we present a universal model of small molecule adsorption to NPs [1], which is able to accurately capture the adsorption behavior of metal NPs of any size, shape, or composition with minimal computational cost. By combining this adsorption model with our new genetic algorithm approach to NP structure prediction, we are now able to efficiently traverse the vast chemical space of bimetallic NPs and identify possible catalytic NPs to feed experiments. Overall, this work leverages cutting-edge chemical modeling techniques, such as first principles calculations, machine learning and statistical regression to significantly accelerate materials discovery and enable the rapid identification of promising bimetallic nanocatalysts.Dean, J.; Taylor, M.; Mpourmpakis, G. Unfolding Adsorption on Metal Nanoparticles: Connecting Stability with Catalysis. Science Advances 5 (9), eeax5101.",1,4.0
"The formation and breakage of chemical bonds at active sites is the molecular basis of catalysis. Being able to rapidly compute interaction strengths between bonding entities and understand their trends holds the key to the design of improved catalysts. Despite recent advances, machine learning (ML) faces a tremendous challenge for catalysis applications due to its poor transferability and explainability. Here we present a physics informed machine learning (PIML) approach that integrates convolutional neural networks with the d-band theory of chemisorption for predicting the chemical reactivity of metal surfaces. With *OH and *CO as two representative adsorbates, we demonstrated that the hybrid ML models outperform the purely data-driven ones in both data scarce and rich regions, especially for out-of-sample systems. More importantly, the architecture design enables its physical interpretability, shedding light on the nature of chemical bonding at metal surfaces.",1,5.0
"First principles Kohn-Sham density functional theory (KS-DFT) calculations hinder large-scale searches for new and sustainable heterogeneous catalysts due to significant computational expense. Alchemical perturbation density functional theory (APDFT) is a promising method that provides rapid predictions of catalyst descriptors based on perturbations of electrostatic potentials that arise from isoelectronic transmutations made to a reference catalyst. While straightforward and simple, first-order APDFT approximations are largely inaccurate when large and/or many transmutations are made. Here, we first discuss systematic trends in errors related to these factors for binding energy (BE) prediction of CHx, NHx, and OHx adsorbates on hypothetical alloy variations of fcc Pt(111) surfaces. Based on these observations, we demonstrate our approach fingerprinting locations of transmutations in these hypothetical alloys, and we show how to correct APDFT errors with machine learning models trained using about 3600 BE data points. Our models provide improved prediction accuracy with errors decreased by as much as an order of magnitude, and this approach extends the breadth of APDFT predictions, allowing rapid and accurate BE predictions on many more alloys made by numerous transmutation combinations.",1,6.0
"Many highly selective catalysts have been discovered that are defined by well-isolated sites with tailored metal-organic bonding. The rational design of such sites in de novo transition metal complexes, metal-organic frameworks (MOFs), or single atom catalysts, however, remains challenging. First-principles (i.e., with density functional theory, or DFT) high-throughput screening is a promising approach but is hampered by high computational cost, particularly in the brute force screening of large numbers of materials. In this talk, I will outline our efforts over the past few years to accelerate the design of single-site catalysts. I will describe our software and machine learning (ML) models that both simplify and accelerate the screening of new materials. These tools have enabled us to uncover new design rules and exceptions in both molecular and periodic materials (e.g., MOFs). We have paired ML models with multiobjective optimization strategies, robust uncertainty quantification, and highly parallel accelerated computing to discover optimal materials in weeks instead of decades. Time permitting, I will describe our recent efforts in autonomous computational chemistry by developing ML models as decision engines capable of predicting when calculations will fail and when methods beyond DFT are needed.",2,0.0
"The computation of reaction rate constants has been tackled using a multitude of theories ranging from classical transition state theory1 to the fully quantum flux-flux correlation function approach.2 The main cost of these methods is related to the need to explore potential energy surfaces, PES, either at the level of geometry optimizations, minimum energy path searches or to describe dynamics in time. In this context, machine learning algorithms which accelerate the search for reactive pathways or the prediction of rate constants are of great interest.3,4 We have constructed a database of over 130000 reaction rate constants5 obtained via quantum calculations of the reaction rate constants for particles crossing one dimensional single and double symmetric and asymmetric barriers. A deep neural network, DNN, was trained on the database with the ADAM optimizer and used to predict reaction rate constants. The most significant input features were identified using the Pearson correlation coefficient, and a grid search analysis was carried out for a coarse optimization of the DNN model hyperparameters. The network was trained for hundreds of epochs. The predicted results from the test set are in good agreement with the exact values. This approach shows promise for the application of machine learning to predict reaction rate constants.Pechukas, P. Transition State Theory. Annu. Rev. Phys. Chem. 32, 159–177 (1981).Tromp, J. W. & Miller, W. H. The reactive flux correlation function for collinear reactions H + H2, Cl + HCl and F + H2. Faraday Discuss. Chem. Soc. 84, 441–453 (1987).Amabilino, S. et al. Training neural nets to learn reactive potential energy surfaces using interactive quantum chemistry in Virtual Reality. J. Phys. Chem. A 123, 4486–4499 (2019).Ulissi, Z. W. et al. Machine-learning methods enable exhaustive searches for active Bimetallic facets and reveal active site motifs for CO2 reduction. ACS Catal. 7, 6600–6608 (2017).Valleau, S. Machine learning quantum tunneling in the kinetics of chemical reactions. In preparation. (2020).",2,1.0
"Conversion of biomass feedstocks to hydrocarbon fuels is often fraught with catalyst deactivation due to deposition of coke on catalyst surface or deactivation due to inorganic components of the biomass. Acidic zeolite catalyst get easily deactivated because of the basic oxides in the biomass feedstock. The high aromatic content and macromolecules in the biomass pyrolysis oils also tend to promote carbon formation on the active sites, which eventually lead to their deactivation. We developed a nickel supported on mixed oxide (Ni/RM) catalyst that is more robust than the commercial nickel on silica-alumina catalyst. This new catalyst is regenerable and can be used effectively for hydrodeoxygenation of biomass pyrolysis oils to long chain hydrocarbon fuels. The new catalyst is effective in opening both furan and benzene rings and thus producing hydrocarbon fuels with linear and branched carbon chain length between C6 to C15. The catalyst was not deactivated by inorganic components in the biomass, however, the catalyst activity decreased with deposition of coke on the active sites and required frequent catalyst regeneration through combustion and reduction to restore activity. We developed a machine learning (ML) algorithm which enabled us to reformulate the catalyst and reduce the carbon deposition on the active site. The signature of each nickel supported mixed oxide catalyst (Ni/RM)) was defined as the weight percent of eleven elements  (Al, Ca, Fe, K, Mg, Na, Ni, P, S, Si, Ti). These together with the reaction temperature formed the feature vector (input to ML). The output of the ML was coke formation, which was also a performance measure for the designed catalyst. The network relating the ML inputs to the output was either a simple Neural Network or a Convolutional Neural Network (CNN). Using this ML model as a basis, we inverted the model (inverse modeling) to obtain a new Ni/RM catalyst, which was then validated through experiments. The machine learning approached reduced catalyst development time and improved the robustness of the catalyst. Thus, the carbon deposition rate was reduced and catalyst regeneration frequency was also reduced.",2,2.0
"Machine learning models have proven useful in a variety of chemical and materials applications. However, one drawback of these models is that they are often geared to a particular application, and thus a new model must be created for a new application. This is particularly noticeable in applications that feature guest-host interactions, broadly defined, as there is a huge combinatorial challenge in considering many possible guests and many possible hosts. For example, this combinatorial challenge applies to catalysis, adsorption, doping, and intercalation. In this work, we develop and apply strategies to improve the generality and reusability of machine learning models.We have developed a machine-learning architecture that takes advantage of inherent aspects of chemical systems, and is particularly geared towards those that feature guest-host interactions. This framework partially decouples the guests from the hosts by introducing intermediate variables, which are a set of implicit host properties that control how the host interacts with all guests. These intermediate variables are learned during the fitting process. We create separate sub-models for different host elements for predicting these intermediate variables, and separate sub-models for how different guests respond to these host variables. The sub-models are all fit simultaneously. Our framework takes advantage of the fact that elements are discrete entities, and greatly simplifies the huge combinatorial challenge of considering many possible guests and many possible hosts. We apply this framework to multiple datasets and show how it can be used to improve the efficiency and accessibility of initial screening in a variety of applications.",2,3.0
"The Open Catalyst Project aims to develop new ML methods and models to accelerate the catalyst simulation process for renewable energy technologies and improve our ability to predict activity/selectivity across catalyst composition. To achieve that in the short term we need participation from the ML community in solving key challenges in catalysis. One path to interaction is the development of grand challenge datasets that are representative of common challenges in catalysis, large enough to excite the ML community, and large enough to take advantage of and encourage advances in deep learning models. Similar datasets have had a large impact in small molecule drug discovery, organic photovoltaics, and inorganic crystal structure prediction. We present the first open dataset from this effort on thermochemical intermediates across stable multi-metallic and p-block doped surfaces. This dataset includes full-accuracy DFT calculations across 53 elements and their binary/ternary materials, various low-index facets. Adsorbates span 56 common reaction intermediates with relevance to carbon, oxygen, and nitrogen thermal and electrochemical reactions. Off-equilibrium structures are also generated and included to aid in machine learning force field design and fitting. Collectively, this dataset represents the largest systematic dataset that bridges organic and inorganic chemistry and will enable a new generation of catalyst structure/property relationships. Fixed train/test splits that represent common chemical challenges and an open challenge website will be discussed to encourage competition and buy-in from the ML community.",2,4.0
"The discovery of abundant and affordable materials for sustainable energy applications is a significant challenge of this decade. The space of possible material compositions and crystal structures is so vast that brute-force screening approaches are unfeasible. However, density functional theory (DFT) combined with machine-learning (ML) techniques have a huge potential to speed up the search and hold a great promise for the discovery of novel materials.  Here, we present a ML-based active-learning framework to search for stable and meta-stable inorganic materials. The workflow includes the generation of candidate structures, which is followed by automated DFT job submission, ML-training, and acquisition. When using a search space of experimentally observed crystal structures, this approach was recently applied to the discovery of new stable polymorphs of IrOx, predicted to have an enhanced stability and catalytic activity for the oxygen evolution reaction (OER). In this talk, we extend the search to hypothetical crystal structures that are generated from a bottom-up prototype enumeration scheme, based on spacegroup and Wyckoff positions. This is applied to the search for cheaper and more abundant materials. Last, we envisioned how the screening approach can be extended to surface and chemisorption properties, on order to extend the search to catalytic applications.",2,5.0
"In this work, we apply statistical learning (SL) to identify physical descriptors for predicting electronic metal-support interactions (EMSI) in catalysis. Charge transfer between metal atoms and their underlying oxide support influences both the binding strength of the metal to the support and the binding strength of reaction intermediates to the metal. Tuning charge transfer by introducing surface-modifying dopants or co-adsorbates is an important strategy for controlling catalytic behavior. We use SL to build models for predicting metal atom binding energy to the oxide surface as a function of readily available physical descriptors containing properties of the metal, the support, and the surface-modifiers. Models are trained against DFT datasets consisting of metal adsorption energies on modified MgO(100) surfaces featuring various aliovalent dopants (e.g., Al, B, Li, and Na) and adsorbates (e.g., F, H, OH, and NO2). These modifications generate both electron-rich and electron-poor MgO surfaces, which in turn enhance the extent of charge transfer between the metal and the support. We also test multiple SL procedures, including LASSO, Horseshoe prior, and Dirichlet-Laplace prior, and find that features derived by Dirichlet-Laplace prior construct the most accurate predictive models (i.e., achieving RMSE of ~0.2 eV for predicted binding energies). Moreover, we find that the features selected by SL using MgO training data generally are transferable to similar oxides, such as CaO(100), BaO(100), and ZnO(100), where data for these oxides are excluded from the feature selection process. This demonstrates the robust nature of features selected by the Dirichlet-Laplace prior procedure, both in terms of model accuracy and transferability to oxide surfaces beyond those included in the training set.",2,6.0
"Over the past two decades, multiscale modeling has advanced tremendously, and several algorithms currently exist. Yet, our ability to apply first principles modeling to process design is seriously limited due to multiple challenges. One of the challenges relates to error of first principles calculations, reaction mechanisms, active sites, etc. and its impact on catalyst discovery and reactor design. In this talk we will discuss sources of errors and new statistical learning methods that are transferable and explainable to quantify and correct some of the errors. We will discuss data quality and probabilistic graph models as a framework that integrates data, knowledge, and physical models to enable uncertainty quantification.",2,7.0
"Numerical algorithms for estimation of probability distributions are the backbone of advanced process design, monitoring and optimization techniques, e.g. stochastic dynamic optimization (Rossi et al., 2016), robust state estimation (Mondal et al., 2010) and real-time risk-based decision making (Si et al., 2012). The latter have recently attracted the attention of both industry and academia, due to the emergence of the industry 4.0 paradigm and the steady increase in the availability of readily accessible experimental and process data. In view of these considerations, the development of new and more efficient methods for PDF estimation could facilitate the systematic implementation of the industry 4.0 guidelines, thus improving process automation, economics, reliability and safety.Most of the conventional algorithms for estimation of probability density functions (PDFs), e.g. Metropolis-Hastings, Gibbs sampling and Hamiltonian Monte Carlo (Gamerman and Lopes, 2006), consist of Monte Carlo sampling strategies, which rely on acceptance/rejection criteria to draw samples from the high probability regions of the PDF of interest. These algorithms are well-established, reasonably accurate and generally reliable but suffer from some important drawbacks: (I) poor performance in the presence of high parameter correlation; (II) limited applicability to complex PDF estimation problems (these state-of-the-art algorithms may not properly capture the features of multimodal PDFs, heavily skewed PDFs and/or PDFs with non-convex level sets); (III) considerable (often unaffordable) computational cost; and (IV) poor scalability and limited intrinsic concurrency. Unfortunately, probability distributions with complex shapes and high parameter correlation are very common in engineering applications, thus there is a need for new algorithms for estimation of probability distributions, which can mitigate some of the aforementioned drawbacks.This contribution proposes a new strategy for PDF estimation, which combines an adaptive space partitioning scheme, inspired by multidimensional integration algorithms (Hahn, 2005), with a quasi-deterministic sampling method, based on optimization. Adaptive space partitioning alternates with optimization-driven sampling in an iterative fashion, until appropriate convergence conditions are met, e.g. the sample moments of the probability distribution of interest no longer vary between consecutive iterations. The adaptive cubature step serves three principal purposes (Figure 1): (I) identification of those regions of the uncertainty space (the space spanned by the parameters of the probability distribution of interest), in which the current estimate of the PDF requires further refinement; (II) detection of very low probability density regions, which can be safely ignored; and (III) improvement of the concurrency and overall computational efficiency of the algorithm (sampling can be conducted in parallel within different subregions). The optimization-driven sampling phase complements the former, in that it allows selection of optimal samples, which uniformly span every single subregion of the uncertainty space, without the need for acceptance/rejection schemes, which often accept only a small fraction of all the candidate samples analyzed. As consequence of the combination of adaptive space partitioning and quasi-deterministic, optimization-driven sampling, the proposed PDF estimation method can generate accurate estimates of complex PDFs at reasonable computational cost, is generally insensitive to parameter correlation, and exhibits better scalability features than similar state-of-the-art algorithms.The new PDF estimation strategy, proposed in this contribution, has been demonstrated on several different case studies, namely, the estimation of several known probability distributions as well as the computation of the PDF of the parameters of the mathematical model of a continuous drug product manufacturing plant (the scale of these PDF estimation problems ranges from 2 to 8 dimensions). As a basis for comparison, all of these applications have also been solved with conventional Monte Carlo methods. These case studies confirm that the new PDF estimation strategy, proposed in this work, is more accurate and robust than state-of-the-art Monte Carlo methods.ReferencesGamerman, D., Lopes, H.F. (2006). Markov Chain Monte Carlo - Stochastic simulation for Bayesian inference. Taylor & Francis Group, New York (NY).Hahn, T. (2005). Cuba – a library for multidimensional numerical integration. Computer Physics Communications, 168, 78-95.Mondal, S., Chakraborty, G., Bhattacharyy, K. (2010). LMI approach to robust unknown input observer design for continuous systems with noise and uncertainties. International Journal of Control, Automation and Systems, 8, 210-219.Si, H., Ji, H., Zeng, X. (2012). Quantitative risk assessment model of hazardous chemicals leakage and application. Safety Science, 50, 1452-1461.Rossi, F., Reklaitis, G., Manenti, F., Buzzi-Ferraris, G. (2016). Multi-scenario robust online optimization and control of fed-batch systems via dynamic model-based scenario selection. AIChE Journal, 62, 3264-3284.",3,0.0
"Statistics, machine learning, and signal processing are the dominant paradigms used to analyze data; unfortunately, such techniques provide limited capabilities to analyze certain types of datasets. A couple of interesting examples that illustrate this limitation are the anscombe quartet [1] and the datasaurus dozen [2] datasets. These datasets are visually distinct (deﬁne a different geometrical object) but they have the exact same descriptive statistics (e.g., mean, standard deviation, and correlation).The recent application of algebraic and computational topology to data science has led to the development of a new ﬁeld known as Topological Data Analysis (TDA) [3]. TDA techniques are based on the observation that data can be interpreted as elements of a geometrical object; as the name suggests, TDA utilizes techniques from computational topology to quantify the geometry of data [4]. Fundamentally, topology studies geometric and spatial relations that are persistent (are stable) in the face of continuous deformations of an object (e.g., stretching, twisting, and bending). This perspective provides multiple advantages over other techniques [3, 5]:• Topology studies the geometry of the data in a manner that is independent of the chosen coordinates.• Topology studies the geometry of the data in a way that minimizes sensitivity to the metric chosen.• Topology generalizes well to high-dimensional spaces.The main focus of this talk is a technique in the ﬁeld of TDA that is known as persistence homology [6, 7]. The goal of persistent homology is to extract topologically dominant features within the data in the form of basic features such as connected components, holes, loops, and voids. This feature information can be quantified and leveraged by statistical and machine learning techniques to perform regression, classiﬁcation, hypothesis testing, and clustering tasks [8, 9, 10, 11, 12, 13].TDA can be seen as dimensionality reduction technique that maps data from its original high-dimensional space to a low-dimensional space that it is easier to understand and visualize. This is similar in spirit to principal component analysis (PCA), which projects the data into a low-dimensional space by extracting latent variables (principal components) that contain information in terms of variance. In TDA, the latent variables are homologies that contain information in terms of topological features.In this talk, we review orelevant concepts and computational methods of TDA from the perspective of chemical engineering applications. We show how to apply persistent homology to analyze datasets described by point clouds and functions in high dimensions and we discuss fundamental stability results of topological features in the face of perturbations. We present multiple case studies with complex synthetic and experimental datasets to demonstrate the concepts and advantages of TDA. Speciﬁcally, we show that TDA extracts informative features from complex datasets that correlate strongly with emerging features of practical interest. For instance, we demonstrate the application of these techniques in the analysis of time series and state space geometry, and in the geometric analysis of 2-dimensional diffusion scalar fields. Our work seeks to open new research directions and applications of TDA in chemical engineering.[1] Francis J Anscombe. Graphs in statistical analysis. The american statistician, 27(1):17–21, 1973.[2] Justin Matejka and George Fitzmaurice. Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pages 1290–1294, 2017.[3] Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46(2):255–308, 2009[4] Herbert Edelsbrunner and John Harer. Computational topology: an introduction. American Mathematical Soc., 2010.[5] Afra Zomorodian. Topological data analysis. Advances in applied and computational topology, 70:1–39, 2012.[6] Robert Ghrist. Barcodes: the persistent topology of data. Bulletin of the American Mathematical Society, 45(1):61–75, 2008.[7] Gunnar Carlsson, Afra Zomorodian, Anne Collins, and Leonidas J Guibas. Persistence barcodes for shapes. International Journal of Shape Modeling, 11(02):149–187, 2005[8] Peter Bubenik. Statistical topology using persistence landscapes. arXiv preprint arXiv:1207.6437, 3, 2012.[9] Peter Bubenik, Gunnar Carlsson, Peter T Kim, and Zhi-Ming Luo. Statistical topology via morse theory persistence and nonparametric estimation. Algebraic methods in statistics and probability II, 516:75–92, 2010.[10] Peter Bubenik and Paweł Dłotko. A persistence landscapes toolbox for topological statistics. Journal of Symbolic Computation, 78:91–114, 2017.[11] Andrew J Blumberg, Itamar Gal, Michael A Mandell, and Matthew Pancia. Robust statistics, hypothesis testing, and confidence intervals for persistent homology on metric measure spaces. Foundations of Computational Mathematics, 14(4):745–789, 2014.[12] Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence images: A stable vector representation of persistent homology. The Journal of Machine Learning Research, 18(1):218–252, 2017.[13] Jan Reininghaus, Stefan Huber, Ulrich Bauer, and Roland Kwitt. A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4741–4748, 2015.",3,1.0
"The wide utilization of the AM method lies in its advantages including rapid prototyping, faster manufacturing, reduced time and operating cost, and also the possibility to create almost any geometry. For complicated, high yield strength and high precision metal part construction, laser powder bed fusion (LPBF) has been extensively researched and prototyped [1]. To identify errors in real-time to avoid detrimental damage to the machine and material and energy waste, in-situ sensor monitoring technologies are developed to record the manufacturing information [2]. Nevertheless, the overwhelming amount of image based data produced by the optical sensors poses challenges in the data analysis and storage [3].To address the aforementioned problem, we develop an automated data-flow that integrates the simulation model and machine learning network for real-time process simulation and sensor data analytics. In particular, the simulation models, such as thermal history analysis, are tailored to mimic the manufacturing platform characteristic and to incorporate potential process disturbances. From the simulation result, the heat map images are reconstructed with corresponding classified disturbances categories, according to the industrial sensor technologies. Advanced machine learning algorithms like convolutional neural networks (CNN) are designed and trained with transfer learning to achieve a robust yet computationally acceptable in-situ process analysis. [1] Frazier, W.E., 2014. Metal additive manufacturing: a review. Journal of Materials Engineering andPerformance 23, 1917–1928.[2] Everton, S.K., Hirsch, M., Stravroulakis, P., Leach, R.K., Clare, A.T., 2016. Review of in-situ process monitoring and in-situ metrology for metal additive manufacturing. Materials & Design 95.[3] Scime, L., Beuth, J., 2019. Using machine learning to identify in-situ melt pool signatures indica-tive of flaw formation in a laser powder bed fusion additive manufacturing process. AdditiveManufacturing 25, 151–165.",3,2.0
"Chemical substances available for industrial and commercial activities or uses may present an unreasonable risk to human health and the environment. In the U.S., the chemical risks should be evaluated according to the Toxic Substances Control Act, while in Europe, this is under the Registration, Evaluation, Authorization, and Restriction of Chemicals (REACH). Therefore, knowing what occurs with a chemical at each stage of its life cycle is essential for its evaluation, selection, use, and regulatory decision-making. However, gathering information to evaluate chemical risk is a time-consuming task, and especially at the end-of-use (EoU) stage because of the high uncertainty about chemical fate and exposure pathway. The contribution of this work is a data engineering approach for gathering information about pollution control units (PCUs) from publicly-available regulatory databases and filling data gaps based on technical information. The data collected include the type of waste stream having the chemical of interest (e.g., liquid waste), composition, and PCUs (e.g., absorber) and their efficiencies, which the framework transforms into a machine-readable structure for future automation. Thus, the developed approach can rapidly streamline chemical release estimations from PCUs and allocate potential effluents. Besides, it could be applied to identify and recommend the application of industrial PCUs for managing chemicals at generator facilities. The developed approach will be applied for case studies with the resulting identification of PCUs and release estimates. This approach may support the risk evaluation process by developing learning-from-data models to select PCUs and estimate releases.The views expressed in this abstract are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency.",3,3.0
"In power grid control centers, optimal power flow (OPF) problems are solved several times per day to find optimal setpoints given load demands. However, an even more challenging problem is to have the ability to quickly assess the security of the optimal operating points in the case of contingency events which can result in operating at insecure or suboptimal points. Static grid security analysis can be done through N-1 contingency enumeration, where each contingency event is simulated and the resulting OPF solution is checked for voltage and line flow violations. An operating point that has no voltage or line flow violations is N-1 secure, but this becomes intractable to solve online for large grids that have hundreds of contingencies and thousands of variables. Deep machine learning models have had success in predicting system security under contingency [1, 2] or functioning as a quick OPF solver [3], but operators are wary of these fully black-box models because they require a lot of reliable training data, they ignore well-established power-flow physics, and have no interpretability.Physics-informed machine learning models have been shown to approximate complex, nonlinear functions while retaining physical generalizability in the areas of physics, differential equations, and systems engineering [4, 5]. In this work, deep neural networks with embedded physics have been trained to predict OPF solutions under contingency and the resulting system security. Various approaches are considered for the generation of balanced and realistic training data, using offline simulations of varying load profiles and rigorous optimization for all contingencies using power flow software package Egret [6]. Different techniques for developing the physics-embedded NN models will be presented, including an augmented loss term that considers Kirchoff’s Current Law at every node in the system, which allows for fewer training points and better performance outside of the training set. This framework is demonstrated on IEEE case studies of increasing size and complexity, and all solutions are compared to fully black-box approaches with respect to accuracy, training data requirements and complexity. Finally, integration of dimensionality reduction techniques with physics-informed NN training is presented for large case studies, which allows for maintaining model accuracy and tractability in the case of high-dimensional input spaces.References:Sunitha, R., R.S. Kumar, and A.T. Mathew, Online static security assessment module using artificial neural networks. IEEE transactions on power systems, 2013. 28(4): p. 4328-4335.Donnot, B., et al., Fast Power system security analysis with Guided Dropout. arXiv preprint arXiv:1801.09870, 2018.Hu, X., et al., Physics-Guided Deep Neural Networks for PowerFlow Analysis. arXiv preprint arXiv:2002.00097, 2020.Raissi, M., P. Perdikaris, and G.E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 2019. 378: p. 686-707.Lutter, M., C. Ritter, and J. Peters, Deep lagrangian networks: Using physics as model prior for deep learning. arXiv preprint arXiv:1907.04490, 2019.Knueven, B., et al., Egret v. 0.1 (beta). 2019: ; Sandia National Lab. (SNL-NM), Albuquerque, NM (United States). Python.Disclaimer: Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia LLC, a wholly owned subsidiary of Honeywell International Inc. for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the USDOE or the United States Government.",3,4.0
"The pulp and paper industry is the third-largest consumer of energy in the US industrial sector thereby leading to tremendous opportunities to improve its energy efficiency and productivity. The pulping process, which converts woodchips into pulp by displacing lignin from cellulose fibers, is one of the most important operations in a pulp and paper mill. For the pulping process in a pulp & paper plant that uses wood as raw material, it is important to have real-time knowledge about the moisture content of the woodchips so that the process can be optimized and/or controlled correspondingly to achieve satisfactory product quality while minimizing the consumption of energy and chemicals. Both destructive and non-destructive methods have been developed for estimating moisture content in woodchips, but these methods are often lab-based that cannot be implemented online due to time constraint [1], relatively complex and costly to install and operate[2], or too fragile to stand the harsh manufacturing environment. Due to these limitations, at present, the moisture level is measured only a few times throughout the year in the pulp and paper industry.To address these limitations, we propose a non-destructive and economic approach based on the Internet-of-Things (IoT) based Wi-Fi and use channel state information (CSI) to estimate the moisture content in woodchips. We extract CSI by modifying the open-source device drivers for Intel Wi-Fi link 5300 Network Interface Card based on CSITool [3]. CSI contains information about the channel in the form of individual data sub-carriers capturing indoor channel features like the effect of scattering, fading, and power decay with distance. We hypothesize that when a Wi-Fi signal passes through wood chips, the water or moisture content in woodchips would cause a corresponding change in the Wi-Fi signal which is then captured in CSI. The goal is to develop a data-driven model to extract the relationship between the change in CSI and the woodchip moisture content. CSI has been successfully used for moisture detection in wheat [4]. However, compared to wheat moisture detection, woodchip moisture detection is much more challenging due to the significant heterogeneity in the shape, structure and arrangement of the woodchips. For example, the woodchip arrangement in the container is expected to have a significant impact on the CSI data.In this work, our goal is to develop a robust approach that is insensitive to the heterogeneity of the woodchips, and in particular the effect of shuffling, to achieve a consistent and accurate moisture estimation. To achieve this goal, our proposed approach is based on statistics pattern analysis (SPA) framework [5][6] we developed previously, where SPA is integrated with different classification approaches to build robust multivariate statistical models for multiclass moisture classification in woodchips. Specifically, SPA is used to extract features from extremely noisy raw Wi-Fi CSI data thereby converting the messy big data into so called smart data for accurate sensing. These features are then used to build multiclass classification models using machine learning techniques including linear discriminant analysis (LDA), subspace discriminant classification (SDC), support vector machines (SVM), and artificial neural networks (ANN). We demonstrate an extensive study on the above-mentioned approaches for multiclass classification which, when combined with SPA, provide potential solutions for estimating woodchip moisture content for the pulp and paper industry. Our results show highly accurate classification of more than 20 moisture levels even when two moisture classes are separated by a very small margin of <1% (wet basis). Our approach is not only economic and non-destructive but can also be implemented online, making it highly desirable for the pulp and paper manufacturing environment.[1] R. Govett, “A PRACTICAL GUIDE FOR THE DETERMINATION OF MOISTURE CONTENT OF WOODY BIOMASS A Practical Handbook of Basic Information, Definitions, Calculations, Practices and Procedures for Purchasers and Suppliers of Woody Biomass.”[2] S. O. F. Sensing, W. H. O. Uses, and E. Method, “Learn the Six Methods For Determining Moisture.”[3] D. Halperin, W. Hu, A. Sheth, and D. Wetherall, “Tool Release: Gathering 802.11n Traces with Channel State Information,” ACM SIGCOMM Comput. Commun. Rev., 2011.[4] W. Yang, X. Wang, S. Cao, H. Wang, and S. Mao, “Multi-class wheat moisture detection with 5GHz Wi-Fi: A deep LSTM approach,” in Proceedings - International Conference on Computer Communications and Networks, ICCCN, 2018.[5] Q. P. He and J. Wang, “Statistics pattern analysis: A new process monitoring framework and its application to semiconductor batch processes,” AIChE J., 2011.[6] J. Wang and Q. P. He, “Multivariate Statistical Process Monitoring Based on Statistics Pattern Analysis,” Ind. Eng. Chem. Res., 2010.",3,5.0
"In the era of IoT and digitalization, developing user-friendly tools to facilitate the maintenance of critical assets within a chemical plant is an area of growing industrial interest. We have integrated concepts of data analytics along with information and insights obtained from our industrial collaborator to develop a tool that will aid engineers to monitor the health of centrifugal pumps using plant data. In the proposed talk, we will report the development of PROAD – a health monitoring dashboard for centrifugal pumps, a critical and common equipment in a range of process industries spanning from oil and gas, chemicals, and petrochemicals, to food, agriculture, paper and pulp, and pharmaceuticals.In our work, we have developed a comprehensive methodology that allows health monitoring, diagnosis, and prescriptive maintenance of centrifugal pumps. We adopt a multi-pronged approach where we use process operational data for health monitoring, and equipment-specific data for prescriptive maintenance and fault diagnosis. Process operational data are used to compute and monitor the deviation of the pump head as a function of flow rate (pump head curve) from the vendor-supplied head curve for a fresh pump. Recent trends of these deviations are then used to predict the remaining useful life of the pump w.r.t. a given threshold head deviation. On the other hand, continuous readings from different vibration sensors on a pump are analyzed for trends to prescribe more detailed spectral checks for potential incipient faults. In addition, a diagnostic tool is also developed that analyzes spectral data and recognizes known patterns linked to various mechanical faults (such as imbalance, misalignment, bearing damage, cavitation, etc.) common in centrifugal pumps. Our diagnostic tool also has the ability to add new and unknown faults that may arise during the pump operation and get diagnosed by maintenance engineers. Besides these process/mechanical health indicators, we estimate the energy equivalent economic loss and carbon emissions associated with the pump over its operational period.The above methodologies are implemented within a standalone plug and play desktop application (PROADv2020) easily implementable in any chemical plant without requiring any external software license. The app revolves around a graphical user interface (GUI) – based dashboard built using the MATLAB app designer feature, which displays all pumps on a single screen. Data input to PROAD is via well-directed, interactive, and robust MS-EXCEL templates. Based on different performance and diagnostic metrics, health status of each pump is indicated by specific color codes. Detailed health status and historical performance of each pump are conveyed via reports, and visualized via head, efficiency, vibration, and spectral plots. PROAD maintains a database of all reports and plots for future access. PROAD also has a feature that summarizes the health status of all pumps, thus providing an easy way to assess their performance at once. The GUI of PROAD is developed in consultation with engineers from our industrial collaborator, thus any information obtained from PROAD can be easily interpreted by any industry personnel. An overview of health monitoring of centrifugal pumps via PROAD-v2020 is illustrated in Figure 1.We have tested PROAD-v2020 on multiple centrifugal pumps with about 20 months of real data obtained from our industrial collaborator. The health status and diagnoses inferred from PROAD were consistent with the performance observed and confirmed by the plant engineers. The tool is currently being tested by our industrial collaborator and refined with their feedback.AcknowledgementThis project is funded under Energy Innovation Research Programme (EIRP) Award NRF2017EWT-EP003-020, administrated by the Energy Market Authority (EMA). The EIRP is a competitive grant call initiative driven by the Energy Innovation Programme Office and funded by the National Research Foundation (NRF) of Singapore. We would also like to thank our industrial collaborator for providing plant data and feedback on PROAD’s development.",3,6.0
"Digitalization is the central pillar towards the introduction of Industry 4.0 in manufacturing. However, the pharmaceutical industry lags behind in this regard and sophisticated approaches in terms of data analysis are still seldom used despite the availability of an abundance of recorded process data. Highly regulated fields such as drug manufacturing require manufacturers to store process data over many years for backtracking purposes. The analysis of such stored data highlights the potential to use it for predictive maintenance purposes.This work presents a data-driven approach for equipment monitoring and the implementation of predictive maintenance (PdM) in an aseptic filling line in drug product manufacturing. Multiple years’ worth of industrial data are used for this purpose. The data mainly consists of Supervisory Control and Data Acquisition (SCADA) variables which incorporate parameters such as pressure, temperature or vibration data. The goal of this work is to replace currently implemented time-based maintenance strategies with more efficient predictive and condition-based ones. The data obtained from the production facility varies in their source and structure based on their main intended purposes. The data is therefore first preprocessed and prepared. In this work, different machine learning algorithms are compared to identify the most suitable given the structure of the available data (i.e. the installed sensors at different locations in the line), the information content as for certain areas specific measurements are recorded (e.g. changeover processes), and the resulting prediction accuracy. Random forest algorithms are the most commonly applied in this regard due to their high performance potential with limited information incorporated in the data and an avoidance for over-fitting. Artificial neural networks are also popularly applied for a diverse set of PdM problems followed by support vector machines and k-means models. Ensemble learning or combinations of the different algorithms are also tested. A detailed analysis of the results obtained by the different algorithms is provided. The best strategies for each equipment are determined in terms of maintenance frequency.A decision support tool is proposed to determine if an equipment is expected to fail in the foreseeable future. Expert knowledge is also then incorporated in the tool to determine the appropriate course of action identified with each type of failure or countermeasures needed to prevent failure. Different maintenance strategies are compared based on the expected downtime, uncertainty of the decision and the associated costs. Such a tool can be easily integrated with other facility scheduling decision tools.",3,7.0
"There is great potential in combining heterogeneous metallic nanoparticle catalytic active sites and enzymatic active sites for sequential chemical transformations. This work combines enzymes, namely oxidases, and metallic nanoparticles in one material for tandem catalytic activity, analogous to substrate channeling. We investigate the cooperativity between the metallic and oxidase catalytic active sites in two selective oxidation reaction systems.Gold nanoparticles and gold alloy nanoparticles are very selective oxidation catalysts using mild oxidants such as hydrogen peroxide. Oxidases convert sugars to carboxylic acids and hydrogen peroxide. This work demonstrates tandem oxidative activity using glucose oxidase and gold and gold alloy nanoparticles. The glucose oxidase will oxidize glucose to gluconic acid and hydrogen peroxide, and the gold/gold alloy nanoparticle will perform a second oxidation reaction.In the first reaction sequence, glucose is oxidized to gluconic acid and hydrogen peroxide using glucose oxidase, and the gluconic acid is further oxidized to saccharic acid using hydrogen peroxide as the oxidant on the gold nanoparticle surface. We demonstrate that glucose oxidase is bound to the gold nanoparticle surface and that the hybrid system produces saccharic acid, while the glucose oxidase on its own does not produce saccharic acid, only gluconic acid. The selectivity of the hybrid system can be adjusted by changing the pH. Higher pH values promote oxidation to saccharic acid.The second reaction sequence uses the same glucose oxidase hybrid gold/gold alloy nanoparticle system for the oxidation of glucose by glucose oxidase, and the hydrogen peroxide product is used to partially oxidize benzyl alcohol to benzaldehyde. Cooperativity is demonstrated between the glucose oxidase and the gold nanoparticles through increased production of benzaldehyde in the hybrid system versus the gold nanoparticles on their own. The effect of the alkalinity of the solution on the catalytic performance is also demonstrated in this system.",4,0.0
"Molecular hydrogen is an important high-energy carrier for future energy technologies if produced from renewable electrical energy. Hydrogenase enzymes offer a pathway for bioelectrochemically producing hydrogen that is advantageous over traditional platforms for hydrogen production due to low overpotentials and ambient operating conditions. However, catalysis is often limited by the rate of electron transfer from the electrode surface to the enzyme’s active site. We demonstrate the ability of a cobaltocene-functionalized polyallylamine (Cc-PAA) redox polymer to immobilize and mediate rapid electron transfer to three different hydrogenases from Clostridium pasteurianum and Methanococcus maripaludis for hydrogen evolution at a cathode. Furthermore, Cc-PAA-mediated hydrogenases can operate at high faradaic efficiency (80-100%) and low apparent overpotential (-0.578 to -0.593 V vs. SHE). Specific activities of these hydrogenases electrosynthetically evolving hydrogen via Cc-PAA were comparable to their respective activities in traditional methyl viologen assays, indicating that Cc-PAA mediates electron transfer at high rates, to most of the embedded enzymes.",4,1.0
"Beta-lactam antibiotics have made large contributions in the fight against bacterial infections ever since their discovery in 1928. Currently the industrial production of beta-lactam antibiotics is conducted via batch-wise chemical synthetic pathways consisting of multiple reaction steps, use of protecting groups, harsh solvents and very low temperatures, generating large amounts of hazardous waste. To approach this, a great deal of research has been conducted in developing a more efficient enzymatic synthetic processing pathway. The biocatalyst of interest is penicillin G acylase from E. coli (EcPGA). The combination of distinct acyl-donor moieties and nucleophilic beta-lactam moieties results in the synthesis of different semi-synthetic beta-lactam antibiotics. While wild-type EcPGA has been shown to have poor synthetic properties due to high product affinity, protein engineering on the binding pocket has altered the binding properties of substrates and products to enhance specificity.We have improved the expression of wild -type EcPGA 20-fold to 45 mg/L culture. Additionally, we have characterized the synthetic properties of wild type EcPGA, βF24A, and Assemblase®, a commercial variant of EcPGA for cephalexin synthesis. We found that Assemblase® possessed marginally improved synthetic parameters over βF24A and wild-type enzymes. Higher product accumulation during long-term reaction experiments confirmed this observation. Next, wild type EcPGA was immobilized on various epoxy-functionalized supports to enable recycling of the biocatalyst between reactions. Approximately 30% of free enzymatic activity was retained after immobilization with decreased apparent activity attributed to diffusion limitations, as suggested by the decreasing apparent specific activity with increasing particle sizes. The synthesis to hydrolysis ratio decreased slightly, possibly due to internal product accumulation due to diffusion limitations. We are currently modeling the reaction and diffusion within the carrier pores using a reaction diffusion model to predict optimum enzyme loading to achieve high productivity while minimizing reduction of the synthesis to hydrolysis ratio.",4,2.0
"We present experimental evidence of pH oscillations and bistability in the urea-urease enzyme reaction carried out in a continuous stirred tank reactor (CSTR) and compare these results with dynamics predicted by a detailed model of urea-urease reaction.The CSTR is open to atmosphere with three inlet streams delivering solutions of urease, urea and sulphuric acid and one outlet stream. Sulfuric acid is used as a second substrate to form a feedback loop controlling production of ammonia making it possible to display nonlinear dynamical effects. The pH in the reaction mixture is measured via a pH electrode. Concentration of the inflow reactants are taken as variable parameters as well as the flow rate k0. We show experimental k0 - pH hysteresis curves for a wide range of urease concentrations and various inlet concentrations of sulfuric acid. More importantly, we also report pH oscillatory behavior with amplitudes of ~1 pH units occurring in a region of parameters adjacent to the region of hysteresis.The model is based on a full scheme for protonation/deprotonation of the enzyme and its activated complex and, in addition, includes inhibition by the ammonium ions that are produced in weakly acidic conditions. A closer analysis based on the reaction network theory reveals that it is the inhibition that is responsible for the oscillatory behavior observed in the experiments. The model also shows an asymmetric double-bell-shaped curve of steady state reaction rate vs external pH in agreement with prior work. Near the higher peak there is a region of instability, where stable oscillations occur via Hopf bifurcation. Set of kinetic parameters for which the oscillatory behavior occurs was determined by the constrained stoichiometric network analysis, which uses linear optimization to select proper network parameters consistent with experimental observation of oscillations.",4,3.0
"Enzymatic catalysis offers a green and potent alternative for efficient CO2 conversion due to its high selectivity and specificity, high efficiency, and mild operational conditions. Carbonic anhydrase (CA) enzyme-based absorption technology for CO2 capture has been intensively investigated. The main issue related to this method is the activity, stability and reusability of the CA enzyme in vitro. To address this issue, CA@ZIF-L-stabilized Pickering interfacial catalytic system was constructed. In detail, CA enzymes were embedded into zeolitic imidazolate framework-L (ZIF-L) particles through biomimetic crystallization. The as-synthesized CA@ZIF-L particles were then spontaneously assembled into capsule at the interface between the oil phase (hexadecane) and water phase during the formation of Pickering emulsions. CA@ZIF-L-stabilized Pickering interfacial catalytic system was finally constructed to convert CO2 to bicarbonates. The performance of the system was evaluated by adding calcium ions to form calcium carbonate.The results showed that CA could rapidly initiate the crystallization of ZIF-L to form CA@ZIF-L particles under mild aqueous conditions. The immobilization efficiency of CA could be above 90%. The synthesized CA@ZIF-L, which was served as a solid emulsifier to stabilize Pickering emulsion with its self-assembly properties at the oil-water interface, exhibited excellent catalytic activity in Pickering interfacial reaction and the corresponding cyclic performance. In detail, CA@ZIF-L-stabilized Pickering interfacial catalytic system exhibited higher catalytic efficiency than that of free CA@ZIF-L mainly because of the shortened diffusion path of CO2 from gas phase to enzyme active center. The production rate of CaCO3 through Pickering interfacial catalysis reached 2.73 mg min-1, which is 1.14 times higher than free CA@ZIF-L. In the cycling experiment, about 96.2% of conversion efficiency could be retained for our system after 6 times recycling. These findings above implies that the system we constructed exhibits high activity and stability, particularly, superior recyclability for conversion of CO2 into CaCO3.",4,4.0
"Abstract: Threonine aldolase can synthesize β-hydroxy-α-amino acids from glycine and aldehyde in one step, which are important intermediate with two chiral centers. The free threonine aldolase is difficult to be separated and reused, and its stablity is not satisfied under industrial condition. Enzyme immobilization technology can effectively solve these problems. Based on the evaluation of enzyme activity recovery and reaction batch stability, immobilization methods of threonine aldolase such as adsorption, entrapment, covalent binding and crossing-linking were studied. After extensive screening, the covalent binding using amino resin with further glutaraldehyde cross-linking appeared to be the most promising method. Then we optimized the immobilization conditions such as the amount of carrier, glutaraldehyde concentration, the activation time for carrier, immobilization time, pH, and temperature. After optimizations, the immobilized enzyme activity recovery was up to 119.3%, and the enantiomeric excess (ee) value of products was 98%. After continuous immersion in the reaction solution for 18 days(enzyme activity is measured daily), the enzyme activity remained 82.4%. And the enzyme activity retention basically unchanged after being stored in a refrigerator at 4 ℃for 62 days. Compared with free enzyme, immobilized threonine aldolase has better batch reaction stability, storage stability and stereoselectivity. This work also contributed to the economic flexibility of β-hydroxy-α-amino acids biosynthesis.",4,5.0
"Many computational methods for predicting polymorphic behavior generate many possible crystal structures and rapidly evaluate the lattice energies of these candidate structures to determine the most likely to be observed experimentally. However, lattice energies alone do not fully represent the stability of systems that are at room temperature and sample a range of configurations due to the thermally-induced motions at non-zero temperatures, which have both harmonic and anharmonic character.In this talk, I survey our work using molecular dynamics to examine the free energy differences between enantiotropic crystalline systems, where the approximation that the lowest lattice energy crystal is the most stable must necessarily break down. We have found that although small rigid crystals are well-described by quasiharmonic approximations, even moderately conformationally flexible molecules can have extremely complex potential energy surfaces, making it hard to find appropriate structures from which to compute quasiharmonic approximations. We also find that many of these minima are accessible at room temperature, and proper annealing can help identify appropriate ensembles of structures to use for quasiharmonic analysis, and that bulk crystals differ thermodynamically in important ways from single unit cells. We also describe the extent to which there is moderate entropy-enthalpy compensation between different models and computational methods, potentially opening the door for using lower levels of computational detail to determine stability at room temperature even if classical potentials do not provide sufficient accuracy to determine lattice energies.",5,0.0
"Knowledge of the solid form landscape is key during the pharmaceutical development process. Crystal structure prediction is increasingly used to help survey these landscape, and density functional theory (DFT) has established a successful track record of predicting the most stable crystal structures for a variety of species. However, a few significant limitations of the widely used functionals have recently become apparent. Typical density functionals predict the polymorph stabilities poorly in certain conformationally flexible pharmaceuticals, for example. In other cases, these functionals incorrectly predict salt formation in are actually neutral co-crystals. More accurate correlated wavefunction methods can overcome these limitations, albeit at a much higher computational cost. The origins of these deficiencies will be discussed, and a simple, computationally inexpensive approach that can overcome the difficulties in conformational polymorphs will be presented. Examples will include prolific polymorph formers ROY, Axitinib, and Galunisertib.",5,1.0
"One of the most popular strategies of the optimization of drug properties in the pharmaceutical industry appears to be a solid form changing into a cocrystalline form. A number of virtual screening approaches have been previously developed to allow a selection of the most promising cocrystal formers (coformers) for an experimental follow-up. A significant drawback of majority of those methods is related to the lack of accounting for the crystallinity contribution to cocrystal formation. To address this issue, we propose a virtual coformer screening approach based on a modern cloud-computing crystal structure prediction (CSP) technology at a dispersion-corrected density functional theory (DFT-D) level.1 The CSP-based methods were validated on challenging cases of indomethacin and paracetamol cocrystallization, for which the previously developed approaches provided poor predictions. The calculations demonstrated a dramatic improvement of the virtual coformer screening performance relative to the other methods. It is demonstrated that the crystallinity contribution to the formation of paracetamol and indomethacin cocrystals is a dominant one and, therefore, should not be ignored in the virtual screening calculations. GX Sun, Y Jin, S Li, Z Yang, B Shi, C Chang, YA Abramov.  Phys. Chem. Lett. 2020, 11, 8832−8838.",5,2.0
"Crystal morphology plays a significant role in many industrial applications. For example, it has a great impact on the end-use properties of the materials and the downstream performance of the manufacturing process. However, due to the large number of crystals, such as more than 1,000,000 crystals in the Cambridge Structural Database (CSD) at present [1], coupled with numerous growth conditions including solvent, additive, temperature, supersaturation, and crystallization, it is a great challenge to the industry to obtain specific crystals with particular product functionality and morphology.The above challenge inspired our group at UCSB to develop the first version of our automated crystal morphology prediction software ADDICT (Advanced Design and Development of Industrial Crystallization Technology) [2]. Very recently, we have upgraded ADDICT’s software framework to enable it to implement crystal growth models for any crystal complexity. [3] and rewritten its code using object-oriented programming in Matlab as well as addition of new features in the third version of ADDICT (ADDICT3) [4]. Therefore, based on these recent advances we can start attempting to predict the morphology for organic salt crystals.Organic salt crystals are an important class of materials in the chemical, pharmaceutical, and nonlinear optical (NLO) industries. It is challenging to use mechanistic growth models to predict the morphology of such crystals, and in fact there are no other papers on the prediction of their morphology grown from solvents using such models. Organic salt crystals generally have stronger interactions than organic crystals, resulting in more complex periodic bond chains (PBCs) that are difficult to handle.In this study, the concepts of a neutral growth unit, a building unit, and new PBC rules are proposed to predict the morphology of organic salts. We use two examples to study the morphology of such crystals grown from solvent. They are: (1) L-leucine hydrobromide and (2) glycinium trifluoroacetate. The predicted crystal shapes grown from water solution are consistent with the experimental results, which provides evidence for the reliability of the proposed method.References[1] https://www.youtube.com/watch?v=4G6cFXtbwNo (accessed Jul 2, 2019).[2] Li, J., Tilbury, C. J., Kim, S. H., & Doherty, M. F. (2016). Prog. Mater. Sci., 82, 1-38.[3] Landis, S., Zhao, Y., & Doherty, M. F. (2020). Comput. Chem. Eng., 133, 106637.[4] Zhao, Y., Tilbury, C. J., Landis, S., Sun, Y., Li, J., Zhu, P., & Doherty, M. F. (2020). Cryst. Growth Des., (Accept)",5,3.0
"Punch sticking is an outstanding common problem that hinders commercial tablet manufacturing. The occurrence of punch sticking is evident when powder mass adheres onto tooling surfaces during the course of tablet compression. It causes poor tablet quality, such as rough tablet surface and inconsistent drug content, and results in low manufacturing efficiency due to the required intermittent process stoppage for cleaning. The commercial CEL (Form III) is an API with known high punch sticking propensity. An L-proline cocrystal of celecoxib was identified based on a computational approach for predicting crystal mechanical properties to significantly reduce the punch sticking propensity of celecoxib. The reduced punch sticking propensity by this cocrystal was attributed to both reduced plasticity through deactivating slip planes and minimizing exposure of high electronegative functional groups to punch tip during compression. This material sparing and reliable approach of integrated computational and experimental investigation of punch sticking holds promise in solid form selection of drugs for identifying crystal forms suitable for developing tablet formulations.",5,4.0
"Immobilizing an enzyme on a support often is essential for downstream separation of the biocatalyst from a product and retaining the valuable enzyme in many enzymatic processes. This is particularly true in the case of continuous enzymatic manufacturing processes where the enzyme needs either to be retained in the reactor or separated from the product downstream and recycled back to the reaction vessel. This becomes challenging when the product is in solid form, e.g., crystals, necessitating the separation of two types of solid particles, biocatalyst and the crystalline product. This is the case for the continuous synthesis and crystallization of β-lactam antibiotics using the enzyme Penicillin G acylase (PGA) [1].In this contribution, we propose a 3D-printed microfluidics-based device for downstream separation of the biocatalyst from the crystalline antibiotic product. Microfluidics is a technique characterized by the precise manipulation of fluid dynamics at the microscale and has shown a great promise for industrial processing applications. Among microfluidic devices, inertial microfluidics has experienced massive growth due to their high throughput, automation, operational simplicity, and low cost [2]. In such a device, particles with different sizes are separated by taking advantage of inherent inertial lift and drag forces. Inertial microfluidics can be employed by various channel shapes and structures; spiral channels are frequently used for the aim of high-throughput particle focusing by reduction of lateral migration positions and they can be an ideal candidate for this particular application. Here, we focus on the separation of high aspect ratio crystals from spherical biocatalyst particles. The efficiency of the separation for different product and catalyst sizes, and potentials/challenges for scaling up the proposed separation strategy will be discussed.[1] McDonald, M. A., Bommarius, A. S., Grover, M. A., & Rousseau, R. W. (2019). Continuous reactive crystallization of β-lactam antibiotics catalyzed by penicillin G acylase. Part II: Case study on ampicillin and product purity. Computers & Chemical Engineering, 126, 332-341.[2] Moloudi, R., Oh, S., Yang, C., Teo, K. L., Lam, A. T. L., Warkiani, M. E., & Naing, M. W. (2018). Inertial-based filtration method for removal of microcarriers from mesenchymal stem cell suspensions. Scientific reports, 8(1), 1-10",6,0.0
"Continuous crystallization has been in the focus of pharmaceutical industry as an appealing method to obtain drug substances of consistent critical quality attributes (CQAs), which meet strict regulatory guidelines. Traditionally, continuous crystallization is done in mixed suspension mixed product removal (MSMPR) crystallizers, but one main disadvantage of MSMPR crystallizers is the broad crystal size distribution (CSD) generated due to broad residence time distribution (RTD).1 On the other hand, the new emerging continuous oscillatory baffled crystallizers (COBCs) offer plug-flow conditions with narrow RTD properties to generate product with narrow CSD.2 Unlike traditional plug flow crystallizers that use high volumetric flow rate to achieve plug flow conditions, the COBCs use an oscillating piston to generate turbulent mixing within its concentric baffles without a high volumetric flow rate. However, a main disadvantage of COBCs is fouling and encrustation due to nucleation on the crystallizer walls which can lead to clogging and process failure.3 To remedy fouling and prolong the operating time of the COBC, seeding is a common methodology to not only reduce the supersaturation in the system for nucleation but also provide polymorphic form control. Although seeding is very common method in the pharmaceutical industry, seeding for continuous crystallization has proven to be challenging, especially in the case of the COBC. Consistent manual preparation of seeds is labor intensive and economically infeasible, and, furthermore, any fluctuations of seed quality can impact startup dynamics.4 In addition, seeds can be washed away after a few residence times, which no longer provide the benefits of seeding.In this work, a novel integration of the MSMPR crystallizer with a COBC demonstrated in-situ seed generation capabilities while demonstrating CSD and polymorphic control in continuous combined cooling antisolvent crystallization (CCAC). Firstly, the 3D solubility surfaces of the metastable and stable form were generated to provide an operating regime for the continuous CCAC. It was discovered that the continuous generation of the metastable form required higher concentration of solvent than that of the stable form. Secondly, the MSMPR crystallizer was integrated into the COBC as a separate temperature zone to generate seeds via primary nucleation. The seeds generated were then fed into the COBC without pump transfer to allow for further growth. While compared to a single stage MSMPR crystallizer, the integrated system showed little signs of fouling and encrustation while generating product of uniform CSD. While both platforms are capable of polymorphic control using a temperature switch, the integrated system was able to maintain the narrower CSD while compared to the MSMPR, which makes this combined system a novel next generation continuous crystallization platform for systems with polymorphism and seeding requirements.  References:(1) Ferguson, S.; Morris, G.; Hao, H.; Barrett, M.; Glennon, B. Characterization of the Anti-Solvent Batch, Plug Flow and MSMPR Crystallization of Benzoic Acid. Chem. Eng. Sci. 2013, 104, 44–54. https://doi.org/10.1016/j.ces.2013.09.006.(2) Oliva, J. A.; Pal, K.; Barton, A.; Firth, P.; Nagy, Z. K. Experimental Investigation of the Effect of Scale-up on Mixing Efficiency in Oscillatory Flow Baffled Reactors (OFBR) Using Principal Component Based Image Analysis as a Novel Noninvasive Residence Time Distribution Measurement Approach. Chem. Eng. J. 2018, 351 (June), 498–505. https://doi.org/10.1016/j.cej.2018.06.029.(3) Pena, R.; Oliva, J. A.; Burcham, C. L.; Jarmer, D. J.; Nagy, Z. K. Process Intensification through Continuous Spherical Crystallization Using an Oscillatory Flow Baffled Crystallizer. Cryst. Growth Des. 2017, 17 (9), 4776–4784. https://doi.org/10.1021/acs.cgd.7b00731.(4) Wood, B.; Girard, K. P.; Polster, C. S.; Croker, D. M. Progress to Date in the Design and Operation of Continuous Crystallization Processes for Pharmaceutical Applications. Org. Process Res. Dev. 2019, 23, 122–144. https://doi.org/10.1021/acs.oprd.8b00319.",6,1.0
"Pervaporation membranes can be used to avoid azeotropic distillation, enable flow chemistry technology, degas organic solvents, dehydrate solvents, drive reactions towards product formation, and more. Compact Membrane Systems (CMS) has demonstrated the effectiveness of their fluoropolymer pervaporation membranes to degas and dewater organic solvents for the pharmaceutical and specialty chemical industries in recent years. These membranes are applicable to a wide variety of solvents and operating conditions including difficult solvents such as NMP, DMSO or THF. In this talk, CMS will describe their newest pervaporation case study, performed in collaboration with Snapdragon Chemistry. A CMS membrane was used to selectively remove ethylene from a ring closing metathesis reaction in toluene to drive product formation and enable the use of flow chemistry.Using traditional batch techniques, implementing olefin metathesis reactions on the commercial scale has proven challenging, if not impossible. Ring closing reaction mechanisms are commonly used during the preparation of small molecules for the pharmaceutical and fragrance industries. Membranes are a modular, scalable solution that can continuously remove the ethylene byproduct and enable the use of commercial scale continuous ring closing metathesis. This case study details the advantage of using a sheet-in-frame membrane reactor as opposed to a stainless steel tubular reactor. The fluoropolymer membrane has excellent chemical and thermal stability which allows for an extended range of thermal conditions without sacrificing selectivity or flux. Pure ethylene flux of the membrane was also investigated to ensure that the mass transport across the membrane would always be greater than the reaction’s ethylene production rate. Scale up considerations including residence time, required ethylene removal rate and module design will be addressed. The feasibility of using a membrane reactor to remove byproducts shown in this case study is only one example of the reactions that could benefit from the implementation of membrane reactors.",6,2.0
"In past decade there has been significant interest in the development of continuous processes in the pharmaceutical industry. Key drivers for continuous processing are product quality improvement, yield and process robustness, footprint reduction of processing area and increased process safety. Hydrogenations are commonly used in synthesis of pharmaceutical intermediates and active pharmaceutical ingredients. Types of hydrogenations include removal of protecting groups such as benzyl groups, reduction of nitro compounds and asymmetric hydrogenations. In this work we have demonstrated:Application of a laboratory scale tubular reactor for development and scale-up of a continuous asymmetric hydrogenations process.The effect of operating parameters such as pressure, gas to liquid ratio, temperature, catalyst loading and residence time of the substrate on enantioselectivity and product quality, are studied to develop a scalable and robust process.A ‘Fit for purpose’ model was developed and found to be very effective in efficiently identifying the optimal reaction conditions, assessing risk to product CQAs, and increasing overall robustness.Disclosures: All authors are employees of AbbVie and may own AbbVie stock. AbbVie sponsored and funded the study; contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final publication.",6,3.0
"Peptides, an important class of polymers, mainly used for therapeutic purposes, are generally synthesised using solid phase methods. While the rapid synthesis characteristic of solid phase peptide synthesis (SPPS) benefits the drug discovery, this technology faces challenges for large scale synthesis, where its non-quantitative coupling leads to error sequences. In contrast liquid phase peptide synthesis (LPPS), offers high peptide purity and scalability but its development is hampered by inefficient intermediate separations.Liquid phase peptide synthesis via nanostar-sieving (LPPS-NS) is a platform which synthesises peptides in solution with facile intermediate separations. Amino acids (AA) are coupled iteratively onto a 3-armed, star-shaped macromolecule, forming peptide-nanostar intermediates. After coupling, the unreacted AA is quenched and subsequently proceeded to N-terminal deprotection. The bulky intermediates are then ‘sieved’ out from the debris and quenched AA all together via organic solvent nanofiltration (OSN) thus omitting the post-coupling isolation step. This synthetic cycle is repeated until the desired peptide length is achieved, as shown in Figure 1. Standard Fmoc peptide chemistry is applied throughout the synthetic cycle. The use of nanostar greatly enhances the molecular sieving efficiency by the >3-fold mass difference between the nanostar and the unreacted building blocks. Most importantly, real-time reaction monitoring can be undertaken by LC-MS with high accuracy because of the monodispersity of the nanostar. OSN plays a pivotal role in synthesis efficiency. A solvent resistant membrane made of crosslinked polybenzimidazole (PBI) polymer was chosen for OSN. The polymer-based membrane was proven to be durable and to have a high separation factor which remained consistent throughout many synthesis cycles. We speculate that this new concept of LPPS is a truly one-pot synthesis where no transfer of liquids between cycles occurs and has a good potential for full-automation.In this work we demonstrate the successful synthesis of Enkephalin-type model peptides (~5-10 mers) via nanostar-sieving technology. The products are of higher purity than, or at least comparable to, peptides produced by a reliable solid phase vendor, while using less equivalents of AA during coupling. To further validate this technology, high purity linear Octreotide (8AA) was synthesised. Our ambition is to develop this platform into a robust technology for large-scale, fully automated and high purity peptide synthesis.",6,4.0
"Extraction and separation operations are present in most API processes, in different stages, and with different purposes. Both unit operations are traditionally carried out under batch mode in multi-purpose reactors and consist in two/three sequential steps of mixing immiscible liquids with different densities and then separate them by gravity, which results in a time-consuming stage. Moreover, sometimes emulsions are obtained leading to difficulties in phase separation with impact in cycle time, yield and, potentially, product quality. Another bottleneck from these operations is related to the reactor occupancy at this stage, considering that usually one phase is carried forward and the other is discarded, which results in a limitation of batch size and production capacity.To tackle all these operation deficiencies, continuous manufacturing is an attractive approach. The main drivers associated when going from batch to continuous highlight the opportunity to improve reproducibility, capacity, faster development, among others. For extraction and separation processes, several alternatives are available in the market with different characteristics and advantages.In this work, we demonstrate our approach in selecting technologies for continuous process of extraction and separation, and case studies with different purposes such as pH adjustments, product/API transfer between phases and washings. The examples will show characteristics of the selected technologies (membrane based separator and centrifugal extractor) and types of liquid-liquid extraction and separation process evaluated along with the impacts resulting from going from batch to continuous manufacturing.",6,5.0
"Continuous manufacturing (CM) offers significant advantages over traditional batch manufacturing for chemistries that generate or require high energy (usually in the form of heat). The comparatively large surface-area-to-volume ratio of flow reactors makes them particularly attractive for chemical reactions such as lithiation-carboxylation, nitro reduction, debenzylation, and thermal cyclization. Continuous processing also allows the reaction rates for these transformations to be orders of magnitude higher than would be possible with batch manufacturing. Additionally, performing these reactions in flow allows them to be telescoped to reduce operator handling and processing downtime.This presentation will focus on development and process intensification of a three step continuous flow process toward synthesis of an active pharmaceutical ingredient (API). It will detail how construction of fit-for-purpose continuous reactors improves safety, cost, and process mass intensity while enabling use of preferred reagents and unlocking otherwise unattainable operating conditions.",6,6.0
"Beta-lactam antibiotics have been used extensively to treat bacterial infections for nearly a century. The two largest produced beta-lactam antibiotic classes, semi-synthetic penicillins (e.g. amoxicillin) and cephalosporins (e.g. cephalexin), constitute the largest amount of sales of antibiotics for the treatment of bacterial infections. Currently the industrial production of beta-lactam antibiotics is conducted largely via batchwise chemical synthetic pathways consisting of multiple reaction steps, use of protecting groups, harsh solvents, and very low (-40°C) temperatures. Using these methods, the chemical synthesis route produces a large amount of chemical waste with an E factor of roughly 50 and is highly energy intensive due to the operation of low temperature refrigeration equipment. Additionally, due to increases in labor and raw material costs, much of the production of beta-lactam antibiotics has moved to China and India [1]. Considering recent events regarding COVID-19, the disruption international supply lines could lead to widespread shortages of highly important antibiotics in Western countries [2]. Improving the economic viability and environmental sustainability of beta-lactam antibiotic production is paramount to globalizing the production of these important drugs to decrease the chance of shortages.Recently, we have been working to develop a continuous enzymatic beta-lactam antibiotic production process to address many of the issues of the current batchwise chemical synthesis processes to produce cephalexin and amoxicillin. These drugs were selected because they are the two highest volume manufactured and prescribed antibiotics worldwide and have been included in the world health organization (WHO) list of essential medicines as key access antibiotics [3]. Next, the move from chemical synthesis to enzymatic synthesis decreases waste production and increases yield due to high enantioselectivity of enzymes and the ability to operate in a mild temperature aqueous environment. We have expressed, purified, and engineered penicillin G acylase from E. coli (EcPGA), an industrially relevant enzyme used to produce precursors for semi-synthetic beta-lactam antibiotics, to synthesize cephalexin and amoxicillin. Additionally, we have immobilized EcPGA on various solid supports via covalent attachment, allowing for the recycle of the biocatalyst and decreasing the overall cost of the catalyst for production. Next, the induction of crystallization of the API within the reaction vessel protects the desired product from hydrolysis by EcPGA into a minimally soluble byproduct. The in-situ crystallization of the API enables the downstream separation of the solid crystals from the mother liquor into a filtration, washing, and drying protocol.By combining these elements, we have conducted several successful seeded batch reactive crystallization experiments to produce solid cephalexin crystals. We found that after three crystallization experiments of three hours each, the catalyst suffered a 20% decrease in specific activity. This decrease may be due to crystallization within the pores of the solid catalyst, as the catalyst experienced no measurable deactivation under reaction conditions without crystallization. We have also studied the effect of different sized support particles as well as different support materials (e.g. agarose, methacrylate, polypropylene) on the diffusive properties of cephalexin and its precursors as well as on-line stability of the immobilized biocatalyst. Additionally, during reactive crystallization, after approximately four hours, the scarcely soluble byproduct, phenylglycine, generated enough supersaturation to precipitate and contaminate the solid phase. The progress of cephalexin crystallization and growth as well as phenylglycine precipitation were detected via in-situ microscopy and focused beam reflectance measurement (FBRM). We are currently working to implement these same elements and use these experiments to inform the design and operation of continuous MSMPR. The shift from batch to continuous production will improve the capacity and decrease the overall footprint of the production process as well as decrease the amount of off-specification product due to more robust control during steady state operation. The implementation of process analytical technology (PAT) such as Fourier transform infrared spectroscopy (FTIR) will allow for the on-line monitoring of the liquid phase components while FBRM and in-situ microscopy will allow for the on-line monitoring of the solid phase components. Additionally, reverse phase HPLC will be used to confirm final product purity. With the information of the components’ concentrations in both phases, we will study and develop a control strategy to maintain a “state-of-control” operation of the reactive crystallization process for the production of cephalexin and amoxicillin.    References:[1] Elander, R. P. (2003). Industrial production of β-lactam antibiotics. Applied Microbiology and Biotechnology, 61(5-6), 385–392. doi: 10.1007/s00253-003-1274-y[2] Oehler, R. L., & Gompf, S. G. (2020). Shortcomings in the US Pharmaceutical Supply Chain. Jama. doi: 10.1001/jama.2020.1634[3] WHO Model Lists of Essential Medicines. (n.d.). The SAGE Encyclopedia of Pharmacology and Society. doi: 10.4135/9781483349985.n433",7,0.0
"Islatravir is a nucleoside reverse transcriptase inhibitor small molecule API (active pharmaceutical ingredient) targeted for HIV PrEP (pre-exposure prophylaxis). Islatravir has multiple crystal forms and a complex phase diagram. This talk will focus on the conversion from the monohydrate to an anhydrous form. To produce anhydrous API, we have designed a slurry conversion process, during which the less stable monohydrate form dissolves and a more stable anhydrous form crystallizes simultaneously. The solution composition also comes into play, as the process requires operating within a specific window of water-activity for effective form conversion. The slurry conversion process is also designed to proceed in a controlled manner to reduce crystallization of another kinetically favored phase. In addition, Merck has recently published a novel biocatalytic synthetic scheme for the synthesis of Islatravir*. This presentation will discuss differences in anhydrous crystallization performance (morphology, particle size, and milling performance) when starting with API from a traditional chemical synthetic route vs. the novel biocatalytic route. In this talk, we discuss the factors we can manipulate to design the most robust process, ultimately producing anhydrous API with consistent properties across various batches.*Huffman et al. “Design of an in vitro biocatalytic cascade for the manufacture of islatravir.” Science, 2019, 366 (6470), 1255-1259.",7,1.0
"The recognition of the selective nature of biopharmaceutical drugs and the advances in biotechnology have led to over 246 approved products with cumulative revenues reaching $140 billion [1]. Upstream biomanufacture of proteins has advanced, such that the time and expense bottleneck in biopharmaceutics lies in the chromatographic purification steps [3-5]. Crystallisation is a more economic and energy efficient alternative purification step, producing crystalline proteins with higher purity and stability [3, 6]. However, this technology is yet to mature to manufacturing scale and robustness.Here, we report our optimised crystallisation, additive and seeding strategies to crystallise model proteins and demonstrate selective crystallisation of target proteins from a binary mixture.Mesoporous silica nanoparticles (NPs) are shown to be effective heterogenous nucleants for the selective crystallisation of lysozyme from a binary mixture of lysozyme and thaumatin. From hanging drop solution conditions which can crystallise both proteins, the ‘hard template’ SBA-15 NPs are found to not only promote the nucleation of lysozyme, but also suppress the crystallisation of thaumatin. These conditions are also effectively scaled up to the ml level. Further to this, the effectiveness of graphene oxides and DNA polymeric additives are also examined as seeds for protein crystallisation promotion.Here, we report on the use of several templating strategies for the crystallisation of proteins demonstrating the feasibility for selective crystallisation of proteins for downstream separations.ACKNOWLEDGEMENTThis study acknowledges by the SCoBiC project funded by the UK’s EPSRC (EP/N015916/1).ReferencesDemain, A.L., REVIEWS: The business of biotechnology. Industrial Biotechnology, 2007. 3(3): p. 269-283.Yang, H. and J.H. ter Horst, Crystal nucleation of small organic molecules, in New Perspectives on Mineral Nucleation and Growth. 2017, Springer. p. 317-337.Basu, S.K., et al., Protein crystals for the delivery of biopharmaceuticals. Expert opinion on biological therapy, 2004. 4(3): p. 301-317.Parambil, J.V., et al., Effects of Oscillatory Flow on the Nucleation and Crystallization of Insulin. Crystal Growth & Design, 2011. 11(10): p. 4353-4359.Delmas, T., M.M. Roberts, and J.Y. Heng, Nucleation and crystallization of lysozyme: Role of substrate surface chemistry and topography. Journal of Adhesion Science and Technology, 2011. 25(4-5): p. 357-366.Harrison, R.G., et al., Bioseparations science and engineering. 2015: Topics in Chemical Engineering.Shah, U.V., et al., Heterogeneous nucleants for crystallogenesis and bioseparation. Current Opinion in Chemical Engineering, 2015. 8: p. 69-75.Shah, U.V., D.R. Williams, and J.Y.Y. Heng, Selective Crystallization of Proteins Using Engineered Nanonucleants. Crystal Growth & Design, 2012. 12(3): p. 1362-1369.",7,2.0
"Over the last decade, bio-active peptides have emerged as the next generation of bio therapeutics with a rapidly growing demand [1]. A lack of cost-effective and reliable processes for downstream purification is a main industrial challenge for meeting the growing demand of bio-active peptides [2]. Crystallisation of peptides is a novel strategy to debottleneck downstream purification, offering a more economical and efficient technique than expensive traditional techniques [3].Due to their low solubility and large molecular size, one of the main challenges in peptide crystallisation is the lack of controllability of the nucleation. However, very little has been done in using templates for controlling nucleation to achieve a consistent and reproducible crystallisation process. Chen et al. recently reported that highly mesoporous inorganic surfaces, referred to as “hard templates”, can remarkably induce significantly faster and more controlled nucleation, even at low supersaturation [4]. In our recent work, we have successfully demonstrated a “soft templating” strategy by using soft, organic molecules, such as amino acids, to enhance nucleation. In buffer solution, amino acids form various interactions with peptides, whereby electrostatic interactions have been identified as most important. Therefore, amino acids that exhibit strong electrically charged residual groups for the relevant buffer conditions were investigated. In this presentation, the impact of types, amount and surface charge of the amino acids on the crystallisation performance of insulin at various supersaturation will be reported. With the aim of providing a strategy suitable for high-throughput peptide crystallisation, the possibility of scaling this templating strategy up, from the microlitres to the tens of millilitres, will be evaluated.Insulin was used as model peptide and comprises of two chains, A and B, with a combined 51 amino acids and an overall molecular weight of 5.8 kDa. The impact of amino acids on crystallisation is evaluated by means of measuring induction time, crystal yield, and size distribution as a function of time. Citric buffer with a pH between 5 and 7 was used with zinc sulphate as the precipitant. The impact of amino acids on crystallisation is evaluated by monitoring the peptide concentration with UV-vis spectroscopy every 20-30 minutes. Using this, the induction time and crystal yield are obtained. Furthermore, the pH is measured at the start and the end of the crystallisation. Images obtained with a microscope are analysed to evaluate the crystal sizes and shapes. The experiments were carried out in batch crystallisers with volumes up to 15 millilitres to demonstrate the scalability.Our recent results show that amino acids reduce the induction time of insulin crystallisation by up to 40% when compared to the induction time with no amino acids. The faster decrease in insulin concentration when amino acids are added also implies an increase in the crystal growth rate. As amino acids accelerate the crystallisation process, a yield of 90% can be achieved in a shorter period of time in crystallisers with volumes of up to 15 millilitres. Preliminary investigations show that the type and amount of amino acid added have only a minor impact on solubility, leading to a slight increase. Instead of impacting the solubility significantly, the charged residual-groups of the added amino acids increase the pH significantly. This increase in pH leads to an increase in stability of the tertiary structure of insulin by altering its physio-chemical surface properties which result in a faster nucleation. As a result of the more controlled crystallisation with amino acids, a narrower crystal size distribution can be obtained while achieving the same mean crystal size (~20 µm) when compared to the mean crystal size obtained with no amino acids. The insulin crystals obtained with amino acids show a more uniform crystal size distribution with a smaller span for crystalliser volumes reaching from the scale of microliters to the tens of millilitres. The soft templating strategy could be successfully demonstrated for crystalliser volumes of up to 15 millilitres while identifying mixing as one of the main limiting factors of scaling-up.It has been demonstrated that amino acids as organic templates have the potential of enhancing the controllability of insulin crystallisation even at low supersaturations and to the scales of tens of millilitres. These includes a significant reduction of residence time needed in the crystalliser to obtain a desired crystal yield by maintaining a high level of consistency. Although different polar amino acids were investigated, the magnitude of advancing crystallisation differs dramatically between them as well as the ratio of amino acids concentration to insulin supersaturation.Bibliography  Lau, J.L. and M.K. Dunn, Therapeutic peptides: Historical perspectives, current development trends, and future directions. Bioorg Med Chem, 2018. 26(10): p. 2700-2707. Yang, H., et al., Development and Workflow of a Continuous Protein Crystallization Process: A Case of Lysozyme. Crystal Growth & Design, 2019. 19(2): p. 983-991. Roque, A.C.A., et al., Anything but Conventional Chromatography Approaches in Bioseparation. Biotechnol J, 2020. Chen, W., et al., High Protein-Loading Silica Template for Heterogeneous Protein Crystallization. Crystal Growth & Design, 2019. 20(2): p. 866-873.",7,3.0
"Sensors are great assets to quick and sensitive detection inside miniaturized devices were the amount of sample is small. Connecting an integrated microfluidic system to a control unit increases the accuracy of the experiments due to the automated adjustments of parameters. One of the examples for the potential application of a sensor integrated microfluidic system is screening crystalline materials where there is a high demand for accurate measurement of the concentration within the crystallization zone. In our previous studies, we developed a continuous microfluidic mixer for screening crystalline materials at the constant supersaturation, where the homogeneous concentration was calculated using COMSOL Multiphysics. However, it was not possible to perform an in-situ measurement for the actual concentration to experimentally verify that. Here we have coupled the microfluidic mixer design with an electrochemical sensor to enable in-situ measurement of the actual concentration inside the mixing zone. Moreover, integration of a sensor within the continuous microfluidic platform we can control the imposed supersaturation by manipulation of the flowrates. To validate the sensor integrated platform experimentally, we have done the protein crystallization of lysozyme and NaCl. Products of this crystallization process can exhibit different morphologies according to the ratio of the starting solutions. Several different experimental conditions are explored to study the morphologies obtained at assigned initial concentration as well as the growth kinetics.",7,4.0
"The solid state characteristics of an Active Pharmaceutical Ingredient (API) have great impact on the Drug Product (DP) performances and its formulation development approach.One Critical Quality Attribute (CQA) that has to be controlled in order to ensure the success of a given formulation strategy in terms of manufacturability is the API Particle Size Distribution (PSD). In this case study, the API (PSD) had to be controlled to produce a blend which was homogeneous and flowable once combined with the chosen excipients and, in parallel, to reduce the risk of segregation during the DP manufacturing process. The initial drug substance batch available for the formulation prototype production and selection showed a D(v,0.9) around 350mm. A mechanical PSD reduction via FitzMill was required in order to tune the drug substance PSD to match the PSD of the chosen excipients to ensure proper blend flowability and homogeneity attributes as well as to mitigate the risk of segregation during the encapsulation. As the alternative, the control of the PSD via process crystallization was investigated. Ultimately an anti-solvent mediated seeded crystallization process was developed. This process was able to deliver drug substance in the desired PSD range and avoided the need for post isolation milling.",7,5.0
"The spectrophotometric determinations of glucose in blood serum and urine are commonly used for diabetes testing in Honduras. However, the high cost of spectrophotometers often limits access to diabetes testing in this and other low-income countries. To make this testing more accessible, a highly affordable light-emitting diode (LED) spectrophotometer was designed, built, and tested for 3 years using best engineering practices and state-of-the-art electrical components. In addition, this LED spectrophotometer used a Raspberry Pi credit-card sized computer for a user interface, and for data processing and storage. The cost and performance of this LED spectrophotometer/Raspberry Pi system was compared against 3 common commercial spectrophotometers. More specifically, the performance of these instruments was evaluated from the upper limits of linear range, upper limits of operational range, calibration sensitivities, R2 values, estimated limits of detection, and percent calibration check standard recoveries for the determination of glucose using o-toluidine in glacial acetic acid. The parts for our LED spectrophotometer/Raspberry Pi system costs $168 United States dollars. The 3 commercial spectrophotometers ranged in price from $2,382 to $3,382. Our results suggest that the LED spectrophotometer/ Raspberry Pi system has upper limits of linear range, upper limits of operational range, calibration sensitivities, R2 values, estimated limits of detection, and percent calibration check standard recoveries that are generally comparable to those of the 3 commercial spectrophotometers used in this study. In conclusion, this LED spectrophotometer/Raspberry Pi system is highly affordable and could be used to make diabetes testing in Honduras and other low-income countries more widely accessible.",8,0.0
"We present the application of a newly developed technique, embedded droplet printing in yield-stress fluids (1), to the formation of uniform spherical pharmaceutical particles. Yield-stress fluids are rheologically complex materials that are solid-like below a critical stress but undergo a dramatic drop in viscosity and flow above their yield stress. Printing droplets that are embedded inside a yield-stress fluid enables their processing in the absence of exterior influences including convection and solid boundaries. This makes embedded droplet printing ideal for performing sensitive processes like pharmaceutical crystallization.In this study, we crystallize various drug-excipient systems using embedded droplet printing methods. We demonstrate the advantages of our technique in obtaining particles that are as spherical and uniform as possible, key targets in the manufacturing of pharmaceutical materials. The powders we produce show significant improvement over conventional materials in terms of their flowability and processability, and we believe embedded droplet printing can help pave the way for the manufacturing of more exquisite and targeted pharmaceutical products.A. Z. Nelson, B. Kundukad, W. K. Wong, S. A. Khan, P. S. Doyle, Embedded droplet printing in yield-stress fluids. Proc. Natl. Acad. Sci. 117, 5671–5679 (2020).",8,1.0
"Purpose:The temperature-shift spray-drying process enables high-throughput, commercially relevant manufacturing of poorly soluble active pharmaceutical ingredients (APIs) in organic solvents by increasing API solubility by at least 10-fold relative to room temperature operation. In the temperature-shift technique, an in-line tube-in-tube heat exchanger rapidly increases the temperature of a spray suspension to fully dissolve an API immediately before entering the spray dryer. API dissolution kinetics are a key factor when designing the process and optimizing operating parameters. The temperature and residence time must be selected to create an operating space to allow for full dissolution without causing API degradation. A first-principles model was developed to provide guidance on the heat and mass transfer requirements to aid in experimental design. Temperatures of the solution flowing through the heat exchanger were measured, and the model was found to accurately predict the heat transfer. Additional experiments were done with a model compound to assess the dissolution model and were also found to be in good agreement. This tool will help improve thermal shift spray drying process development and process parameter optimization to achieve the desired product quality more efficiently.  Methods:A scale-independent model to describe both heat and mass transfer was developed using first principles approaches. To develop a heat transfer model, we employed an energy balance across the heat exchanger and then evaluated typical heat transfer correlations and dimensionless numbers. These correlations from literature were found to be applicable to our physical situation. This approach allowed calculation of the temperature profile of both the hot and cold sides of the tube-in-tube heat exchanger as a function of length. Dissolution of an API was modeled using the Noyes Whitney Equation. By knowing the temperature profile of the heat exchanger, the concentration of dissolved drug was determined based on the changing solubility of the API as a function of temperature. Additionally, fundamental parameters like the diffusion coefficient and solution viscosity were also allowed to vary as a function of temperature leading to a good approximation of the physical properties of the suspension during dissolution in the heat exchanger. An experimental set-up was fabricated to measure the temperature of the inlet and outlets of both the hot and cold streams. Initial trials were completed testing ranges of temperatures, flowrates, and heat exchanger geometries using water as a test fluid. To further test the heat transfer model, placebo spray drying experiments using the dispersion polymer HPMCAS were conducted on a clinical scale dryer with a water-acetone mixture. HPMCAS was tested at two solids loadings to see how solution temperatures with different viscosities were predicted by the model. To evaluate the dissolution of an API through the heat exchanger, Riboflavin was used as a model compound. An in-line UV probe was used to determine the effective concentration in solution as a function of heat exchanger residence time across a range of flowrates. Results:The temperatures of the tube side are predicted with less than 5% error compared to experimental data spanning small scale feasibility to large commercial scale for water, solvent, and polymer solutions. Dissolution of Riboflavin in water was predicted with good accuracy over the flowrates tested at small scale. Conclusions:The model was found to accurately describe the heat and mass transfer for this novel rapid heating spray drying process. This can be used for better designing DOE studies to understand the operating space with respect to drug dissolution and degradation. Future work will focus on better understanding this sensitivity of API dissolution kinetics during the temperature shift process.",8,2.0
"Purpose: Amorphous solid dispersions can provide bioavailability enhancement to active pharmaceutical ingredients with low aqueous solubility, which enables oral delivery. A spray drying process is a scalable technique for manufacture of amorphous solid dispersions. Manufacturing spray dried dispersions (SDDs) provides opportunities for particle engineering to optimize quality attributes such as flowability, compactibility, dissolution rate and drug release profiles that can be essential to drug product development. One of the limiting factors that can impact the SDD formulation and process is a drug’s solubility in organic solvents. Low drug solubility in organic solvents can have several process and product impacts that can make an SDD challenging as a lead formulation technology, including: low throughput, large batch size and non-optimal powder properties (e.g. particle size and bulk density). One novel technique to overcome the organic solubility limitation is to take advantage of the temperature-shift solubility enhancement. Achieving rapid drug dissolution from short term heating of a spray drying solution requires consideration to the drug’s chemical stability and modification of the processing equipment to solubilize the drug in an inline heat exchanger followed by flash atomization, which utilizes vaporization and solvent cavitation for droplet formation. Here, we discuss the technical considerations of developing such a process for an SDD of Alectinib, a BCS class II oral dosage oncology drug.Methods: The solubility of Alectinib was first determined as a function of temperature. Two small-scale techniques were compared: 1) High temperature NMR, and 2) thermogravimetric analysis (TGA). The solubility improvement at the selected temperature was then investigated using a high pressure solution vessel with a sight glass to visually confirm solubility. Lastly, two internally-developed models were used to optimize a PSD-1 spray drying process: 1) an in-line heat exchanger model to tune flow rate and temperatures required for heat and mass transfer and 2) a spray drying thermodynamic model to predict drying gas inlet and outlet temperatures. The temperature-shift SDD was then characterized using DSC, XRPD and dissolution testing to compare the performance relative to crystalline drug. A control SDD was manufactured by a standard spray drying process using a dilute, ambient-temperature spray solution for further comparison.Results: High-throughput solubility screening using a TGA method at 25°C and 37°C showed that a 90/10 methanol/water solvent system provided the highest temperature-shift advantage with a thermodynamic solubility of 0.4wt% at 25°C and 0.5wt% at 37°C. High temperature NMR enabled solubility measurements up to 120°C and demonstrated an increase to 1.2wt% solubility at that temperature. Extrapolating the two-point TGA method to higher temperatures with the van’t Hoff relationship resulted in excellent agreement with the NMR solubility measurements in the anticipated operating range of 100-130°C, which was also confirmed by using a larger volume pressurized solution vessel. Improvements to the NMR solubility was also observed with the addition of dissolved HPMCAS-H. Successful manufacture of amorphous SDDs was achieved on a PSD-1 spray dryer through a model-based process optimization strategy and demonstrated a 6x increase in API throughput using the temperature-shift process. Dissolution in gastric and intestinal media with varying micelle concentrations showed significant dissolution rate, extent and AUC improvements against pure crystalline material. No performance difference was observed between SDD manufactured using the inline heating process when compared to a dilute solution sprayed at room temperatureConclusion: Utilizing the described temperature-shift process with a challenging low organic solubility drug compound provided significant throughput improvements while reducing spray drying energy requirements by enabling higher spray solution concentrations and reducing batch sizes. Further application of this process can provide continued cost reduction in the spray drying manufacturing process while promoting the advancement of research in critical areas including atomization, drying kinetics and drug/polymer interactions.",8,3.0
"Under the best of circumstances, powder flow is difficult to control. Powders can bridge in hoppers, rat hole, avalanche, segregate, and adhere to surfaces through electrostatic or other interactions. Cohesive powders are especially challenging, and a significant body of research spanning many decades has been devoted to their study. These flow challenges are amplified in continuous pharmaceutical manufacturing processes where powders must not only be fed reliably, but also at precisely controlled feed rates. In these cases, feed screws are typically employed to meter powders at a desired mass flow rate. While the screw feeder design provides excellent results for many powders, they can fail to provide adequate results when feeding very low levels (mass flow rates) of cohesive powders or powders with very low bulk density. These low-level ingredients may include lubricants such as sodium stearyl fumarate and magnesium stearate, glidants such as silica, or various active pharmaceutical ingredients which are often milled to mean particle sizes below 20 microns. In this work, a novel continuous dispensing mechanism was explored to achieve continuous feeding of pharmaceutical powders. Initial results suggest that powders can be fed as low as 1/10th of the mass flow rate and with improved precision compared to twin feed screws.",8,4.0
"ABSTRACT: In this work, we explore the possibility of developing a quantitative and predictive model for chemical processes, without having to first gain a complete understanding of the underlying mechanisms. We find that this can be done efficiently combining: (1) expectations for the functional expression of the process—extracted from a general, qualitative understanding on how the chemical process ought to behave—and, (2) small data sets providing examples of the process outputs resulting from varying process inputs. The strategy discussed is labeled knowledge-constrained machine learning. The main example we take is on reaction kinetic models—and, here, the expectations are written in terms of constraints on the model predictions over time, and the data is a series of time-point measurements for the reaction outcome under different conditions. But, the intent of the presentation is to discuss a general methodology that may be used to develop predictive models across the range of unit operations found in the processes used to produce synthetic drug substance API’s.",9,0.0
"While continuous manufacturing (CM) has been employed for directly compressible tablets, the continuous wet granulation has also been used as an alternative in continuous tableting lines. To date, no matrix-based extended release tablets manufactured by a CM technology has been approved. Compared to batch granulation, continuous twin screw granulation exerts higher shear to produce denser granules with broader size distribution within very short time (seconds). Changing the screw configuration, process, and formulation parameters can alter the physical properties of the granules, e.g., shape, flow, compressibility, density, size distribution, etc. In twin screw granulation, complex interactions among these parameters manage the granule properties. This study aimed at identifying the critical material attributes and process parameters and their interactions to be controlled for switching a batch process to a CM granulation process.Using batch high shear granulation, we evaluated the formulation and process parameters that can produce metoprolol succinate extended release tablets. These formulation and process parameters were successfully transferred to continuous twin screw granulation to produce the same dissolution properties of the finished product. The effects of the screw configuration parameters on the characteristics of the granules were further investigated. The interaction between the processing, e.g., liquid to solid ratio, screw speed, barrel fill volume, etc., and the significant screw parameters was studies to unveil the processing space and to identify the failure modes.Similar granule quality attributes were obtained for most of the formulations; although, the conditioned bulk density and compressibility of the granules determined the dissolution properties of the tablets. The number of kneading and sizing elements had the most significant impact on granule characteristics. The effects of the staggering angle and distance between the kneading zone of the screws were negligible.This presentation will then highlight the processing space of screw granulation to produce extended release tablets. Some of the failure modes will be discussed along with the feasibility of implementing PAT tools to monitor the granulation process and granules properties.DISCLAIMERThis presentation reflects the views of the presenter and should not be construed to represent FDA’sviews or policies.",9,1.0
"Protein therapeutics is an important field of the pharmaceutical industry. Current challenges in this field include meeting the increasing market demands, handling the entrance of biosimilar products and complying with quality by design (QbD) principles [1-3]. To tackle these challenges effectively, efficient use of existing tools, methods and solution approaches together with the development of novel ones is necessitated. This work provides an overview of our recent efforts to address some of the aforementioned challenges using model-based dynamic optimization strategies.Model-based approaches are powerful tools towards facilitating process development and advancing process performance. They offer a promising alternative to costly and time-consuming experimentation [1], and can have a profound effect on process sustainability and profitability. Recent advances in cell culture modeling (with cell cultures representing a typical production platform for many of the products of this class) have enabled significant insight to the system’s dynamics and the underlying production mechanisms, and have paved the way towards developing dynamic optimization strategies for process intensification (cf., [4-6]).Existing numerical methods and optimization software for solving the resulting dynamic optimization problems rely almost exclusively on local techniques. Due to the noncovexity of the described complex biological systems, the vast majority of these problems exhibits the presence of multiple local minima. Nevertheless, deriving suboptimal local solutions can have direct quality and cost impacts. Despite the significant progress in deterministic global optimization [7], it is not yet tractable for the considered systems. Hence, alternative methods to overcome the hazards of suboptimal local optimality are required. Stochastic and heuristic approaches can be advantageous to this end. In our previous work [8], we have shown how incorporating biological process knowledge into the optimization problem formulation can facilitate the derivation of superior local solutions. In a similar direction, we illustrate how proper decomposition of the optimization problem can outweigh certain advantages [9].Beyond increasing product yield, monitoring and controlling product quality is essential. Regulatory authorities including FDA are increasingly recommending consideration of QbD to develop processes that consistently meet quality specifications [3, 10]. This requires further attempts in modeling, as well as reconsiderations in the formulation of the dynamic optimization problem to effectively account for direct quality attributes. Yet, current methods in this direction are still in an early stage. The recent work in [11] demonstrates a successful example of model-based optimization to maximize galactosylation (a glycosylation related metric, with glycosylation presenting a critical quality attribute for many therapeutic proteins [12, 13]) content for an antibody case study. In our recent study [14], we have combined model simulations and design of experiments to investigate the effects of nutrients and nucleotide sugars feeding in protein glycosylation. Building on this basis, we have developed an optimization framework to maximize process performance while meeting quality specifications, and thus provided a successful example for utilizing dynamic optimization to apply the QbD paradigm in protein therapeutics [15].Acknowledgements:This work has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement no.675251.References:[1] Farzan, P., Mistry, B., & Ierapetritou, M. G. (2017). Review of the important challenges and opportunities related to modeling of mammalian cell bioreactors. AIChE Journal, 63(2), 398-408.[2] Shukla, A. A., Wolfe, L. S., Mostafa, S. S., & Norman, C. (2017). Evolving trends in mAb production processes. Bioengineering & Translational Medicine, 2(1), 58-69.[3] Collins, P. C. (2018). Chemical engineering and the culmination of quality by design in pharmaceuticals. AIChE Journal, 64(5), 1502-1510.[4] Dhir, S., Morrow Jr, K. J., Rhinehart, R. R., & Wiesner, T. (2000). Dynamic optimization of hybridoma growth in a fed‐batch bioreactor. Biotechnology and Bioengineering, 67(2), 197-205.[5] Kiparissides, A., Pistikopoulos, E. N., & Mantalaris, A. (2015). On the model‐based optimization of secreting mammalian cell (GS‐NS0) cultures. Biotechnology and Bioengineering, 112(3), 536-548.[6] Kappatou, C. D., Mhamdi, A., Campano, A. Q., Mantalaris, A., & Mitsos, A. (2018). Model-based dynamic optimization of monoclonal antibodies production in semibatch operation—Use of reformulation techniques. Industrial & Engineering Chemistry Research, 57(30), 9915-9924.[7] Chachuat, B., Singer, A. B., & Barton, P. I. (2006). Global methods for dynamic optimization and mixed-integer dynamic optimization. Industrial & Engineering Chemistry Research, 45(25):8373–8392.[8] Kappatou, C. D., Mhamdi, A., Campano, A. Q., Mantalaris, A., & Mitsos, A. (2017). Dynamic optimization of the production of monoclonal antibodies in semi-batch operation. In Computer Aided Chemical Engineering (Vol. 40, pp. 2161-2166). Elsevier.[9] Kappatou, C. D., Altunok, O., Mhamdi, A., Mantalaris, A., & Mitsos, A. (2019). Sequential and Simultaneous Optimization Strategies for Increased Production of Monoclonal Antibodies. In Computer Aided Chemical Engineering (Vol. 46, pp. 1021-1026). Elsevier.[10] Pais, D. A., Carrondo, M. J., Alves, P. M., & Teixeira, A. P. (2014). Towards real-time monitoring of therapeutic protein quality in mammalian cell processes. Current Opinion in Biotechnology, 30, 161-167.[11] Kotidis, P., Jedrzejewski, P., Sou, S. N., Sellick, C., Polizzi, K., del Val, I. J., & Kontoravdi, C. (2019). Model‐based optimization of antibody galactosylation in CHO cell culture. Biotechnology and Bioengineering, 116(7), 1612-1626.[12] Hossler, P., Khattak, S. F., & Li, Z. J. (2009). Optimal and consistent protein glycosylation in mammalian cell culture. Glycobiology, 19(9), 936-949.[13] Blondeel, E. J., & Aucoin, M. G. (2018). Supplementing glycosylation: A review of applying nucleotide-sugar precursors to growth medium to affect therapeutic recombinant protein glycoform distributions. Biotechnology Advances, 36(5), 1505-1523.[14] Ehsani, A., Kappatou, C. D., Mhamdi, A., Mitsos, A., Schuppert, A., & Niedenfuehr, S. (2019). Towards model-based optimization for quality by design in biotherapeutics production. In Computer Aided Chemical Engineering (Vol. 46, pp. 25-30). Elsevier.[15] Kappatou, C. D., Ehsani, A., Niedenführ, S., Mhamdi, A., Schuppert, A., & Mitsos, A. Quality-targeting dynamic optimization of monoclonal antibody production. (Submitted)",9,2.0
"Personalized medicine is gaining a lot of attention in the pharmaceutical industry as it can treat diseases that earlier had no cure. In the past few years, we witnessed exponential growth in the development of cell and gene therapy products. Cancer Research Institute reported that 753 cancer cell therapies are currently under development all over the world, of which 375 are in clinical trials [1]. There has been an increase in the growth and demand for autologous therapy due to the recent approval from the FDA [2]. Therefore, autologous therapy is undergoing the transformation from laboratory to commercial scale.The autologous therapy starts with a collection of cells from the patient, which are cryopreserved and sent to the manufacturing site. The cells are then formulated and enriched. The formulated cells are again cryopreserved and transferred back to the patient, where they are injected back. Unlike traditional pharmaceutical drugs that are produced in bulk, these drugs are patient-centric and have to be manufactured for each patient separately. In addition, the process also deals with living cells that go around different nodes in the supply chain. Therefore, the cells have to be cryopreserved in storage device and transported across the supply chain.An integrated approach is considered for supply chain network design and inventory management of the raw materials. We consider a network with a set of hospitals where the patients arrive. The facilities to be set up are distribution centers and manufacturing centers. The manufacturing center receives the samples from the patient and engineers the cells to create the drug. The distribution centers have the inventory for the storage containers and are installed for the intermediate halt for the sample transportation between the hospital and manufacturing center. To account for the transit times for the material transfer between facilities, we define the parameter accounting for both transportation time and waiting time for different transportation modes as well as the processing times at each of the node. We also simultaneously optimize inventory policy. We use periodic-review policy for the raw materials inventory at the distribution center [3]. Under this policy, if the inventory is below the defined level, the order is placed to restore the inventory to level . Since the storage devices are reusable, the policy is modified to account for the returning devices after the delivery of the drug.Given the demand distribution, the potential locations of distribution and manufacturing centers, costs and time for processing, inventory, and transportation, we determine the location of different centers, inventory levels at the distribution centers, response time, and cost of delivery for each sample, with objective function of minimizing the cost and response time. The problem is formulated as a single objective optimization where response time is multiplied by a factor and added to the cost function. A case study of China is considered, with 20 hospitals situated in different provinces and a total of 130 patients over a period of 1 month. Also, the location of 20 potential locations of the distribution centers and 20 manufacturing centers are given. We found out that the majority of the cost is incurred in setting up manufacturing and distribution centers. The cost accounting for the response time depends on the multiplicative factor we choose. In addition, we also explored the importance of batching the samples at the distribution centers and transporting to the manufacturing center subjected to the response time constraint. Decisions related to inventory of raw material are critical to have an efficient supply chain. Also, sensitivity analysis is conducted to study the effect of different parameters on the optimal network design and overall cost.References:Tang, J., et al., The global landscape of cancer cell therapy. Nat Rev Drug Discov, 2018. 17(7): p. 465-466.Bach, P.B., S.A. Giralt, and L.B. Saltz, FDA approval of tisagenlecleucel: promise and complexities of a $475 000 cancer drug. Jama, 2017. 318(19): p. 1861-1862.Brunaud, B., et al., Inventory policies and safety stock optimization for supply chain planning. AIChE Journal, 2019. 65(1): p. 99-112.",9,3.0
"As the demand for biomass and pharmaceutical materials is increasing in the market, various genetically modified cells are tested in a batch bioreactor in order to increase production rate [1]. At the same time, a robust and efficient control strategy is required to successfully produce the product with uncertainty of cellular behaviors [2]. However, limitation of the information on the system dynamics makes it challenging problems to optimize the operating conditions of a bioreactor and to implement an automatic controller. In general, the biosystem is less reproducible and shows higher nonlinearity compared to other engineering systems. In particular, most of the parameters in the first-principle model are lumped type and these values would be varied not only in a single batch operation with changes of the states but in a batch-to-batch fashion. Therefore, implementing an automatic control to track the optimal control trajectories obtained off-line is not a robust approach to produce fine products for a large number of batch reactors.In this work, we propose a two-stage control strategy to optimally control a fed-batch type bioreactor that produces penicillin. The proposed strategy utilizes model-based Reinforcement Learning (RL) off-line to optimize the operating trajectories of state and input. In on-line, the Moving Horizon Estimation (MHE) is applied to estimate the parameter values, and an adaptive Model Predictive Control (MPC) is utilized to simultaneously modify and track the trajectories obtained in off-line. First, the first-principle model of the system dynamics is represented in Stochastic Differential Equations (SDEs) form which could theoretically handle the white noise in a continuous system. In addition, the variance of the process noise could shift a mean value of drifting terms of the system so that the changes in parameter values could be presented by adjusting the variance of the process noise [3]. These SDEs are numerically integrated and served as a virtual plant. The next step is obtaining the operating trajectories of the reactor off-line. The optimal control method could be classified in three classes, direct, indirect and RL (dynamic programming-based method). The direct method is the one mostly applied in practice since the problem formulation and the procedure of obtaining the solution is intuitive. On the other hand, RL has several advantages over the direct method. RL provides a closed-loop solution of the optimal control problem so that the control policy could be obtained instead of just a control trajectory [4]. This is a more robust approach since RL evaluates the control cost for the case where the states have deviated from the optimal trajectories. In addition, value based RL provides not only the optimal trajectories but also the value of state which is an expected total cost of the control policy starting from the state [4]. Although this value is just a by-product to find the optimal control policy, it is useful information when the trajectories are modified on-line.The trajectories suggested by RL could serve as a baseline of optimal control. However, due to the change in parameter values and disturbances, tracking those trajectories might not be feasible which requires a modification of trajectories. For this purpose, MHE and adaptive MPC could be utilized to estimate the parameters and modify the trajectories on-line. However, even though changes in parameter values are successfully estimated by MHE, the modification of trajectories by MPC for the entire horizon could not be feasible by the lack of computational time. This forces MPC to solve the optimal control problem with a limited length of prediction horizon where the terminal state of the bioreactor is not considered. This becomes a problem because most of the important cost is imposed only at the terminal state of the bioreactor. Therefore, providing a proper terminal cost of MPC becomes a crucial point to properly modify the trajectories. At this point, the value calculated from the RL could be used as the terminal cost of MPC. Although this value would not be exact by the changes of parameter values, the simulation results suggest that this approximation is valid for MPC to stably calculate the new trajectories.The key to this combination of MPC and RL is selecting the proper length of the prediction horizon. Under this scheme, the length of the prediction horizon is not only determined by the computational limits, but it is related to the reliability of the parameter values. The long prediction horizon implies that MPC is more rely on the newly estimated parameter values than the one used for RL. The presence of the parameter changes is one support on using the longer prediction horizon. However, the reliability of the estimated parameter values is related to both the quality and quantity of measurement data. Therefore, the choice of prediction horizon length should be carefully determined by evaluating the reliability of the parameter values. We propose one method for evaluating such reliability and propose a method of adjusting the prediction horizon length.[1] C. Larroche, Sanromán Maria Ángeles, G. Du, and A. Pandey, Current developments in biotechnology and bioengineering. Amsterdam: Elsevier, 2017.[2] Shuler and Kargi, Bioprocess engineering. United States: Academic Internet Publishers, 2007.[3] H.-H. Kuo, Introduction to stochastic integration. New York: Springer, 2006.[4] R. S. Sutton, Reinforcement learning: an introduction. Cambridge, Mass: MIT Press, 2018.",9,4.0
"In January 2020, AstraZeneca announced its ambition to eliminate greenhouse gas emissions from its sites and fleet by 2025 and to become carbon negative across the entire value chain by 2030. The development and manufacture of active pharmaceutical ingredients (API) contributes significantly to AstraZeneca’s carbon footprint and process engineers have a responsibility to play a leading role in achieving the zero-carbon target.This presentation will share the approach taken to implement a strategy for measuring and reducing environmental impact during the development and manufacture of APIs. This approach comprises of influencing ways of working during API process development, implementing robust measurement of process environmental impact and identifying opportunities for innovation through science and technology to minimise AZ’s carbon footprint.This approach describes developing a procedure to appropriately consider sustainability alongside other key drivers for process design such as product quality, manufacturability and regulatory factors. Similarly, creating a culture to deliver continuous improvement against established metrics for sustainability in every aspect of API development and manufacturing.This case also describes initiation of novel technology investigations to significantly reduce the carbon footprint of AstraZeneca’s API manufacturing processes. This will be a leading factor in achieving a 50% reduction in carbon footprint for API development and the zero-carbon ambition of the company.",9,5.0
"Continuous powder mixing technology application during continuous direct compression process has emerged as a leading technology used in the development and manufacture of solid oral dosage forms [1, 2]. The critical quality attributes (CQAs) of the final product is heavily depending on the performance of the mixing step as the quality of mixing will directly influences the content uniformity of the tablet. Therefore, the aim of this study was to investigate the impact of blend properties (bulk density, API sizes) and process parameters (process throughput, hold up mass and impeller speed) on the mixing performance. Mixing performance was characterized using the residence time distribution (RTD) of the process, which has been broadly used to characterize unit operations in pharmaceutical processes. In this work, the RTDs for a vertical continuous mixing device termed Continuous Mixing Technology (CMT) were obtained in order to evaluate mixing performance over a defined operating space. The RTDs were determined using a spike injection of tracer material into the mixer and measuring the concentration of the tracer in the outgoing material. This was achieved using a PAT interface situated at the exit of the CMT that presents the material to a near-infrared (NIR) probe. The theoretical residence time of the mixer is given by the hold-up mass divided by the throughput [1], and for each experiment, NIR spectra of the exiting material was measured for five theoretical residence times after tracer injection to obtain the full RTD.As the CMT contains an upper de-lumping screen and a lower mixing chamber, the residence time distributions of this system can be described as two CSTRs in series with different residence times [1] where the parameter ‘r’ describes the ratio between these residence times. The best value of ‘r’ for all experiments was determined through simultaneous optimization of all measured RTDs. The results showed that the CMT operates close to a single CSTR over the whole operating space, has good ability to dampen the fluctuations from the gravimetric feeders, and that the mixing performance of the CMT is not significantly impacted by blend properties (i.e. bulk density and API particle size).References:[1] Blackwood, D.O., Bonnassieux, A. and Cogoni, G. Continuous Direct Compression Using Portable Continuous Miniature Modular & Manufacturing (PCM&M). In Chemical Engineering in the Pharmaceutical Industry: Drug Product Design, Development, and Modeling, 2nd Edition, am Ende, T., am Ende, D. J., Wiley, 2019, pp 547-560.[2] Markarian, J. (2 April 2018). Modernizing Pharma Manufacturing. www.pharmtech.com",10,0.0
"INTRODUCTIONA commonly used technique for the production of pharmaceutical products is hot melt extrusion on a co-rotating intermeshing twin screw extruder [1]. An active pharmaceutical ingredient is embedded in a molten polymeric carrier and the product is conveyed to the die [2]. The extruder screws have a modular design with, for example, conveying, kneading or mixing elements. Each element has specific properties in order to combine different unit operations in one process unit. The extrusion process is mainly influenced by the chosen screw configuration and the geometry of each screw element. [3]Conveying elements are, depending on their pitch, responsible for the pressure build-up and the conveying of the material to the die. Kneading elements are mainly used in the melting zone because they introduce shear forces into the material according to the staggering angle of the kneading discs. Mixing elements are responsible for distributive and dispersive mixing. [4] The different types of elements can be seen in Figure 1. Each specific screw element can be characterized by dimensionless numbers regarding the pressure and power characteristic (Figure 2). These parameters are linearly related to the dimensionless volume flow. The intersection points with the axes result in the dimensionless numbers A1 and A2 for the pressure build-up and B1 and B2 for the power input into the material. [6] All parameters mentioned affect the residence time distribution, which indicates how long the material is exposed to the process conditions. For this reason, especially this parameter is of crucial importance for the manufacturing of pharmaceutical products with a constant product quality. Furthermore, it requires process understanding and the prediction of important process parameters. [7] Based on these challenges, the modelling has been gaining substantial interest for pharmaceutical applications within the last decade [8]. Especially the 3D simulation has moved into the focus of development due to its resolution in all special directions and its closeness to reality. Currently, it is not possible to calculate complete screw configurations of a production extruder in a reasonable time because of the high computational effort. The high amount of time is caused by the use of dynamic meshes. [9]The aim of this work is the implementation of a 3D simulation in Ansys Fluent with the possibility to calculate a complete extruder screw configuration in a reasonable time. Furthermore, the impact of abrasion of the screw elements, which is unavoidable by the daily use in the production process, on important parameters for the extrusion process should be investigated on the basis of the explained dimensionless numbers. In addition to these correlations, the residence time distribution will be used as an important process parameter to characterize the screw elements. For this purpose, a dimensionless relationship analogous to the pressure and power characteristics will be derived. METHODS AND MATERIALSThe simulation was performed in Ansys Fluent based on the Navier Stokes equations. A stationary, laminar and isothermal flow with an incompressible Newtonian fluid was assumed. Normally, a dynamic mesh is used for an exact representation of the screw rotation. In this case, however, an apparent movement using the method of pulse transmission was implemented. In a first step conveying elements with different pitches (20, 30, 40 mm) were constructed in Solid Works and then imported to Ansys. For the determination of the dimensionless pressure and power characteristics, the mass flow, the pressure difference across the screw element and the torque, which can be converted into a power, were evaluated. In order to investigate the effect of abrasion, screw elements currently used in the laboratory were compared with new elements and those specified by Leistritz as the lower limit. The abrasion of the barrel was not considered.The simulation was validated by extrusion experiments performed on a twin screw extruder (ZSE27 MAXX, Leistritz Extrusionstechnik, Nuremberg, Germany) with a screw diameter of 27 mm and a customized length of five barrel elements. Only the last two elements (8D) before the die were part of the extrusion process to avoid disturbances. A silicon oil with a viscosity of 85 Pa s (Bluesil FLD 47V100000, Brenntag GmbH, Duisburg, Germany) was extruded. The pressure difference was measured with two pressure sensors (S-11, WIKA Alexander Wiegand SE & Co. KG, Klingenberg, Germany) with a distance of 30 mm. The mass flow was measured with a scale (MSE2203S-100-DR, Satorius, Göttingen, Germany) and the torque was automatically recorded by the extruder. The screw speed was set at 50 rpm and the mass flow was varied by using different die diameters (2, 4, 6 mm). For the determination of the residence time distribution a passive scalar tracer was placed on the stationary solution of the fluid field simulation. The discretization was performed with the second order upwind method. Experiments to validate the simulation were carried out in analogy to the measurements for the dimensionless numbers. A tracer (Eosin, Merck, Darmstadt, Germany) was added at the material inlet and the resulted signal was detected at the die with an inline UV/Vis-Spectrometer (InspectroX, ColVisTec AG, Berlin, Germany). From the obtained residence time distribution, the Bodenstein number was calculated, which indicates the ratio of convective flows to dispersive flows and is thus a measure of the remixing within the extruder. The computations were performed with an internal python script, which fits the residence time density functions based on the axial dispersion model. This number was plotted over the screw speed for different die diameters, in order to clearly determine the resulting residence time distribution for each screw element. By a convolution of the individual residence time distributions, the resulting residence time distribution for a desired screw configuration can be determined. RESULTSThe 3D simulation was successfully implemented in Ansys Fluent with the pulse transmission method. The dimensionless numbers regarding the pressure and power characteristics of the screw elements could be validated by extrusion experiments. Furthermore, the impact of the abrasion on the process parameters was investigated. The pressure and power characteristics were determined for conveying elements with different inner (Din) and outer (Dout) screw diameters. Table 1 shows the results for the dimensionless pressure numbers. New, unused screw elements (new), screw elements at the lower limit (old) and actually used screw elements (used) were compared. It can be seen that the gap between the screws and the barrel and the two screws have a great impact on the pressure build up during the extrusion process. The greater the abrasion of the elements, the less pressure build-up is possible. Next to the pressure build up, the gap width affects other process parameters, like the viscosity or the residence time distribution. For this reason, it should be also possible to predict the residence time for individual screw elements. The residence time density function determined with Ansys Fluent was used to determine the Bodenstein number and the mean residence time and plotted against screw speed for different die diameters (Figure 3). It is obviously that the Bodenstein number increases with increasing volume flow while the mean residence time decreases.CONCLUSIONIn this work a 3D simulation in Ansys Fluent was implemented with the aim of a shorter calculation time for the representation of the extrusion process. Dimensionless numbers were used to validate the simulation and to show the impact of abrasion of the behavior of screw elements during the process. The deviations may also be significant in relation to the application of models to predict important process parameters. One of the most important parameter is the residence time, which is therefore used as a further parameter for the characterization of screw elements. By determining the Bodenstein number and the mean residence time, it is possible to predict the residence time distribution of the entire process for the specific extrusion process. REFERENCESRepka, M., et al.; Applications of hot-melt extrusion for drug delivery, Expert Opin Drug Deliv, 5 (12), 2008Lang, B., McGinity, J.W., Williams, R.O.; Hot-melt extrusion - basic principles and pharmaceutical applications, Drug Dev Ind Pharm, 40 (9), 2014Crowley, M. M., et al.; Pharmaceutical applications of hot-melt extrusion. Drug Dev Ind Pharm, 33(9), 2007Gogos, C.G., Huiju, L., Wang, P.; Laminar Dispersive and Distributive Mixing with Dissolution and Applications to Hot-melt Extrusion, in: Douroumis, D. (Ed.), Hot-melt Extrusion: Pharmaceutical Applications, 2012Sarhangi Fard, A., et al.; Tools to Simulate Distributive Mixing in Twin-Screw Extruders, Macromol Theor Simul, 21 (4), 2012Eitzlmayr, A., et al.; Mechanistic modeling of modular co-rotating twin-screw extruders, Int J Pharm, 474 (1-2), 2014Wesholowski, J., Berghaus, A., Thommes, M.; Investigations Concerning the Residence Time Distribution of Twin-Screw-Extrusion Processes as Indicator for Inherent Mixing, Pharmaceutics, 10 (4), 2018Bochmann, E. S.; Gryczke, A.; Wagner, K. G.; Validation of Model-Based Melt Viscosity in Hot-Melt Extrusion Numerical Simulation, Pharmaceutics, 10 (3), 2018Bravo, V. L., Hrymak, A. N., Wright, J. D.; Numerical simulation of pressure and velocity profiles in kneading elements of a co-rotating twin screw extruder, Polym Eng Sci, 40 (2), 2004",10,1.0
"Drug product formulations typically contain low-percentage components, such as lubricants, glidants, disintegrants and sometimes drug substance. For continuous manufacturing these materials require relatively low mass feed rates. Low-end limitations of twin-screw feeders commonly used for cDC led to the investigation of a novel feeding device that uses brief, frequent pulses of compressed gas to temporarily fluidize and propel powder through a narrow, adjustable gap. For cDC relevance, the device was outfitted with an appropriately sized hopper, and additional modifications were made including vibration and agitation to disrupt powder bridging. A series of experiments were conducted with common low-percentage pharmaceutical excipients, and additionally with cohesive drug substance. An initial target of 60 grams per hour was identified which most materials could achieve, and some materials were able to be fed below 10 grams per hour. Preliminary results are promising; however, additional development of this technology is needed prior to integrating into a continuous manufacturing process train.",10,2.0
"Screw feeders serve as the critical first unit operation in a continuous manufacturing of drug product (CMDP) processes, influencing the mass flow rate of active pharmaceutical ingredients and excipients downstream. There is great industrial interest modeling the behavior of screw feeders accurately. Existing flow sheet models focus on deterministically simulating the powder mass flow rate [2]–[4]; however, these models only capture the average mass flow rate, and lack the stochastic behavior of the mass flow. The use of custom Discrete Element Method (DEM) models provide highly realistic simulations of particles’ residence time and by extension mass flow, but prohibitively require long computation times to simulate only minutes of operation [5]. In the pursuit of designing better processes and improved controllers, there is a need for a computationally quick-to-solve flow sheet model that can simulate the stochastic nature of a real screw feeder. This is precisely the gap addressed in the present paper.This work presents the novel characterization and modeling of the stochastic nature of a screw feeder’s mass flow rate using statistical time series analysis and a deterministic flowsheet model. First, a battery of experiments was performed using different powders and a variety of operating speeds on a Coperion K-tron KT20 screw feeder. Then, for each experiment, the parameters of a hybrid mechanistic-empirical screw feeder model, based upon the work of [4], were estimated. After, the stochastic element of the mass flow is isolated by subtracting the flowsheet model’s deterministic mass flow rate from the feeder-reported instantaneous mass flow rate. Next, each experiment had an autoregressive moving average model (ARMA) [6] fit to its stochastic remainder, characterizing its mass flow variation. Finally, the set of ARMA models was used to develop a predictive model, relating the flow rate stochasticity to powder properties and operating conditions. This predictive model was integrated into the current deterministic feeder model, yielding a novel hybrid mechanistic-empirical-stochastic flow sheet model that simulates a realistic, high variance mass flow rate and is suitable for the development of CMDP processes and controllers.[1] S. García-Muñoz, A. Butterbaugh, I. Leavesley, L. F. Manley, D. Slade, and S. Bermingham, “A flowsheet model for the development of a continuous process for pharmaceutical tablets: An industrial perspective,” AIChE Journal, vol. 64, pp. 511–525, 2017.[2] Y. Yu, “Theoretical modelling and experimental investigation of the performance of screw feeders,” PhD thesis, 1997.[3] F. Boukouvala, V. Niotis, R. Ramachandran, F. J. Muzzio, and M. G. Ierapetritou, “An integrated approach for dynamic flowsheet modeling and sensitivity analysis of a continuous tablet manufacturing process,” Computers & Chemical Engineering, vol. 42, pp. 30–47, 2012.[4] D. Bascone, F. Galvanin, N. Shah, and S. Garcìa-Muñoz, “A hybrid mechanistic-empirical approach to the modelling of twin screw feeders for continuous tablet manufacturing,” Industrial & Engineering Chemistry Research, 2020.[5] P. Toson and J. G. Khinast, “Particle-level residence time data in a twin-screw feeder,” Data in brief, vol. 27, p. 104672, 2019.[6] G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: Forecasting and control. John Wiley & Sons, 2015.",10,3.0
"AbstractDimensionless analytical models for cylindrical pans under fixed bed conditions were developed that provided mechanistic relationships between material, equipment, and process parameters; and coating uniformity for development- and production-scale processes in batch and continuous operation. The batch coating model was verified with the exact analytical solution derived using the binomial distribution and was found to agree well with literature data. The models were then used to provide insights into formulation changes that resulted in higher coating uniformity for batch and continuous coating processes. A stochastic model was used to check the validity of the non-cylindrical pans and dynamic bed conditions. Both analytical and practical solutions for bridging coating performances between batch and continuous processes, and different formulations are presented. Extended AbstractThe main goal of a tablet coating process is to arrive at processing conditions that result in high-quality, evenly coated tablets. It follows that the control strategy for coating processes should be the coating uniformity endpoint but, apart from the drug-layering process, this is rarely done in the industry largely due to a lack of knowledge and/or the high cost of coating uniformity characterization. This work was undertaken to provide an analytical comparison for easier visualization of performance differences between batch and continuous coating processes, as well as between development-scale and production-scale processes for seamless technology transfers. This is achieved through derivations of dimensionless mechanistic models by applying simplification to a few process/equipment parameters provided in a coating uniformity simulation patent (Choi, 2009).Model DerivationStatistical and analytical coating uniformity models were derived based on the coating schematics shown in Figures 1 and 2 for the batch and continuous mode of operation, respectively. The models assume the same quantity of coating is received per pass through the coating zone and the probability of entering the coating zone per cycle is xc. This assumption proved to be a good approximation regardless of the shape of the coating distribution since all approach normal distribution with an increasing number of cycles according to the central limit theorem. The exact analytical models were derived by summing all probabilities of tablets passing through the coating zone. The resulting binomial distribution models for the batch and continuous (see Choi and Meisen, 1997) coating processes, respectively, were: Equation (see attachment) (1)And Equation (see attachment) (2)The statistical model for the batch coating schematic shown in Figure 1 was derived using the central limit theorem as follows: Equation (see attachment) (3)Substituting the geometric relationship for a cylindrical pan shown in Figure 3; assuming a static bed; and using the tablet wall velocity relationship given by Chen et al. (2010); Equation 3 could be expressed as Equation (see attachment) (4)where Equation (see attachment) (5)and Equation (see attachment) (6)ResultsThe exact analytical model (Equation 1) results were indistinguishable from those of the statistical model (Equation 3); therefore, Equations 1 and 3 are considered the same. The statistical model (Equation 4) agreed well with the Chen et al. data: the bed loading had the index of 0.58 compared with 0.5 in both Pandey et al. (2006) and Chen et al. (2010). The model predicted 3.85% compared to the experimental result of 3.8%ii when the pan speed was changed. The effect of tablet diameter could be matched exactly to the experimental data (Chen et al., 2010). The statistical model, by using a formulation coefficient for xc, also predicted the higher coating uniformity results seen in a high-solids-load coating formulation (Karan et al., 2019) for batch and continuous coating processes.To test the validity of the assumptions using static bed and cylindrical pan, stochastic simulations for a non-cylindrical pan and dynamic bed were conducted. Stochastic simulations of a 60” pan with 6” cone extensions on either side pan showed no discernable difference in coating uniformity from that of a cylindrical pan with the same diameter. The stochastic simulations showed 5% increase in coating uniformity for a 22% weight gain while the statistical model gave a 4% difference between the initial and the final bed size with 22% weight gain. Despite the simplicity of assumptions used, the models derived in this work agreed well with the literature data and stochastic simulations. ImplicationsThe implications of this work is that the analytical models can be used to select and monitor coating uniformity, typically expressed in relative standard deviation or Acceptance Value, as the critical quality attribute to ensure the desired performance in a manufacturing control strategy for both functional and non-functional tablet coating processes. For process bridging, the models can be used to calculate the relationship between different scales of equipment as well as between batch and continuous coating processes. Figure 5 shows an example of how the coating time needs to be adjusted with increasing pan diameter for the same relative bed loading and spray coverage. Figure 6 shows the characteristics of continuous coating design that is needed to match the coating uniformity of batch coating process.Symbolsdp tablet diameter, Sauter or surface-volume diameter usedD pan diameterFB coating distribution for the batch processFC coating distribution for the continuous processh bed heighti denotes the position in the number of cycles or the number of passes through the spray zonek coefficientn number of cyclesPi probability of iRSD relative standard deviationxc probability of tablet going through the spray zonexe exit probability of tablets in continuous processesΔ depth of the spray zoneφ pan rotational speedσ population standard deviationθ angle shown in Figure 1θΔ angle accounting for the thickness of the coating zone depthτ total coating timeReferencesChen, W., Chang, S-Y, Kiang, S., Marchut, A., Lyngberg, O., Wang, J., Rao, V., Desai, D., Stamato, H., and Early, W., “Modeling of Pan Coating Processes: Prediction of Tablet Content Uniformity and Determination of Critical Process Parameters”, J Pharm. Sci., 99 (7), July, 2010.Choi, M., “Pan Coating Simulation for Determining Tablet Coating Uniformity”, US Patent #10/981,812, December, 2009.Choi, M. and Meisen, A., “Sulfur Coating of Urea in Shallow Spouted Beds”, Chemical Engineering Science, v52, p1073 (1997).Karan, K., Hach, R., Lenick, L., During, T., Marjeram, J., Stevenson, G., Kuczynski, C., and Botnick, M. “Evaluation of Tablets Coated in a High Throughput Continuous Coater Using an Ultra-high Solids Film Coating System”, Poster T1330-08-56, presented at the Annual AAPS Meeting & Convention in San Antonio (2019).Pandey, P., Katakdaunde, M., and Turton, R., “Modeling Weight Variability in a Pan Coating Process Using Monte Carlo Simulations”, AAPS PharmSciTech, 7 (4), 2006.",10,4.0
"We present a process design method considering continuous and batch modes in the compounding unit for injectable manufacturing. The method can economically optimize the design variables such as the compounding mode, the lot size, and the number of filling needles. The method integrates two types of models. One is “process-oriented models” which captures the continuous nature of the process by residence time distribution (RTD), among others. The other is “discrete-oriented models” which describes the aspects such as failure events, operating shifts, and inventory by discrete and stochastic modeling. Know-how at the manufacturing shopfloors are fully used to estimate important model parameters such as the trend of failure events happening in the process. Net present value (NPV) is used as the objective function, considering pharma-specific costs such as change control and quality control. This economic evaluation model is an extension of our previously presented model for lot-sizing in the injectable manufacturing [1].We applied the this method in an industrial case study. Using the parameters obtained in an actual injectable manufacturing process, the design variable, especially the choice between continuous and batch compounding, could be optimized. Furthermore, a generalized map was also created by varying the specified parameters in the possible ranges of products, process, and the market characteristics. The map provided quantitative insights on the cases where continuos compounding could be economically advantageous than batch compounding. The result could be useful for both retrofitting and grassroot design for injectable manufacturing processes.[1] M. Yamada, S. Badr, S. Fukuda, M. Nakaya, Y. Yoshioka, H. Sugiyama, Journal of Pharmaceutical Innovation, accepted",10,5.0
"One of the major challenges in drug development nowadays is low water solubility (BCS classes II and IV). A widely used approach that had been proven successful in improving drug solubility is the formulation of amorphous solid dispersions (ASDs). Typically an ASD is composed of the poorly soluble drug and the hydrophilic polymer. If the combination is correct, the polymer propels the concentration of the poorly soluble drug into a supersaturated state during dissolution (spring effect). However, often the drug precipitates back into the solution, causing the concentration and thus bioavailability to drop. This issue can be circumvented by using a second polymer, which acts as a precipitation inhibitor (parachute effect). Yet, finding out the correct ratios in this tri-component system (drug: polymer 1: polymer 2) can often prove to be difficult.In this work, we produced multi-component solid dispersions by spray drying in an efficient manner. As a widely recognized model system, we chose cyan, magenta, and yellow (CMY) printer dyes and lactose as a carrier. We can conclude that the ratio between the components in certain collected batch (experimental values) can be different from the ratio between the components in the inlet feed (theoretical values) as the residence time of particles in the system can be greater than the collection period. We evaluated the cross-contamination of samples by UV-Vis spectroscopy. Across all 15 samples, which formed a ternary diagram, the average deviation from the theoretical compositions was approximately 1.7 %. The first part of the figure below shows all collected samples arranged in the ternary diagram, forming visually confirmable color gradients. In the second part, the results from UV-Vis spectroscopy are plotted into the ternary diagram. These results show that we can produce multicomponent solid dispersions efficiently without significant cross-contamination between samples.Acknowledgements:Authors acknowledge financial support by the Technology Agency of the Czech Republic, project number TJ02000095 (Zeta Program).",10,6.0
"The Buchwald-Hartwig cross-coupling has become one of the most important aryl-heteroatom bond connection strategies available to the pharmaceutical industry. Owing to high selectivity, mild processing conditions, and broad availability of palladium sources and ligands, Buchwald-Hartwig reactions are conceptually well-suited for late introduction into the GMP synthesis. Despite these advantages, poor predictability upon scale-up as well as complex and/or expensive options for palladium removal can combine to limit the overall use of such transformations in the synthetic route.This contribution focuses on control strategy development for a Buchwald-Hartwig cross-coupling introduced late in a drug substance synthetic route. Key to the control strategy was the control of the residual aryl halide process intermediate, process induced impurities, and palladium to single digit ppm levels in the drug substance. The talk outlines how automation and data-rich sampling were leveraged to enable reaction characterization. Informed by the data, a kinetic model was developed that incorporated both the catalytic cycle and mass transfer of the coordinated catalyst to the surface of the heterogeneous base. Once verified, the model was applied to determine critical process parameters and tested to predict potential failure modes of the process Critical Quality Attributes (CQAs). In addition, development of a homogeneous coordination and liquid-liquid extraction strategy for palladium removal will be discussed. Optimization of a liquid-phase approach afforded a palladium removal process with considerable cost and operational advantages relative to adsorption on functionalized silica. The overall control strategy for both the reaction and subsequent Pd removal was successfully transferred from the lab to full cGMP production scale to deliver more than 600 kg of drug substance.",11,0.0
"The process to make an active pharmaceutical ingredient (API) evolves as a project advances through clinical stages, requiring project teams to implement innovative control strategies throughout drug development to maintain API quality. An enabling process is typically used to generate small quantities of API quickly for early clinical trials, yet these enabling processes typically are not optimized and in some cases rely on costly chromatographic purifications to ensure the API meets quality requirements. As processes evolve and maturate, a common goal among project teams is to improve the overall process efficiency and scalability (e.g. obviate the need for chromatography) while producing high-quality API.In this presentation, we highlight the evolution of the control strategy employed for a complex small molecule API as it transitioned from an early-stage enabling process to a more commercially viable process. As part of this process development, a new synthetic route was implemented; we chose to use the penultimate step as the synthetic quality gatekeeper to mitigate any risk to the final API quality. There were substantial challenges to achieve comparable purity of the penultimate between the enabling and desired process. While the penultimate step of the enabling process consisted of simple nucleophilic deprotections using chromatographed input, the penultimate step of the intended long-term process consisted of a complex telescoped reaction (intramolecular phosphoramidate cyclization + 2 nucleophilic deprotections) with substantially less pure inputs. In an attempt to move away from chromatographic purifications, we designed and implemented a two-drop crystallization procedure to remove product-related impurities, obtaining solids with > 99 LCAP. Despite high purity, the potency was low due to impurities generated from the polymerization of a by-product of the cyclization reaction. Controlling these oligomers and polymers required two mitigations: extractive work-up to remove the precursor monomer and recrystallization to remove the polymer. Through this process development, we have realized the targets of a commercially viable process by increasing the overall yield 10-fold, reducing our process mass intensity (PMI) by 98%, removing all chromatographic purifications, and, critically, maintaining API quality requirements.",11,1.0
"Process and product understanding is at the root of manufacturers’ efforts to produce safe and effective medicines. The 2009 ICH and FDA Guidance for Industry Q8(R2) Pharmaceutical Development describes a quality by design (QbD) approach that “emphasizes product and process understanding and process control, based on sound science and quality risk management” (1). During the product development stage, the critical quality attributes required to ensure product quality are defined and their relationship to the critical process parameters which impact them are determined. Manufacturers can use a risk-based approach to define a design space within which the combination of process inputs and process parameters have been demonstrated to provide an assurance of product quality (2,3). Consistent product quality is ensured by development of a control strategy that maintains the process within the bounds of the design space. Process models are an efficient way to define a design space, from the level of individual unit operations or of connected unit operations up to the level of the entire manufacturing process. Process models can be used by manufacturers to ensure product quality, to make predictions, to modify procedures, and for control. Insights gained from process models can lead to improved troubleshooting capabilities during manufacturing and improved control of the critical quality attributes through long-lasting changes in process operating protocols. Here we present a platform for the cultivation of this understanding via a fully instrumented, integrated continuous manufacturing testbed for monoclonal antibodies (mAbs). The testbed consists of 4 parallel upstream bioreactor setups including 4 perfusion devices, with one reactor assembly integrated with a fully continuous downstream purification system including Protein A chromatography, in-house designed viral inactivation, and ion exchange chromatography. This presentation describes the design and implementation of integrated control strategies for the system. The control strategy discussion begins with tuning of lower level controls (i.e. pH, DO, VCD, media addition, etc.) during process development and for extended cultivations. Following experimental demonstration of the control strategies for integrated operation, some first-principles and data-driven models (4) for the prediction of product-related quantities (i.e. N-linked glycoform distribution) are constructed and validated experimentally. These models are then used to inform a control strategy for the stable, continuous manufacturing of a monoclonal antibody that explicitly controls critical product quality attributes rather than only process quality attributes and process operations.",11,2.0
"Control of critical quality attributes (CQAs) is essential for the manufacture of high-quality biologics. Control over glycosylation of antibodies within specified ranges can be challenging but is crucial for product efficacy, immunogenicity and pharmacokinetics. Particularly for biosimilar products, the glycosylation ranges for demonstrating similarity to the innovator product can be very small. Changes in bioreactor process parameters at the manufacturing scale could impact glycosylation outside the specified ranges.Several bench-scale experiments were conducted to understand the relationship between TAF and early culture pH. It was demonstrated that the TAF of the product harvested was affected by pH early in the culture. Given this information, pH control was improved using traditional univariate setpoint control in the early phase of bioreactor culture. This resulted in lots with acceptable TAF.More sophisticated bench-scale studies were carried out to understand the effect of initial pH in combination with other levers with the aim of constructing a predictive model. The pH was intentionally manipulated in early culture to create a defined disturbance in TAF. A Process Analytical Technology (PAT) strategy was used to monitor and control TAF at large-scale.",11,3.0
"The real-time release testing (RTRT) is one of the important cornerstones to achieve end-to-end continuous manufacturing in the pharmaceutical industry. Beside many innovative chemometric model-based process analytical technologies (PAT) to infer the critical quality attributes (CQAs) of drug product, mechanistic model-based predictive testing for target product profiles (TPPs) is also emerging rapidly in the past decade, e.g., tablet dissolution profile [1]. In this study, a predictive mathematical model for tablet dissolution was developed and implemented in an end-to-end integrated continuous manufacturing pilot plant at CONTINUUS to produce immediate release tablets with the specific Extrusion-Molding-Coating (EMC) unit operation. Within the EMC, the active pharmaceutical ingredient (API, a hydrochloride salt) particles are dispersed in the melted polymeric excipients [2]. The model development considered the unique dissolution mechanism of the EMC tablets, which was mainly controlled by the swelling and eroding of the polymeric matrix due to liquid penetration. The major governing equations of the mechanical model consisted of the dissolution, diffusion, and population balance of API particles in the swollen polymeric matrix, as well as the mass balance of API solute in the buffer solution. During in-line implementation, PAT tools, such as laser diffraction for API particle size distribution and near infrared spectroscopy for tablet dosage strength, were OPC connected to the computational server as model inputs. The model output of dissolution time of 70% total API in the tablet was sent to the control system for process monitoring and facilitating the decision-making of real-time release. More importantly, an equivalence study of the model-based predictive dissolution testing was conducted by comparing the model prediction to the experimental dissolution profiles conducted according to USP42-NF37General Chapter <711> Dissolution. Similarity factor f2 values of >50 were obtained for two levels of drug dosing. A f2 value of >50 indicates the similarity between the predicted and experimental dissolution profiles. Sensitivity of the model parameters and risk analysis of the model-based dissolution testing were also investigated in this study. Last but not the least, the developed predictive dissolution testing strategy is not limited to the specific API and can be generalized to other APIs for RTRT implementation using the EMC technology.References:[1] Zaborenko N, Shi Z, Corredor CC, Smith-Goettler BM, Zhang L, Hermans A, Neu CM, Alam MA, Cohen MJ, Lu X, Xiong L, Zacour BM. First-principles and empirical approaches to predicting In Vitro dissolution for pharmaceutical formulation and process development and for product release testing. The AAPS Journal. 2019; 21:32.[2] Hu C, Testa CJ, Wu W, Shvedova K, Shen DE, Sayin R, Halkude BS, Casati F, Hermant P, Ramnath A, Born SC, Takizawa B, O’Connor TF, Yang X, Ramanujam S, Mascia S. An automated modular assembly line for drugs in a miniaturized plant. ChemComm. 2020; 56: 1026-1029.",11,4.0
"Crystal morphology engineering is an important component of process development in the pharmaceutical industry, since the crystal shape or habit directly influences downstream processing, formulation, and the physical properties of the API. Most commonly the desired habits of crystals are prepared through the modification of crystallization process parameters such as temperature, the crystallization solvent, and/or the addition of a co-solvent or an additive. However, in some cases, these methods do not successfully deliver the desired morphology. Here we present the use of Soxhlet extraction conditions as a valuable tool for morphology modification in a nucleation dominant crystallization process. In this case study, the process to produce a cocrystal between Vertex compound A and coformer X resulted in a fibrous morphology in every attempt under various conditions, leading to mixing and filtration issues and low yields. The use of the Soxhlet extraction method provided very slow mixing of the two compounds coupled with temperature cycling close to the solvent’s boiling point to generate a needle morphology, which provided better filtration times, processability and yields, enabling a path forward for further API development.",12,0.0
"Crystallization is a critical unit operation in the production of small molecule active pharmaceutical ingredients (APIs), as it often determines the form and powder properties of the product. Therefore, crystallization process development focuses on identifying conditions that are controlled and scalable. In general, liquid-liquid phase separation or oiling out of product are undesirable as they may lead to uncontrolled nucleation and growth of particles that impacts critical quality attributes, such as particle size and level of impurities. Therefore, understanding liquid-liquid phase equilibria and solubility of the compound is critical in designing crystallization processes.This presentation will describe how we experimentally investigated the liquid-liquid phase separation that occurred during crystallization of a small molecule API in a ternary solvent system of tetrahydrofuran, acetone and water. Although these solvents are typically miscible, the presence of the API resulted in liquid-liquid phase separation at the seed point under some conditions. In-line process analytical tools such as ParticleViewer and FBRM were incorporated to detect and understand the phenomena. A detailed phase diagram was generated for the crystallization design space as a function of solvent composition, concentration of API in solution and temperature. Along with solubility data, the phase diagram was used to design a process that avoids liquid-liquid phase separation throughout the entire crystallization. In addition, we will describe the impact of wet-milling during seed age that was implemented to ensure that full desupersaturation was achieved, which was critical in avoiding phase separation.",12,1.0
" IntroductionMicrofluidics provide several advantages such as high efficiency, easy control and safety over conventional, and mostly batch reaction systems[1]. Therefore it can be seen as a replacement for batch crystallization as one of the most important separation and purification process especially in pharmaceutical industries[2,3].The aim of this project is the design of a modular and decoupled microfluidic system consisting of separate nucleation and growth sections. The strategy persued in this project is using the two- phase interface in microreactors as a heterogeneous nucleation site. In this regard micro- bubbles are introduced in the system to control the location of the nucleation site and particle aggregates and to limit wall interactions. The velocity of the bubbles determines the residence time and using a gas phase enables a straight forward separation, see Figure 1. Experimental study2.1. Nucleation sectionFor designing the nucleation section, in the first step, the generation of micro-bubbles (i.e. nitrogen as an inert gas) in capillaries is studied. The total bubble surface area per unit volume of the reactor is related to the bubble diameter and the number of bubbles present in the reactor volume. Bubble diameter, bubble velocity and bubble generation frequency were extracted from the images captured by the high-speed camera using an inhouse-generated Matlab script. Using this information, the bubble surface area available as a heterogeneous nucleation site is further calculated and the effect of gas and liquid flowrate is investigated, see Figure 2.This is followed by a systematic study of nucleation of an organic compound (i.e. cooling crystallization of paracetamol) using the developed system employing microbubbles. The particle size distribution is measured at the outlet of the channel subjected to the microbubble residence time and the nucleation and growth kinetics are extracted.The continuous crystallization experiments provided proof that the micro-bubbles act as a heterogeneous nucleation site. An enhanced nucleation rate compared to the operation without bubbles was observed, and the presence of micro-bubbles results in the formation of more crystals, which indicates that nucleation is faster than without bubbles, i.e. the metastable zone width is reduced, see Figure 3. Furthermore, quantifying the crystal yield showed an increase by more than a factor of 2 in the presence of micro-bubbles. 2.2. Growth sectionFurther on a growth section is added to the nucleation section which is kept at the constant temperature equal to the outlet of nucleation section to allow formed nuclei to grow. Sample is collected at the outlet of the growth section for further analysis of particle size distribution, see Figure 4. ConclusionsA decoupled crystallization set-up consisting of different nucleation and growth sections is designed using microreactors. Nucleation is enhanced using microbubbles acting as heterogeneous nucleation sites providing lower surface energy interface for the nucleation to start. Further a growth part is added to allow the formed nuclei grow within the constant temperature growth section.In case of the continuous crystallization of paracetamol the results allow the conclusion that the low-energy heterogeneous surface introduced by the bubbles promotes crystal nucleation. In addition, adding micro-bubbles enables the continuous crystallization for long operating times, as the bubbles act as carriers for the formed crystals and effectively prevent wall deposition and thus clogging.References [1] Gunther, K. F. Jensen, Multiphase microfluidics: from flow characteristics to chemical and materials synthesis, Lab Chip , 2006, 6, 1487–1503[2] S. Flowers, R.L. Hartman, Particle Handling Techniques in Microchemical Processes, Challenges 2012, 3, 194-211[3] Wu, S. Kuhn, Strategies for solids handling in microreactors. Chim Oggi. 2014; 32(3):62-66.[4] Fatemi, Z. Dong, T. Van Gerven, S. Kuhn, Microbubbles as Heterogeneous Nucleation Sites for Crystallization in Continuous Microfluidic Devices. Langmuir (2018). doi:10.1021/acs.langmuir.8b03183",12,2.0
"Crystallization in confinement is a technique used both as an approach for nanocrystal development and as way to gain access to unstable crystal polymorphs. To elucidate this, the effect of decreasing volumes in the nucleation of glycine in water was explored. Using a supersaturated aqueous solution of glycine at decreasing volumes from 1000 μL to 25 μL, induction times for primary nucleation were obtained experimentally. Analysis of probability distribution of β-glycine induction times showed that as sample volume decreases, induction time increases. Image analysis revealed that β-glycine formation serves as the primary nucleation event; however, primary nucleation shifts toward γ-glycine nucleation at volume sizes below 100 μL. These results demonstrate that feasibility of accessing metastable and unstable crystal polymorphs with the aid of confinement. However, confinement also increases induction times which relates to very low probabilities of a nucleation event.",12,3.0
"The main purpose of crystallization is to purify the product by removing undesired impurities. In pharmaceutical industry, crystallizations at different steps in the synthesis are relied upon to control the final quality of the active pharmaceutical ingredient (API). During these operations, the main challenge often stems from dealing with structurally similar impurities, which tend to be difficult to remove. Presented herein is an investigation into how this type of challenging impurities are purged mechanistically in the crystallization, using pharmaceutical case studies. Through these examples, it is shown that formation of substitutional solid solutions is the underlying mechanism for impurity retention. Impurities molecularly replace the product molecules in the crystal lattice during the crystallization. This lattice incorporation entrapment occurs unevenly during the crystallization resulting in a gradient in impurity levels throughout the material. A mathematic framework combined with a previously reported SLIP test is used to elucidate this varying impurity entrapment through a so-called material impurity distribution (MID). The impurity levels are seen in many cases to change by an order of 20 across the material. The effect of the entrapment can also be observed in the physical properties of the material, e.g. by a significant reduction in crystallinity, increase in solubility and earlier onset melting event.",12,4.0
"The pharmaceutical industry has predominantly relied on batch processes for manufacturing of drug substance as well as drug products. However, longer campaign times, labor-intensive nature of batch manufacturing and batch to batch variations in the product quality pose significant drawbacks. In view of this, the Pharma industry is turning towards continuous manufacturing, going beyond flow chemistry to continuous crystallization and downstream operations for the synthesis of drug substances. Continuous crystallization offers many advantages such as better control over the solid form and particle size, and the potential for an end-to-end continuous manufacturing process.The current work describes the design of a continuous crystallization process for a drug substance in order to integrate with upstream and downstream continuous processes. It was not only important to achieve the targeted solid state form (trihydrate, Form I) but also ensure that the filtration rate could be increased to enable integration with the downstream continuous filter and minimize the cleaning frequency. The presented case study discusses in detail the approach, starting from lab-scale batch experimentation to development of a continuous crystallization process at lab followed by scale-up design for a capacity of multi-tons per month.As a first step, fundamental solubility data was collected followed by identification of critical parameters impacting nucleation and transformation between the various forms. It was observed that the solvent (alcohol) to antisolvent (water) ratio, seed loading and temperature had a significant impact on the solid form. A batch crystallization process was first established by combining antisolvent and cooling crystallization to consistently generate the desired Form I, while ensuring the absence of other solid forms.The translation of the batch process to continuous was initially attempted in MSMPRs in series but proved unsuccessful as an undesired form started to appear over time despite seeding. It was hypothesized that several more MSMPRs would be required to achieve the desired form but this would occupy a huge footprint and increase hold-up at scale. While plug flow reactors would be suitable, the risk of fouling is high, particularly when flow rates are reduced to achieve sufficient residence time. Scouting for an appropriate technology lead to evaluation of dynamically mixed reactors from AM Technologies®, which were able to handle a slurry and provide a mixing profile close to plug flow decoupled from the residence time. At lab scale, the Coflore® Agitated Cell Reactor (ACR), which is a bench-top mechanical agitated reactor comprising ten small cells, was able to aid in controlled crystallization of the desired Form 1 with well defined crystals and a consistent particle size distribution. Since additional reactors and residence time were required to gradually cool the slurry from 65°C to 25°C, the ACR was combined with three CSTRs in series (forming a mixed suspension mixed process set up). The trials were executed by combining a stream of alcohol containing the dissolved solute (at 65°C) and another stream of hot water containing seeds (at 65°C) in the ACR. The slurry exiting from the reactor was continuously transferred into the CSTRs, where it was subjected to stage-wise cooling in each CSTR. The residence times were designed to achieve maximum crystallization in the ACR and minimize secondary nucleation in the subsequent CSTRs.After establishing the success and robustness of the continuous crystallization process at lab-scale, the process was scaled up in a 100L, production-scale version of the dynamic mixing reactor - Coflore® Rotating Tubular Reactor (RTR). Optimization of the process parameters revealed that maintenance of higher temperatures (~65 °C), , enhanced mass transfer at the point of nucleation along with the presence of Form 1 seeds, and higher ratios of anti-solvent were critical for achieving the desired polymorph while low mixing shear aided in crystal growth. The growth of these fine, needle-shaped crystals was essential to achieving a sufficiently high filtration rate for the downstream continuous separation of solids. The RTR, which operates as a ten-stage, actively mixed, self-baffled reactor, provided high wall shear with radial mixing while maintaining a low mixing shear, thus enabling crystal growth and improving the filtration rate while minimizing fouling.In summary, the work presents key aspects of design and scale-up of a continuous crystallization for a drug substance with regard to both process and technology selection. One of the main advantages of this continuous process was consistency in the solid form, particle size distribution resulting in improved filtration of the slurry, and ability to integrate with upstream and downstream continuous processes.Internal communication number: IPDO IPM-00637",12,5.0
"Five-fold twinned nanowires can be produced with high aspect ratios from diverse face-centered cubic (fcc) metals. However, the mechanism of their anisotropic growth is still poorly understood. We develop a model combining deposition and surface diffusion to predict the diameters and the aspect ratios of Ag nanowires. We describe two aspects of nanowire growth, denoted as the seed process and the wire process. In the seed process, the deposition time is much longer than the surface diffusion time, so that the seed shape only depends on the difference between interfacet diffusion times. The structure of nanowire seeds is similar to Marks decahedra with {111} notches and {110} steps that accelerate interfacet diffusion leading to wire growth. The result exhibits that the decahedron seeds could lead to three kinds of products with different strucutures as nanowires, decahedron particles and giant decahedron seeds. We focus on the seeds growing to nanowires and introduce the wire process, which occurs subsequently and continues until the diffusion time is equal to the deposition time. We predict the aspect ratio by contrasting the diffusion rate from {100} to {111} to the deposition rate on {100}. The aspect ratios predicted in our simulations are in the experimental range.",13,0.0
"Confined crystallization is an approach to develop nano-crystalline active pharmaceutical ingredients which improves dissolution and bioavailability properties of a formulated drug. To elucidate on this, the effect of volume reduction to the nucleation of acetaminophen in ethanol was explored. Using a supersaturated solution of acetaminophen in ethanol at volumes of 1000 μL to 25 μL, induction times for the primary nucleation event were obtained experimentally. The probability distribution of induction times showed that as sample volume is reduced, the induction time increases exponentially. The induction time at the volume of 25 μL is obtained to be at 4214 minutes, which is at a 5400% increase from the observed induction time of 78 minutes when the volume is set at 1000 μL.These results demonstrate the effect of volume reduction to nucleation induction times. Confinement may relate to very low probabilities of a nucleation event.",13,1.0
"The precipitation is a common step in industry which is relatively ill-understood. In the last decades, many efforts were made to make numerical simulations of this step to avoid using the trial and error method, especially for the precipitation of active compound like plutonium oxalate. Many parameters are required to handle these numerical simulations, including the nucleation rate (the number of particles which appears in the reaction media by unit of time and of volume).The nucleation rate is usually rationalized according to the Classical Nucleation Theory (CNT). This theory predicts the nucleation rate, assuming that thermal fluctuations of the reaction media lead to the formation of some “clusters” of monomers with the same composition and (?) symmetry as the crystal. These clusters tend to immediately dissolve into the reaction media unless they reach a particular size, called the critical size, from which they can grow fast to a macroscopic size. The so-called nucleation rate is the rate at which clusters reach this critical size. However, the CNT is more and more questioned, as it has been evidenced for many systems, including cerium oxalate, that nucleation occurs through amorphous states which are currently ignored by the CNT and which lead to an significant underestimation of the nucleation rate, up to 400 orders of magnitude.In this work, the nucleation of cerium oxalate, a non-radioactive surrogate for plutonium oxalate, has been studied using cryo-microscopy and small and wide x-ray scattering (SAXS and WAXS). Here we show that cerium oxalate has two possible non-classical nucleation mechanisms: at low concentrations, nucleation in presence of amorphous nanoparticles, and at higher concentration, nucleation involving both a transient dense liquid phase and amorphous nanoparticles.",13,2.0
"Securing access to critical metals required for high-performance technologies, particularly the light rare earth elements (REEs = La, Ce, Nd, Pr), has become a major challenge for import-dependent economies such as the EU. The recycling of spent nickel metal hydride (Ni-MH) batteries from hybrid electric vehicles (HEV) serves as an increasingly-attractive secondary source of REEs thanks to the recent development of hydrometallurgical processes.[1] In order to recover REEs, precipitation from pregnant leach solutions (PLS) in sulfate media, using Na2SO4 is often reported in the literature. However, little consideration is given as to whether and how sodium ions influence the precipitation efficiency and selectivity, and detailed phase characterization of the products is rarely reported. This work focuses on a better understanding of the precipitation phenomenon and mass balances in this complex system, by coupling pilot-scale experiments on industrially-sourced PLS, in-depth precipitated particles identification, and thermodynamic calculations.The initial PLS used in this study originates from the pilot-scale leaching of industrial samples of spent Ni-MH battery powders in H2SO4 media, according to a process described in [1]. The metallic content of the battery waste leachate consists of 71%mol Ni, 10 %mol REEs and 19%mol of other metals like Co, Mn and Fe. In the view of metal recycling, it is of great interest to find a selective precipitation pathway to recover REEs from such a complex solution. For each precipitation experiment, 1.5 L of PLS was added to a double-jacketed batch reactor; solution temperature was regulated using a cryostat and pH was monitored. In a standard experiment, a 2.9 M Na2SO4 solution was maintained at 40 °C and added to the PLS at constant flow rate, in simple jet, using a membrane pump. Mechanical agitation was implemented with a 3-blade Teflon marine propeller at 400 rpm. The influence of three operating parameters on precipitation efficiency and kinetics was studied: temperature (25 – 60 °C), Na:REEs molar ratio (1:1 – 4:1, by increasing the total addition time of Na2SO4 solution at a constant flow rate) and Na2SO4 solution addition (all at once or slowly at 7 g.min-1 implying an initial semi-batch mode). Suspension volumes were regularly sampled, filtered on 0.45 µm syringe filters, and solutions were analyzed by ICP-OES. Precipitation efficiency was calculated as the proportion of the REEs initially present in solution that have precipitated. After 1 h, the suspension was filtered on a P3 Büchner filter and rinsed with demineralized water at room temperature. The obtained crystals were dried in an oven at 80 °C overnight before in-depth characterization using multiple techniques: SEM-EDX, powder XRD, TGA, microwave digestion in aqua regia and ICP-OES.Results compiled in Figure 1) a) illustrate the evolution of the proportions of Ni, Co, Mn, Fe, La, Ce, Nd and Pr remaining in solution when precipitation is carried out at 60°C, for a Na:REEs molar ratio of 4:1 and a Na2SO4 solution flow rate of 7 g.min-1. The simultaneous drop of pH when REE concentrations start to decrease (Figure 1) b) demonstrates that pH monitoring serves as an indicator of precipitation initiation and therefore allows determining the induction period, which is of about 3 min in this case. Curves in Figure 1) a) flatten out after approximately 30 min, suggesting that equilibrium has been reached. After 1 h, the precipitation efficiencies of La, Ce, Nd and Pr reach 99.3 %, 100.0 %, 99.7 % and 87.6 %, respectively, corresponding to 70 ppm of La, 0 ppm of Ce, 4 ppm of Nd and 52 ppm of Pr remaining in solution. REE precipitation is highly selective in this configuration since 99 % of Ni, 99 % of Co, 98 % of Mn, and 99 % of Fe remain in solution for further recovery. The molar ratios of REEs in the precipitate during the semi-batch and batch modes are reported in Figure 1) c). These ratios remain relatively constant during the 1 h run, suggesting that the same phases form throughout the whole precipitation period.Precipitation efficiencies and aqueous REE concentrations are summarized in Table 1 for all experimental conditions. This compilation highlights the influence of operating parameters. For a fixed Na:REEs molar ratio of 4:1, increasing the temperature from 25 to 60 °C greatly increases precipitation efficiency of all REEs and yields faster precipitation kinetics; for example, 74.4 % of La is precipitated at 25 °C while at 60 °C, 99.3 % is precipitated. Similarly, increasing the Na:REEs molar ratio from 1:1 to 4:1 at 60 °C, strongly increases the precipitation efficiencies of all REEs. Furthermore, adding the equivalent volume of Na2SO4 solution at once yields the same precipitation efficiencies than when the solution is added at a 7 g.min-1 flow rate but greatly increases precipitation kinetics; however, rapid injection favors nucleation generating very small particles and consequently, after 1 h, filtration was more difficult (low filtration rates). It is interesting to note that, for a given temperature or for a given Na:REEs molar ratio, precipitation efficiencies of the REEs is always as: Ce > Nd > La > Pr.Powder XRD analyses of the precipitates yield diffraction peaks that lie between those of pure NaLa(SO4)2.H2O and NaCe(SO4)2.H2O double sulfates. Crystal lattice parameters were calculated and exhibit intermediate values between those of pure double sulfates suggesting that precipitates are solid solutions of lanthanide-alkali double sulfates. Structures are assigned to the hexagonal crystal system[2], which can be observed on a representative SEM micrograph provided in Figure 2. The chemical composition of the solids was determined by TGA (number of water molecules n) and ICP-OES analysis (elemental content of S, Na, La, Ce, Nd and Pr). The resulting solid compositions are gathered in Table 2 as Nax(REE)1(SO4)y.nH2O, where x and y are the respective Na and S proportions relative to the total REEs content. The water content (n) is close to 1 for each solid, which proves the formation of monohydrate-type crystals. The mass and electrical (3+x–2y) balances yield that both Na and SO4 contents are lower than the stoichiometric ratios (x<1 and y<2), and that the overall charge of such compounds is about +0.35 – +0.40. This charge balance inconsistency, which is currently investigated, is most likely due to the incorporation of additional ions in the structure. As indicated in Table 2, REE molar ratios in the mineralized precipitates are relatively equivalent for all experiments. Therefore, it is concluded that the same lanthanide-alkali solid solution forms regardless of the temperature or the Na:REEs molar ratio.In parallel, equilibrium calculations were performed using the mixed-solvent electrolyte model implemented in the OLI software, which includes an accurate thermodynamic description of rare earth sulfates recently developed by Anderko et al.[3] Calculations were carried out with the various initial PLS conditions (pH, temperature, and ion concentrations). They foresee the precipitation of the compounds NaLa(SO4)2.H2O, NaCe(SO4)2.H2O, NaNd(SO4)2.H2O and NaPr(SO4)2.H2O in all conditions, which is in good agreement with XRD characterizations. The calculated REEs aqueous concentrations, detailed in Table 1, are also rather close to experimental measurements, which indicates that the precipitation reactions are close to thermodynamic equilibrium after 1 h. More specifically, REE aqueous concentrations strictly meet calculated concentrations at 60 °C and for a 4:1 Na:REEs molar ratio. At 60 °C, calculations show that the solubilities of all lanthanide-alkali double sulfates decrease with increasing Na+ concentration, which explains why increasing the Na:REEs molar ratio improves precipitation efficiencies at constant pH. Moreover, for a Na:REEs molar ratio of 4:1, increasing the temperature decreases the double sulfates solubilities (apart from NaCe(SO4)2.H2O which shows the opposite trend). Therefore, based on both experimental results and thermodynamic trends, we conclude that the configuration (60 °C and 4:1 Na:REEs molar ratio) yields the best precipitation efficiencies for La, Ce, Nd and Pr (99.3 %, 100.0 %, 99.7 % and 87.6 %, respectively).Key takeaways of this study are a straightforward approach to perform a grouped extraction of REEs contained in complex solutions originating from the leaching of industrial samples of spent Ni-MH batteries. Using a concentrated Na2SO4 solution, highly selective precipitation was obtained after 1 h at 60 °C and for a Na:REEs molar ratio of 4:1, whereby only 70 ppm La, 0 ppm Ce, 4 ppm Nd and 52 ppm Pr remain in solution and more than 98 % of the major elements do not co-precipitate. Thanks to multi-analytical characterization backed by thermodynamic calculations, the precipitated crystals were determined to be lanthanide-alkali double sulfate solid solutions of overall composition Na0.79La0.59Ce0.20Nd0.07Pr0.14(SO4)1.72.H2O. Results suggest that the same lanthanide-alkali solid solution forms regardless of the experimental conditions.[1] M. Zielinski, L. Cassayre, P. Destrac, N. Coppey, G. Garin, B. Biscans, Leaching mechanisms of industrial powders of spent nickel metal hydride batteries in a pilot-scale reactor, ChemSusChem. 4 (2020) 616–628.[2] A.C. Blackburn, R.E. Gerkin, Sodium lanthanum(III) sulfate monohydrate, NaLa(SO4)2.H2O, Acta Crystallogr. C. 50 ( Pt 6) (1994) 833–838.[3] G. Das, M.M. Lencka, A. Eslamimanesh, P. Wang, A. Anderko, R.E. Riman, A. Navrotsky, Rare earth sulfates in aqueous systems : Thermodynamic modeling of binary and multicomponent systems over wide concentration and temperature ranges, J. Chem. Thermodyn. 131 (2019) 49-79.",13,3.0
"The crystallization of struvite (MgNH4PO4·6H2O) is relevant for nutrient (nitrogen and phosphorus) recovery in water purification and scale formation in pipelines, and it is a primary component of so-called infection stones arising from urinary tract infections. Research efforts have focused on understanding the effects of different parameters (e.g., supersaturation, pH, inhibitors) on crystallization of struvite; however, prior studies have only focused on the use of bulk techniques and have not provided mechanistic insights into these controls.Herein, we present a comprehensive approach to evaluate struvite formation at both macroscopic and microscopic length scales in the absence and presence of various inhibitors. We conducted in situ kinetic measurements to estimate the extent of inhibition posed by each inhibitor on struvite formation. To this end, we employ a microfluidic platform to track the anisotropic growth rates of struvite crystals under various conditions, including comparisons of solutions with and without the presence of modifiers1. Furthermore, we elucidate the mechanism of inhibitory action by probing the surface dynamics in real time via in situ atomic force microscopy (AFM), which has proven to be a powerful technique to elucidate molecular level details of crystallization2,3. The results reveal unique mechanisms facilitating complete inhibition of growth and in select cases the unprecedented suppression of nucleation, which is not commonly observed for even the most potent modifiers of mineralization. Collectively, our findings identify highly efficient and commercially available modifiers that inhibit struvite formation. This study also uncovers a new class of inhibitory mechanisms that have potentially broader applicability to other minerals, thus establishing new paradigms for the prevention of commercial scale.Kim, D., Olympiou, C., McCoy, C. P., Irwin, N. J., & Rimer, J. D. (2020). Time‐Resolved Dynamics of Struvite Crystallization: Insights from the Macroscopic to Molecular Scale. Chemistry–A European Journal, 26(16), 3555-3563.Chung, J., Granja, I., Taylor, M. G., Mpourmpakis, G., Asplin, J. R., & Rimer, J. D. (2016). Molecular modifiers reveal a mechanism of pathological crystal growth inhibition. Nature, 536(7617), 446-450.Farmanesh, S., Ramamoorthy, S., Chung, J., Asplin, J. R., Karande, P., & Rimer, J. D. (2014). Specificity of growth inhibitors and their cooperative effects in calcium oxalate monohydrate crystallization. Journal of the American Chemical Society, 136(1), 367-376.",13,4.0
"The structuring of the solvent at the crystal-solvent interface largely regulates the activation barrier for incorporation of solute into the crystal. Depending on the groups exposed on the crystal surface, the solvent at the interface may be ordered or disordered 1,2. In general, the consequences of solvent structuring on the molecular level interactions comprising crystallization are poorly understood. Organic solvents, in which the bonds that could support a defined fluid structure are weak and varied present a particular challenge to understand the effect of solvent structuring on thermodynamics and kinetics of crystallization.We explore the thermodynamic parameters of crystallization of a group of supramolecules known as porphyrins in organic solvents. Thermodynamic characteristics of crystallization were used as probes for the presence of solvent structuring which reveals that the interactions that govern the enthalpy and entropy of crystallization are dominated by the van der Waals contact between the solvent aliphatic tails and the non-polar groups of the etioporphyrin I (a free base porphyrin). Time-resolved in situ atomic force microscopy monitoring of the step dynamics on (010) face of etioporphyrin I in two types of solvents, alcohols (octanol, butanol and hexanol) and DMSO, of varying aliphatic chain length and subsequent solvent viscosity demonstrates the impact of solvent structuring, solute-solvent interactions and solvent viscosity on the barrier of solute incorporation into kinks. The relation between step kinetic coefficient and solvent viscosity reveals that the crystallization of organic small molecules is not governed by diffusion limitation but rather by solute-solvent interaction at the kinks. The correlation between the step velocity and the solute concentration on the (010) face of etioporphyrin I for different solvents reveal that for solvents belonging to the same homologous series, solute incorporation scales with the solute diffusion coefficient. All-atom MD simulation for solvent structuring at the crystal interface emphasizes the importance of solute-solvent binding at the crystal interface.These emerging insights offer guidance for polymorph selection, chiral separations, structure design and numerous other open questions relevant to myriad crystallization systems in the pharmaceutical and chemical industries.References1. Vácha, R., Zangi, R., Engberts, J. B. F. N. & Jungwirth, P. Water Structuring and Hydroxide Ion Binding at the Interface between Water and Hydrophobic Walls of Varying Rigidity and van der Waals Interactions. The Journal of Physical Chemistry C (2008) 112, 7689-7692.2. Meister, K., Strazdaite, S., DeVries, A. L., Lotze, S., Olijve, L. L. C., Voets, I. K., & Bakker, H. J. Observation of ice-like water layers at an aqueous protein surface. Proceedings of the National Academy of Sciences (2014) 111, 17732-17736.",13,5.0
"Covalent Organic Frameworks (COFs) have gained significant attention in the field of material science over last two decades [1]. 2D boronate-ester linked frameworks like COF-5 are porous, crystalline, organic polymers with low density, that have numerous applications such as gas storage and as membranes for various industrial purification applications. COF-5 exhibits high thermal stability, high surface area and permanent porosity, widening the scope of its applicability. Controlling crystal morphology is the most critical variable to maximize the applicability of these polymeric frameworks. Computing steady state and dynamic morphologies of COF-5 in a specific environment is essential for gaining comprehensive understanding of ways to control it. Face-specific growth rates have been previously utilized to understand these morphologies. The faces of these crystals are divided into three classes: flat, stepped and kinked faces, according to the Hartman-Perdok theory. This theory also states that layer by layer growth occurs on the F-face of the crystal lattice, which is the basis of studies conducted on COF-5 framework. Attachment energy calculations have been carried out along with kink sites distribution calculations to evaluate growth rates. The predicted rates have been used to understand the morphology of COF-5 crystals and the results are compared with experimental values, providing a deeper insight on optimizing process conditions for morphology control. ",13,6.0
"Unwanted crystallization is a major concern in a number of chemical industries, affecting fuels, agrochemicals, pharmaceuticals and food. The ability to overcome this issue by disrupting the crystallization process, through the use of additives, has been a long studied area of crystallization science and engineering 1. Since the work of Lahav, Leiserowitz and colleagues at the Weizmann Institute of Science2,3, a large number of research studies have focussed on tailor-made additives (TMAs), whereby an additive with a very similar molecular structure to a target crystallization compound is used to inhibit aspects of the crystallization process. The majority of work has focussed on crystal growth inhibition, however, more recently the ability of TMAs to inhibit nucleation has been a growing area of research.This work focuses on the nucleation of alpha-p-aminobenzoic acid (pABA) from ethanol solution, in the presence of TMAs. pABA in ethanol solution is known to nucleate through carboxylic group H-bonding dimer formation, with the crystal structure formed also containing strong amine-carboxyl group H-bonding and π-π stacking interactions 4. TMAs that could potentially disrupt pABA-pABA dimer formation as well as the formation of the previously described synthonic interactions were chosen to study their effect on pABA nucleation inhibition form ethanol solution.Seven TMAs were chosen and molecular modelling using an intermolecular grid search was undertaken to screen and understand the dominant molecular interactions between pABA and the TMAs, in terms of synthonic interaction energies. Two TMAs were found to have stronger interactions to pABA than pABA itself, forming competitive carboxylic H-bonding dimers, these were 4-amino-2-methoxybenzoic acid (AMBA2), 4-amino-3-nitrobenzoic acid (ANBA3). These TMAs had the same molecular structure of pABA, but with methoxy or nitro groups attached, respectively. From calculated atomic charge energies, the nitro group of ANBA3 was assessed to be able to disrupt the subsequent binding of pABA molecules into the crystal structure through electrostatic interactions.A polythermal experimental screen was performed to assess the intermolecular grid search’s ability to predict TMA efficacy. The experimental results corroborated findings from the modelling results, with ANBA3 being the most effective nucleation inhibitor followed by AMBA2.Nucleation kinetics were assessed for pABA vs. pABA-ANBA3 in ethanol solution using the KBHR approach. It was found that pABA nucleates by instantaneous nucleation, whereas the pABA-ANBA3 nucleates by progressive nucleation. The interfacial tension was found to also increase in the presence of ANBA3.References1) J. W. Mullin, Crystallization, Butterworth-Heinemann, Oxford, 4th ed., 2001.2) L. Addadi, Z. Berkovitch Yellin, N. Domb, E. Gati, M. Lahav and L. Leiserowitz, Nature, 1982, 296, 21–26.3) I. Weissbuch, L. Addadi, M. Lahav and L. Leiserowitz, Science, 1991, 253, 637–645.4) D. Toroz, I. Rosbottom, T. D. Turner, D. M. C. Corzo, R. B. Hammond, X. Lai and K. J. Roberts, Faraday Discuss., 2015, 179, 79–114.",14,0.0
"Purpose: Typical micelle-based drug delivery carriers are formed from amphiphilic block copolymers which self-assemble into core-shell nanoarchitectures with a hydrophobic core being able to encapsulate drug molecules and a biocompatible hydrophilic shell. An innovative turbulent co-flow platform of water and ethanol (with drug and polymers) was previously developed to continuously process drug loaded micelle nanocarriers. Polymer aggregation is the result of the intermolecular forces among molecules and along with the jet flow characteristics impact to the formation of micelles. However, the underlying mechanism and the detailed effects from material attributes are only partially understood. In order to investigate the underlying mechanism and quantitative estimation of the effect of material attributes and process parameters on the quality of the polymeric micelle formulation, we implemented a multiscale computational approach to study micelle formation using a coaxial turbulent jet flow.Methods:All-atom (AA), coarse-grained molecular dynamics (CG-MD), as well as computational fluid dynamics (CFD) simulations have been conducted to not only reveal the effects to material attributes and processing parameters during the micelle formation, but also as parametric case studies for this process. The initial conditions applied into the simulations were based on polymeric micelle experiments in which PEG-PLA (2kD-1.7kD) was used as model drug carrier. The CFD simulation was implemented using Large Eddy Simulation (LES) model in COMSOL Multiphysics incorporating energy equation and high-resolution mesh in mixing area. The MD simulation trajectories and their analysis were carried out using GROMACS package. We applied both CHARMM and MARTINI force-fields for all-atom as well as coarse-grained simulations, respectively. The initial coordinates of models and their force field parameters were generated by CHARMM software and MARTINI forward mapping approach. The steepest decent algorithm was used in energy minimization with a 20 fs time step followed by 10 ns equilibration step in isothermal−isochoric NVT ensemble at temperature of 300 K, then the production runs were performed beyond 1 μs in the NPT ensemble using the Nosé−Hoover thermostat and the Parrinello−Rahman barostat at pressure of 1 atm with constant temperatures at every 20 fs time steps. The simulations were carried out using periodic boundary conditions. Computations were performed in High Performance Center Supercomputer Cluster at the University of Connecticut.Results: The self-assembled process of polymeric micelle was studied in the system with and without drug molecules, and in each case, block copolymer was randomly packed in the simulation box with explicit water and ethanol for all simulations. With optimized CG-MD forward mapping strategy and force field parameters incorporating MARTINI standard beads and recent developed S-beads, we observed that polymeric micelles formed successfully and fell into the experimental size range. The simulation of micelles formation and the size distribution the micelles indicate that CHARMM, MARTINI force fields (FFs) can effectively capture the experiment behavior and results . By adjusting Smagorinsky coefficient (Cs) of LES model companied with energy and mass transfer equations, CFD simulations successfully modeled flow patterns and formation temperatures of co-axial turbulent jet flow from experiments, as well as successful comparison and verification. Our CG-MD and CFD simulations  are in good agreement with experimental results.Conclusions:In this present work, the model predictions provide an understanding of the impact of drug-polymer interactions, drug-polymer ratio, initial organic phase polymer concentration, as well as maintaining the agreement between the computational predictions and experimental findings highlight the power of the multiscale approach to be able to cover the long and short length scale predictions. Moreover, the details of the formation process was revealed in micro, macro, and meso scales by connecting discrete and continuum computational modeling work which are found to be complementary to one another. The multiscale approach used in this work can be effectively utilized as a powerful tool in the process of discovery, development, and optimization of new drug delivery systems in the co-flow continuous processing.Acknowledgements: FDA Grant# 1U01FD005773-01.Disclaimer: This article reflects the views of the authors and should not be construed to represent FDA’s views or policies.",14,1.0
"Spray-drying is a manufacturing process utilized by a wide range of industries such as food, agricultural, and pharmaceutical for converting a liquid feed containing insoluble solid particles into particulate form. By removing the moisture, it is intended to achieve a final product with desired chemical composition, morphology (hollow particles, solid cores, agglomerates, etc.), and particle size distribution. There are many variables that affect the drying process, and due to its complexity, traditional trial-and-error methods have been the primary technique in achieving the desired product.The drying dynamics affect the internal flow patterns within a droplet in spray-drying. The flow patterns are dominated by surface tension and evaporative effects which ultimately disperse the suspended particles in the droplet and finally determine the final product morphology. In the presented work, the drying and internal transport of insoluble particles within a droplet were simulated using a volume of fluid method coupled with a Lagrangian solver for tracking particle movement. The simulations were performed within the open-source framework, OpenFOAM. The solver has been heavily modified to include interfacial mass transfer by employing a coupled Eulerian-Lagrangian framework capable of simulating drying of a droplet and the solid particles suspended inside. Modeling droplet’s internal flow created through evaporative effects, and tracking movement of solid particles, drying and the morphological evolution of the product is simulated and analyzed.",14,2.0
"IntroductionSecondary nucleation is a phenomenon present everywhere in nature and of fundamental importance for crystallization processes [1,2]. There are two families of mechanisms of secondary nucleation: the first occurs at the interface of the seed crystal and the solution, and it is a combination of an activated process and the effect of shear, i.e. surface- or shear secondary nucleation. The second family results from mechanical collision happening to the seed crystal, i.e. secondary nucleation by attrition. Attrition is the mechanism through which fragments form after an impact of a crystal with a stirrer, other crystals or the wall of the reactor. Those fragments, if small enough, can be considered secondary nuclei. For both families, there is not a clear understanding of the governing physics, and secondary nucleation rates are usually given by an empirical relationship depending on the extent of mixing, on the amount of crystals suspended and on the supersaturation level [1]. Methods and ModelsIn this contribution, we have extended the well-established mechanistic attrition model developed by Gahn and Mersmann [3] to two different population balance equation (PBE) models, which are used to simulate secondary nucleation processes. The traditional approach describes the formation of attrition fragments due to collisions as a secondary nucleation rate, which is included in the model as a boundary condition (we call it nucleation model). In the alternative approach, the formation of attrition fragments is described as a breakage expression and the growth rate is the result of size-dependent solubility (we call it breakage model) [4,5]. Conversely to the traditional model, this formulation takes into account the size distribution of attrition fragments and their evolution due to growth, see the figure for a schematic representation of the two models. The two approaches have specific challenges and they have different pros and cons.In both models, the number of parameters to estimate has been limited by using expressions, where the physical and mechanical properties of the system, together with operative conditions, have been employed. In particular, for the breakage frequency, a physical model suitable for PBE has been derived and used in the simulations. The traditional attrition distribution of Mersmann has been adapted to a mechanistic breakage daughter distribution, that we were able to use in a PBE. For the secondary nucleation rate, a new expression has been derived and then compared with standard ones[5].ResultsThe physical-based model of Mersmann was successfully integrated into two different population balance models to describe secondary nucleation by attrition for a population of crystals. In order to compare the models, two simulations of a batch, isothermal system have been performed. In both cases, the simulated time was very long, thus resulting in an almost complete depletion of supersaturation. The two models lead to very similar results in the region where supersaturation is high enough to ensure growth and secondary nucleation. However, the breakage model can describe the mechanism of Ostwald ripening, as a result of the embedded size-dependent solubility, when supersaturation is very close to 1, and the simulated time is very long.The two models are solved in two different numerical ways since the population balance in the case of the breakage model is more complex and numerically intensive than the implementation of the boundary condition, which is the only necessary thing to solve for the nucleation model. This required the implementation of two different numerical schemes: the nucleation model has been solved by following a finite volume scheme with boundary conditions for nucleation. The attrition model has been solved by discretizing the growth part with a finite volume scheme and the breakage term with the fixed pivot technique [6,7].ConclusionsThe two models are very similar in the growth regime, thus where secondary nucleation and growth are the dominant phenomena. At extremely low values of supersaturation, thanks to size-dependent solubility, the first model yields to further development of the crystal population, e.g., Ostwald ripening and aging. The main result is that secondary nucleation by attrition can be described as a birth/death term or as a source term according to the final application of the model. Since the two approaches have very different computational intensities, one can choose the right model based on the objective of the simulation study. AcknowledgmentsThis project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program under grant agreement No 2-73959-18. ReferencesAgrawal, S. G., & Paterson, A. H. J. (2015). Chemical Engineering Communications, 202(5), 698–706.Mersmann, A. (Ed.). (2001). Crystallization technology handbook. CRC Press.Gahn, C., & Mersmann, A. (1999). Chemical Engineering Science, 54(9), 1273–1282.Iggland, M., & Mazzotti, M. (2011). Crystal Growth and Design, 11(10), 4611–4622.Bosetti, L., & Mazzotti, M. (2019) Crystal Growth & Design1, 307-319.LeVeque, R. J. (2002). Finite volume methods for hyperbolic problems (Vol. 31). Cambridge university press.Kumar, S., & Ramkrishna, D. (1996) Chemical Engineering Science, 51(8), 1311–1332.Figure. Results of an isothermal desupersaturation experiment. On the l.h.s., schematic representation of the two models. On the center, the evolution of supersaturation (green), and the increase of the zeroth moment (blue for breakage model, red for nucleation model) corresponds to the formation of attrition fragments: in this region, the two models are almost indistinguishable. On the r.h.s., evolution of the initial population with a zoom on the population of fines, where the difference in the mechanisms of formation of nuclei is visible.",14,3.0
"The formation of n-alkane waxes in solution during cold weather is a recurrent problem in the automotive industry. In diesel fuels, crystallisation of long chains n-alkanes can result in the formation of large flat plate-like crystals leading to flow impairment, blockage of filters and engine fuel starvation. Clearly the unwanted formation of these waxes and their associated problems can cause significant issues for the manufacturer and end-users. Current wax mitigation strategies involve the development of chemical additives to control or inhibit the nucleation and growth of crystals so that they no longer form large deposits or block filters. However, the variable nature of fuel composition, specifically the fractionation of the n-alkanes, results in a complex challenge for the additive provider to meet operational needs1. In order to understand crystallisation of waxes in fuels, it is important to understand the crystallisation of n-alkanes in mixtures and singularly. Here we present the results from an experimental study on the nucleation kinetics and solubility behaviour single and mixtures of 8 n-alkanes representative of diesel fuel, ranging from C16H34 to C23H48 in toluene and dodecane. Analysis of the experimental polythermal data making, use of the analytically derived KBHR approach2–4 reveals, that in all bar two cases the preferred nucleation mechanism for single n-alkane solutions is instantaneous. i.e. all the nuclei are formed at once at a certain level of supersaturation, to further grow . Furthermore, the influence of chain length on solubility was observed between odd and even alkanes in both toluene and dodecane. By analysing the nucleation and solubility behaviour within the set of n-alkanes tested, suggestions of the factors controlling wax crystallisation in fuel systems are presented and discussed from first thermodynamic principles.References[1] A. R. Gerson; K. J. Roberts; J. N. Sherwood, Powder Technol., 65 (1991) 243.[2] D. Kashchiev; A. Borissova; R. B. Hammond; K. J. Roberts, J. Cryst. Growth, 312 (2010) 698.[3] D. Kashchiev; A. Borissova; R. B. Hammond; K. J. Roberts, J. Phys. Chem., 114 (2010) 5441.[4] D. M. Camacho Corzo; A. Borissova; R. B. Hammond; D. Kashchiev; K. J. Roberts; K. Lewtas; I. More, CrystEngComm, 16 (2014) 974.",14,4.0
"In this work we investigate computationally the relationship between transient motion of multi-compartment microcapsules and their mechanical properties by flowing them in a microfluidic device. Different active ingredients can be encapsulated separately in inner (daughter) capsules that are surrounded by a larger (mother) capsule. The capsule shell (membrane) can be fabricated with materials that are stimuli (e.g. pH, temperature) responsive. Despite significant experimental advance in the fabrication of multi-compartment capsules [1-4], their deformation mechanisms and rheological behavior are poorly understood. Multi-scale modeling is a tool that facilitates the quality by design (QBD) for the fabrication of pharmaceutical capsules (e.g. Chitosan–alginate microcapsules [5]). QBD of microcapsules increases the probability in targeted delivery and the release of encapsulated materials at desired time and location, and mitigates the risk of cross-contamination between the ingredients. Here, we present the effects of different physical and geometrical parameters such as viscosity ratio, number of inner capsules, their sizes and arrangements on the overall dynamical behavior of multi-compartment capsules, by utilizing our membrane spectral boundary element method [6-9]. We further establish the link between the mechanical properties of these capsules and their deformational behavior, which leads to enhanced manufacturing of these class of drug carriers.[1] Kim, Shin-Hyun, et al. ""Multiple polymersomes for programmed release of multiple components."" Journal of the American Chemical Society 133.38 (2011): 15165-15171.[2] Adams, L. L. A., et al. ""Single step emulsification for the generation of multi-component double emulsions."" Soft Matter 8.41 (2012): 10719-10724.[3] Dams, Stanley S., and Ian M. Walker. ""[5] Multiple emulsions as targetable delivery systems."" Methods in enzymology. Vol. 149. Academic Press, 1987. 51-64.[4] Vasiljevic, Dragana, et al. ""An investigation into the characteristics and drug release properties of multiple W/O/W emulsion systems containing low concentration of lipophilic polymeric emulsifier."" International journal of pharmaceutics 309.1-2 (2006): 171-177.[5] Ghaffarian, Rasa, et al. ""Chitosan–Alginate Microcapsules Provide Gastric Protection and Intestinal Release of ICAM‐1‐Targeting Nanocarriers, Enabling GI Targeting In Vivo."" Advanced functional materials 26.20 (2016): 3382-3393.[6] Dodson, W. R., and P. Dimitrakopoulos. ""Dynamics of strain-hardening and strain-softening capsules in strong planar extensional flows via an interfacial spectral boundary element algorithm for elastic membranes."" Journal of fluid mechanics 641 (2009): 263-296.[7] Koolivand, A., and P. Dimitrakopoulos. ""Deformation of an elastic capsule in a microfluidic T-junction: settling shape and moduli determination."" Microfluidics and Nanofluidics 21.5 (2017): 89.[8] Koolivand, Abdollah, and Panagiotis Dimitrakopoulos. ""Dynamics of Elastic Multi-Compartment Capsules in Microfluidic Channels."" 2019 AIChE Annual Meeting. AIChE, 2019.[9] Koolivand, Abdollah, and Panagiotis Dimitrakopoulos. ""An Advanced 3D Computational Model for Designing Next Generation Drug Carriers."" 2019 AIChE Annual Meeting. AIChE, 2019.",14,5.0
"Liquid phase transmission electron microscopy (LP-TEM) has been widely used to investigate the formation mechanisms of metal nanoparticles in liquid, providing guidance to synthesize nanocrystals with desired structures and functional properties.1, 2 During LP-TEM imaging, the electron beam acts both as the imaging tool and the reducing agent to reduce either metal precursors to form nanocrystals, which enables visualizing nanocrystal formation in real time with nanometer scale spatial resolution. Despite numerous insights gained from these experiments, it is unclear how electron beam induced nanocrystal formation compares to nanocrystal formation during flask-based synthesis. Indeed, a systematic study comparing the chemistry and kinetics of nanocrystal formation during LP-TEM and flask-based colloidal synthesis has not been undertaken. Here we establish ranges of LP-TEM experimental conditions that are representative of flask-based nanocrystal synthesis. We investigate Au/Cu bimetallic nanoparticle synthesis as a model system due to its promising application as a CO2 reduction electrocatalyst.3 Controllable synthesis of alloyed bimetallic nanocrystals is difficult due to differing kinetics and thermodynamics of precursor reduction of different metal species.4 We utilized poly(ethylene glycol) methyl ether thiol (PEG-thiol, Mw=800 g/mol) to complex Au/Cu metal salts into prenucleation complexes (PNC) prior to nanoparticle synthesis.5 Our results showed that flask-based synthesis using sodium borohydride to reduce PNCs formed ~2-3 nm Au/Cu alloy nanocrystals. LP-TEM synthesis, which utilizes radiolysis to produce aqueous electrons and hydrogen radicals as reducing agents,6 formed 3 - 7 nm nanocrystals at low electron dose rates. Imaging at high electron dose rates (high image magnification and beam current) formed irregular aggregated particles. High electron dose rates increase the nucleation rate and create more oxidizing radical species,7 which oxidizes the sulfur group on the PEG-thiol ligand, leading to formation of uncapped aggregated particles. Our study identified key differences between flask-based and LP-TEM synthesis methods, giving important guidance on how to set experimental conditions during LP-TEM to study nanocrystal formation mechanisms at conditions relevant to flask-based synthesis.Zheng, H.; Smith, R. K.; Jun, Y.-w.; Kisielowski, C.; Dahmen, U.; Alivisatos, A. P., Observation of Single Colloidal Platinum Nanocrystal Growth Trajectories. Science 2009, 324 (5932), 1309-1312.Loh, N. D.; Sen, S.; Bosman, M.; Tan, S. F.; Zhong, J.; Nijhuis, C. A.; Král, P.; Matsudaira, P.; Mirsaidov, U., Multistep nucleation of nanocrystals in aqueous solution. Nature Chemistry 2016, 9, 77.Kim, D.; Resasco, J.; Yu, Y.; Asiri, A. M.; Yang, P., Synergistic geometric and electronic effects for electrochemical reduction of carbon dioxide using gold–copper bimetallic nanoparticles. Nature Communications 2014, 5, 4948.Wang, D.; Li, Y., Bimetallic Nanocrystals: Liquid-Phase Synthesis and Catalytic Applications. Advanced Materials 2011, 23 (9), 1044-1060.Marbella, L. E.; Chevrier, D. M.; Tancini, P. D.; Shobayo, O.; Smith, A. M.; Johnston, K. A.; Andolina, C. M.; Zhang, P.; Mpourmpakis, G.; Millstone, J. E., Description and Role of Bimetallic Prenucleation Species in the Formation of Small Nanoparticle Alloys. Journal of the American Chemical Society 2015, 137 (50), 15852-15858.Schneider, N. M.; Norton, M. M.; Mendel, B. J.; Grogan, J. M.; Ross, F. M.; Bau, H. H., Electron–Water Interactions and Implications for Liquid Cell Electron Microscopy. The Journal of Physical Chemistry C 2014, 118 (38), 22373-22382.Wang, M.; Park, C.; Woehl, T. J., Quantifying the Nucleation and Growth Kinetics of Electron Beam Nanochemistry with Liquid Cell Scanning Transmission Electron Microscopy. Chemistry of Materials 2018, 30 (21), 7727-7736.",14,6.0
"Safety is always first priority and a critical concern in the chemical industry. Jiangsu Provincial Emergency Management Administration (JSEMA) in China released <Basic Requirements for Diagnosis and Improvement of Instinct Safety> (Su Emer. NO.53) in June 2019, which specifies the requirements for the main aspects of chemical production. The concept of ""integrated process automation"" in Su Emer. No. 53 poses a great challenge to API manufacturing. It emphasizes mandatory safety and intensified automatic implementation. This paper illustrates the key characteristics in the integrated process automation of API manufacturing and gives some case studies.Such automation system includes sophisticated solution and cutting-edge tools. According to the recipe and process of the product, the consultant optimizes the control strategic with expertise to enhance the safety and meet the regulation requirements. The process control modules or batch control program are deployed in DCS system for the purpose of multifunctional and flexible manufacturing. In case study it gives example of the mixing control strategic in the reactors which is critical and should consider all concerned factors. Usually the geared motor electricity is ignored to monitor, the absence may interfere the cooling system activation in time, its real time monitoring and interlock with cooling media are necessary; another example is the utility facilities configuration and interlock with automatic systems which should be tuned duly to eliminate the hidden trouble. The integrity of safety instrumented functions (SIFs) should be determined through risk assessment tools and rating tools, such as HAZOP (Hazard and Operability Analysis) and LOPA (Layer of Protection Analysis). The related case study reports sharing control components between SIS and DCS will increase the possibility of failure, which should be validated. The allocation of center control room and controller assignment should be concerned with HSE guideline. It also discusses the roadmap of information technology and MES (Manufacture Execution System) for API manufacturing. Our implementation shows that the integrated process automation system can help to reach instinct safety, which increase the productivity and improve the quality of products with regulatory compliance.",15,0.0
"Introduction:The Cour NanoParticle (CNP) platform has been deployed in novel therapies at various stages of the drug development pipeline, including a promising treatment for celiac disease. CNP technology utilizes a stabilized biodegradable PLGA matrix encapsulating protein antigen(s) resulting in nanoparticles for inducing immune tolerance. In this study, we aimed to elucidate the whole body and tissue-resolved absorption/distribution/metabolism/excretion (ADME) pharmacokinetics of antigen-encapsulating CNPs in vivo. Furthermore, a cellular receptor-binding screen was performed using cell microarray technology to detect specific interactions between CNPs and a library of human plasma membrane and tethered secreted proteins. Methods: Model CNPs encapsulating Alexa-Fluor 647-conjugated ovalbumin were synthesized by a double-emulsion solvent-evaporation technique. They were injected via the tail vein to BALB/c mice, which were then sacrificed and imaged over a time course (n=3 mice/time point) up to 48 hours post-injection using 2D optical tomography. Whole body imaging (WBI) and whole organ imaging (WOI) was conducted for each mouse. For the receptor binding screens, the same fluorescent CNPs were pre-incubated with human serum and then added to slides spotted with HEK293 cells transfected to over-express a library of 5484 human plasma membrane proteins and cell surface tethered secreted proteins (16 slide sets, n = 2 slides per slide set). Fluorescence was analyzed on ImageQuant to determine binding of CNP to the specific slide spots over background. Results: The CNPs were characterized by dynamic light scattering (DLS) to be 688 nm in average diameter (PDI = 0.2) and -37 mV in average zeta potential. The fluorescent protein loading was spectrophotometrically determined to be 10.7 µg/mg PLGA (encapsulation efficiency = 53%). WBI signal peaked one-hour post-injection and returned to baseline after 24-hours. The highest fluorescence intensities were observed in the liver, spleen, and kidneys, which exhibited peak signal 15-60 minutes post-injection, depending on the organ. Additionally, cell microarray binding screens yielded an initial target list of 33 plasma membrane proteins that putatively interact with CNPs. Confirmatory screens for interaction specificity yielded a final set of 15 plasma membrane proteins that specifically interact with CNPs, which was enriched in known immune functions such as phagocytosis, apoptotic cell clearance, and inflammation regulation as determined by gene ontology analysis. Conclusions:This study provides initial data on the ADME pharmacokinetics of CNP immunotherapies in vivo. CNP circulation in vivo peaked at 1-4 hours, and the particles were cleared from circulation by 24 hours. Organ-specific imaging supports a hypothesis of rapid particle uptake by phagocytes in the blood, resulting in their redirection to the spleen and liver. Cellular receptor binding screens further support the hypothesis that particles are taken up and re-directed by phagocytes in the blood, as the majority of specific interaction partners were found to be receptors prevalent on immune cell populations that are known to mediate key immune phagocytic and regulatory processes.",15,1.0
"Sodium triacetoxyborohydride (STAB) is a common reducing agent for reductive amination reactions. Due to sensitivities to water and air, the potency of this reagent tends to degrade over time. Traditional analytical assays for STAB fail to accurately assess the content of the active borohydride species, and often report assay purities of 97% or greater, when the true potency is lower. An incorrect or unknown potency of the STAB being used may lead to significant challenges when scaling up reductive aminations. An excess of STAB may be required and reactions may not achieve full conversion with the amount of STAB charged, requiring additional charges. The use of an excess of STAB comes with greater raw material costs and the thermal and H2 evolution hazards associated with quenching the excess reagent.This presentation outlines a simple assay, based on an aldehyde reduction, that was developed to determine the active borohydride content of STAB. The HPLC assay yield of a salicylaldehyde reduction has been shown to accurately determine this potency and has been validated against an H2 evolution method, as well as yields obtained from a reductive amination.",15,2.0
"The microbiome is a collection of bacteria, viruses and fungi that perform essential functions for human health. Dysbiosis, or the dysregulation of the microbiome, can contribute to chronic inflammation, metabolic disorders, neurological diseases, and cancer. Clinically, one of the only methods for altering the composition of the microbiome is with broad-spectrum antibiotics, which contribute to dysbiosis and create selective pressure for antibiotic resistant pathogens. As such, live therapeutic bacteria (LTB) have been identified as an attractive alternative to antibiotics, as they can modulate the composition of the microbiome without eliminating commensal species. However, LTBs are commonly delivered orally and experience significant challenges during gastrointestinal (GI) transit, including survival in the harsh conditions of the stomach, adherence to the intestinal lumen, and navigation of interpatient variability in the microbiome due to factors such as diet. Few engineered drug delivery platforms have been rationally developed to address these challenges. In this work, we describe two complementary platforms that separately adapt LTBs to improve viability during gastric transit and modify LTB surfaces to aid in adhesion along the GI tract. In the first system, we sought to understand the influence of factors such as diet on LTB adaptation and colonization. We delivered an LTB to female BALB/c mice on two distinct diets and found that diet significantly influenced both the appearance of a small colony variant of the LTB, as well as its distribution along the GI tract. As adaptation appeared to significantly affect LTB colonization, we developed an approach to control the adaptation of LTBs to specific environmental insults during GI transit. We found that growth in an in vitro adaptation media can prime LTBs for the acidic conditions of the stomach, significantly improving their viability during a challenge in simulated gastric fluid. Following transit through the stomach, LTBs must adhere to the intestinal lumen to persist and colonize in the GI tract. Therefore, for our second system, we developed a platform to chemically conjugate targeting ligands to the surface of LTBs to improve their adhesion to the GI mucosa. This platform significantly improves both the rate of colonization, as well as the concentration of LTBs in the GI tract of female BALB/c mice. Collectively, this work represents a simple approach to adapt LTBs to the intestinal environment during in vitro growth, while enabling subsequent modification of the LTB surface for improved intestinal adherence. Together, these delivery techniques for LTBs may help overcome challenges associated with GI transit, colonization and adaptation.",15,3.0
"Although much scientific effort has been expended for cancer treatment, this disease is still one of the leading causes of death around the world. The limitations and shortcomings of current therapies highlight the urgent need for more effective therapeutic strategies. So far, most of the chemotherapy drugs act negatively on the cell division without selection, leading to serious adverse effect on normal cells. The current mainstream of anti-cancer drug development is to find the drugs that have specific targets (DNA, mRNA or proteins). Those targets are overexpressed in cancerous cells and are critical for carcinogenesis. In this study, we engineered breast and lung cancer cells to express the enhanced green fluorescent protein (EGFP) under cancer-specific gene promoters for three known carcinogenesis genes - survivin, Bcl-2 and hTERT, which enable online high-throughput screening of drugs that may up- or down-regulate these cancer genes as indicative of efficacy in inhibiting breast and lung cancers. Additionally, the mechanism of drugs and the crosstalk between cancer genes can be elucidated at the early stage of drug discovery. Another issue in the pharmaceutical industry is the high drug failure rate owing to poor predictability of early drug assays in a two-dimensional cell culture system. In such traditional assay, multiple cell-cell and cell-matrix interactions are deficient, leading to incomprehensive cellar responses to drug agents. To improve the drug assay predictability, our assay has focused on the construction of three-dimensional biomimetic cancer cell models with the recapitulation of multiple elements of tumor microenvironment. While developing the new drug screening assay, we also combined the rapid cell detection approach to achieve the high-throughput capability. Our optimized drug screening process can help to discover more anti-cancer therapies.",15,4.0
"Novel, surface modified excipients are prepared using a fluid energy mill (FEM) to simultaneously mill and dry coat microcrystalline cellulose (MCC) with nano-silica to produce different sizes of fine, well flowing MCC excipient [1]. The purpose of these new class of excipients is to enable direct compression (DC). DC route is one of the most efficient and economic methods for tablet manufacturing since only blending and compression operations are required, greatly simplifies the process-train. However, the popularity of direct compression is limited by its stringent requirements, such as good flowability, high packing density, and excellent compaction for excipient-active pharmaceutical ingredient (API) blends. When the blend has high drug loading with a poorly flowing API, the flowability and tabletability of blends is generally decreased sharply. In a recent work, it has been shown that surprisingly, fine excipients of 20 mm size that have improved flow and bulk density after silica dry coating, can greatly improve direct compression performance [2]. Unfortunately, it could not work for very fine cohesive APIs such as 10 mm acetaminophen (APAP) at higher drug loading. In this work, binary blends of several fine sized dry coated MCC-based excipients are considered with three different fine, cohesive APIs at high drug loadings. Bulk density, FFC, and tablet compactability for all these blends are measured and compared to determine which excipient sizes provide the best overall performance. The results are analyzed for the relative sizes of the excipients and APIs to better understand the interplay between the components of the blends and assessment of the extent of silica transfer from coated excipient powders to API powders and its effect on blend bulk density and flowability. It is expected that using such novel surface engineered fine excipients in blends with fine APIs may also help in reduced segregation tendency during overall processing. Hence, these results are expected to provide industry relevant guidelines for the use of finer surface modified excipients in DC tableting.",15,5.0
"A general machine learning feature selection protocol to predict antibody stabilities is proposed. These features only require information from sequences and molecular structures of antibodies. In addition, the physically relevant features are easily interpretable and help to design experiments to understand the underlying mechanisms of antibody stabilities. Two case studies are demonstrating using this protocol. First, we applied this framework to predict aggregation behaviors of 21 marketed monoclonal antibodies at high concentration (150 mg/mL), yielding an accuracy of 97% on validation tests with only three features. Models trained using structural features, rather than sequence features, correlated better with aggregation. Second, the concentration dependent viscosity behavior of 27 FDA approved antibody drugs was measured in concentrated solutions at high concentration (150 mg/mL). Combining molecular modeling and machine learning feature selection, we found that the net charge in the antibodies and the amino acid composition in the variable region are key features which govern the viscosity behavior. We presented two predictive models for aggregation and viscosity based on machine learning trained models, facilitating early stage design.",15,6.0
"The ability to elicit potent humoral and cellular immune responses is a key element required for the development of efficacious prophylactic and therapeutic vaccine formulations. For vaccines against intracellular pathogens, the induction of a balanced, long-lived, and potent CD4+ and CD8+ T cell activation and proliferation is critical, combined with robust antibody responses. One strategy is to activate innate immunity and proinflammatory responses through Toll-like receptor-independent cytosolic pathways. Importantly, engagement of these mechanisms has demonstrated its ability to provide protection against bacterial challenge, and several small molecules have been evaluated in preclinical and clinical studies as vaccine adjuvants against infectious diseases.Small molecules can bind to cytoplasmic receptors triggering signaling cascades that activate innate immunity. Despite their potential as adjuvants, small molecules have some limitations that need to be overcome. For example, there may be low exposure in target tissues due to their size and charge, and are prone to enzymatic biodegradation; with some off-target toxicities. In order to address these challenges, the use of drug delivery systems is being explored. In this work, three different polymeric nanoparticle platforms were evaluated for small molecule delivery and for their capability to act synergistically as co-adjuvants and elicit robust immune responses. In these studies, nanoparticles based on natural and synthetic polymers were fabricated using water/oil/water and oil/water emulsions. Particle characterization was performed using dynamic light scattering, scanning electron microscopy, and zeta potential measurements that confirmed the size, morphology, and charge of the resulting nanoparticles. Small molecule compounds were loaded into each of the platforms, and their loading and release efficiency were determined using liquid chromatography. Finally, these systems were evaluated using in vivo murine models following parenteral administration to assess their ability to induce cytokine secretion, antibody and cell-mediated immunity. The results obtained demonstrate the potential of polymeric particulate systems as dual delivery and co-adjuvant platforms for the development of small molecule-adjuvanted prophylactic and therapeutic vaccines.",15,7.0
"Disk stack centrifugation is one of the main clarification steps used in large scale mammalian cell culture processes. During process development, pilot scale disc stack centrifuge is normally used to develop and characterize the harvest process for new cell culture processes due to the lack of smaller scale model for disc stack centrifugation. In order to increase speed of development, a small scale model (SSM) of a disk stack centrifuge is of high demand to enable harvest development in an earlier stage of process development. A small scale model can also improve throughput and reduce cost. In this study, we have developed a SSM for disc stack centrifugation using a capillary shear device combined with benchtop centrifugation. Capillary tube size, length, and flow rate were optimized to generate energy dissipation rates comparable to large scale disk stack centrifuges. Additionally, shear force from multiple centrifuges at different scales were standardized and compared. Mathematical correlations were established for energy dissipation, viable cell density, cell viability, and lactate dehydrogenase (LDH) activity. Our SSM offers an approach to understand how the separation process, such as centrifuge designs, sizes, and configurations affect filtration performance and product quality. This work can speed up harvest development during process scale-up in biopharmaceutical mammalian cell culture processes.",15,8.0
"Pharmaceutical, Biopharmaceutical, Agrochemical and Specialty chemical companies make many different products in their plants. This is true both for the active ingredient stages as well as the secondary of formulation processing. Cleaning and set up between products is critical and reduce productivity time up to 50% in some cases. The presentation will show the critical areas important in the modelling development in this expert system and how it will aid companies in scheduling their productions to reduce cleaning time and to reduce issues with cleaning. ",15,9.0
"Pharmaceutical, Biopharmaceutical, Agrochemical and Specialty chemical companies make many different products in their plants. This is true both for the active ingredient stages as well as the secondary of formulation processing. Cleaning and set up between products is critical and reduce productivity time up to 50% in some cases. Microbubbles have been shown to enhance many unit operations e.g. distillation. The benefits of microbubbles in cleaning will be presented along with the impact this will enable in regulatory/ registration processes.",15,10.0
"The present work describes the journey of a reaction co-development between a pharma company and a CDMO since the transfer from UCB to large scale production at Hovione. It comprises the transfer of a kinetic model developed by UCB to Hovione, the experimental confirmation of the model by Hovione, a screening of conditions for a better understanding of the design space and, finally, the optimization and scale-up of the reaction to a >100 kg scale.To initially confirm the kinetic model provided by UCB (both the UCB and Hovione used Dynochem for modelling) it was necessary to execute experimental work that can provide enough variability regarding the reaction profiles. At Hovione, we have compiled a workflow/set of experimental conditions that is suitable to a wide range of reactions types and can capture most critical physical and chemical transformation rates during a reaction (e.g. product converted into impurities, reagents reacting with other species (aside from starting raw material), mass transfer limitations). This set of experiments was used to confirm the kinetic model and afterwards we were able to explore the design space using a design of experiments approach.The DoE was a Plackett-Burman design with 4 targeted parameters namely reagent equivalents (upper and lower boundaries set by economic and quality targets), solvent volumes, reaction temperature and acid equivalents. In order to minimize for scale-up issues, the DoE used heating rates similar to the ones used at production scale. Additionally, a maximum reaction time of 33h was used, which enabled to reach a 99% yield for all experimental conditions of the DoE. The kinetic model was able to accurately describe all the experiments carried out during the DoE and was used to define the critical variables proven acceptable ranges (PAR) and normal operating ranges (PAR). The isolated product showed a 99.0 % w/w assay as demonstration trial in the lab, while at a >100 kg scale it was possible to achieve a 99.7 % w/w assay.It was proven that by resorting to a mechanistic modelling approach and a reduced number of experiments we were able to save resources (time and materials) and generate more knowledge than the traditional approach and have a more straightforward way to reach production scale with Right-First-Time batches.",16,0.0
"Digital design of manufacturing processes using mechanistic models is fast becoming an essential tool during Active Pharmaceutical Ingredient (API) process development. It enables rapid and effective exploration of the decision space for Critical Process Parameters (CPP), helping to reduce risk and product time-to-market, thus aiding in the effective and safe production of high-quality pharmaceutical products.This work involves the application of mechanistic modelling of a fed-batch reaction process to help support process scale-up. The reaction kinetic modelling was combined with mixing and heat transfer effects in order to describe the reaction process at laboratory scale using a RC1 reactor. Scale-up calculations were performed, which included optimization of process parameters to ensure that the exothermic reaction was performed within the temperature limits recommended for the reactor. The main of objective of this activity was to assess the process of developing and scaling up a reaction process in a CSTR from reaction kinetics regression to inclusion of mixing and heat transfer effects as a function of scale. Additionally, this work intended to develop an industrial workflow to document how mechanistic reaction modelling could be utilised in this case. This workflow involved the following steps:Regress the kinetic parameters of a CSTR that combines the reaction kinetic modelling with mixing and heat transferAssess the scaling up capability of the modelOptimize the process parameters to obtain the desired qualitiesUpon completion of step 2, the validated mechanistic reactor model was utilised to assess the impact of dosing flowrate on the maximum temperature observed during the reaction process with increasing scale. In this process, the feed rate is used to control the temperature in the reactor. To study this impact, a number of uncertainty analyses were performed to evaluate the reactor temperature with varying feed flowrate. The dynamics of the reactor temperature can be seen in Figure 1 below across three productions scales, namely 80 kg, 400 kg and 800 kg scales. These predictions can be used to define the minimum and maximum reactant feed rates for each scale to ensure operation of the reaction within the desired range of temperatures, shown by the highlighted region in the plots below.",16,1.0
"Multiple commercially available mixing modeling tools were utilized to scale-down a reactive crystallization from the planned process performance qualification (PPQ) scale to the laboratory scale. Differences in the historical crystallization batch data with results in standard laboratory crystallization set-up drove the need to fully characterize the lab scale setup to ensure it was representative of the PPQ equipment train. The mixing modeling tools were used to model mixing at both the PPQ and laboratory scales system and compared with experimental and batch data for physical properties (particle size and surface area) of the product. These tools encompassed both correlation-based and computational fluid dynamics based modeling (DynoChem, M-Star, and MixIT). In this presentation, the modeling results of numerous laboratory scale conditions and setups as well as large scale batch data were used to understand micro-mixing, meso-mixing, energy dissipation rate and strain rates. The modeling results were compare with experimental physical property data to give insight into the strengths and weaknesses of using these tools in identifying laboratory conditions and setup of the reactive crystallization.",16,2.0
"Selecting the recommended centrifugation parameters for full scale manufacturing before the first site run is key to a successful technical transfer. Centrifugation scale-up poses unique challenges due to the differences in centrifuge geometry, rotational speed, and automation levels at different manufacturing sites and lab set-ups. Using a combined approach modeling, lab experiments, and pilot plant data, starting parameters for manufacturing could be selected for loading rate, loading spin speed, and final speed. The recommended starting values were then used to shape the optimization at full scale.",16,3.0
"The ability to model a large, commercial-scale process at a smaller scale in the laboratory is a valuable tool to increase the efficiency of process development. Characterizing the mixing process at commercial scale can be particularly challenging with sterile products where process monitoring tools and sampling can be a risk to the integrity of the sterile boundary. Having a lab-scale mixing model predictive of the large scale operation will enable more effective small scale batches to decrease the number of commercial scale batches needed during process development, consequently saving time, materials, and other costs. Additionally, the use of computational fluid dynamics (CFD) to create a virtual model can further reduce the number of small scale batches needed as well.A scale-down model is being developed using CFD between a commercial scale 700L tank and a development scale 10L tank. These tanks are of stainless steel construction with bottom-mounted agitators used for mixing and formulation of sterile drug products. The primary responses investigated in this liquid-liquid mixing study was time to homogeneity as impacted by varying viscosities and densities of two equal-volume constituent components, mixing speed, and tank fill level. Raman and conductivity probes were utilized to collect real-time data throughout the mixing process, providing value in characterizing liquids that mix too quickly to be conducive to offline sampling. Experiments were designed considering statistical principles in conjunction with ranges of Richardson and Reynolds numbers to interrogate the boundaries between safe operating spaces and regions of failure. These experiments are executed physically in the tank and compared with corresponding CFD simulations to validate the model.",16,4.0
"The agitated filter dryer (AFD) is a critical technology in the pharmaceutical industry for isolating potent active pharmaceutical ingredients (API) post crystallization. There the filtration and drying unit operations take place in a single contained vessel minimizing operator exposure. The central challenge in utilizing AFDs effectively is designing protocols to produce API powders with residual solvent content uniformity and targeted particle size distributions that affect the performance of drug product processes through flowability, blendability, compatibility, etc..Scale-up/tech-transfer of AFDs is made difficult by the variety of different geometries available at the lab, pilot and plant scales. The differences in geometry and scale affect both the heat transfer and stresses experienced by API, and which in turn affects the physical properties of the resultant material, namely through agglomeration and attrition. Moreover, the isolation inherent to AFDs makes direct probing of plant and pilot scale vessels challenging.In this work, we present several computational approaches to investigate the physics present in the spatially varying local environments within AFDs, such as the heat transfer, gas-liquid phase change, mixing, agglomeration and attrition. These investigations utilize a variety of computational tools such as discrete element method, computational fluid dynamics, and machine learning. We will present heuristics and metrics for designing and optimizing AFD protocol at a given scale and considerations for scale-up and tech-transfer.Disclosures: EM, KS and NN are AbbVie employees and may own AbbVie stock. MM is an employee of Tridiagonal Solutions. RB is an employee of Engineering Solutions and Scientific Software. SS is a Professor and VB is a PhD student at Iowa State University and have no additional conflicts to disclose. AbbVie has provided research funding to Tridiagonal Solutions and Iowa State University. AbbVie, Tridiagonal Solutions and Iowa State University contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final abstract.",16,5.0
"In this talk, I will present our recent work on BodyNet concept for stretchable wearable and implantable sensors. Examples include, full body sensing, blood flow sensor and morphing electronics - neurostimulator.",17,0.0
"The large-scale deployment of biomonitoring sensors through integration in wearable consumer electronics enables population-level health and wellness monitoring, and thus can play a critical role in transforming personalized medicine. Currently, physical sensors have been widely incorporated within commercialized wearable platforms, but they cannot capture biomarker information that lie in molecular levels and provide highly specific information about the body’s dynamic chemistry. Electrochemical biosensing interfaces can be positioned to target biomarker molecules in a wearable format, as they can be constructed on miniaturized/flexible footprints/substrates and can convert (bio)chemical signals into electrical signals measurable by electronic readouts. The contact pads on the printed circuit board (PCB), which is a core component of wearable consumer electronics, can directly serve as electrodes and as a substrate for electrochemical sensors. However, we found that the commercialized PCB metal contact pads (e.g., gold) were prone to corrosion and unsuitable for electrochemical modifications in solution environments. Additionally, electrochemical biosensors often are not reusable and need to be replaced frequently, while PCB units are not positioned for disposable applications. To this end, we devise an adhesion-based anti-corrosion strategy for PCB-interfaced electrochemical sensing, which enables seamless sensor-system integration. The strategy is based on decoupling the sensing interface and PCB metal contact pads by an intermediary ultrathin (~50 µm), vertical conductive, and double-sided adhesive film. This strategy has dual benefits of making electrochemical sensors disposable and insulating the contact pads from sounding solutions. We first establish the suitability of the adhesion film to serve as an electrode and as an interconnection film simultaneously. One side of the double-sided adhesive film was patterned with noble metal electrodes, followed by the modifications with recognition layers, such as permselective membrane and enzymes, to render biosensing functionality. The electrochemical biosensors were transferred and adhered to the PCB contact pads with the aid of water-soluble tape. As compared with bare PCB contact pads, this fabrication strategy allows for realizing a corrosion-resistant and stable electrochemical biosensing interface on PCB for operation in biofluid (e.g., sweat). Amperometry and open circuit potential characterization experiments validated the electrochemical corrosion resistance properties in saline solution environments. The excellent anti-corrosion attributes of the modified film could be ascribed to: (1) the polymer glues on both sides of the adhesive film acting as corrosion-protective layers; (2) bypassing the need for oxidation prone-adhesion metal (e.g., chromium), achieved by e-beam evaporation (at the noble metal patterning step); and (3) the electrodeposited permselective membrane (here, phenylenediamine), which has been reported to presented anti-corrosion properties. We specifically tailored the enzymatic layer of the electrochemical interface to target biomarker molecules such as glucose, lactate, and choline as example of informative metabolites and nutrients. By modifying different (bio)chemical layers various other molecules can be targeted. In this way, our strategy serves as a foundation for electrochemical sensor development and integration with wearable consumer electronics.",17,1.0
"Traditional approaches for the detection of trace-level radioisotopes in water require lengthy offsite sample preparations and do not lend themselves to rapid screening. Therefore, a novel sensor platform that combines onsite purification, concentration, and isotopic screening with a fieldable detection system will be an invaluable tool. In this presentation, we will discuss findings on the development of a sensor platform based on reactive polymer film coatings for the rapid isotopic screening of waterborne radionuclides. Nanothin coatings were prepared by grafting uranium-selective polymers from polyethersulfone (PES) surfaces via UV-initiated polymerization, and by introducing uranium-selective functional groups to polyacrylonitrile (PAN) surfaces by chemical reaction. Ellipsometry was used to study film growth kinetics on PES films. X-ray photoelectron spectroscopy of modified PAN films revealed conversion of nitrile groups to amidoxime groups to be as high as 40% and showed that the extent and depth of reaction could be varied precisely. Static uptake experiments with solutions of depleted uranium spiked with 233U were conducted to determine uranium binding capacities and kinetics of the modified polymer films at different pH values from 4 to 8. Sorption isotherm data were fitted to the Langmuir model, and the highest sorption capacities were obtained at pH 6 for modified PAN (M-PAN) and PES (M-PES) films. Capacities at pH 4 and 8 were lower and could be explained by differences in sorption mechanisms. Uranium batch uptake kinetics followed a pseudo-second order rate model. Alpha spectroscopy pulse height spectra were analyzed to study the role of selective layer film thickness on peak energy resolution. Excellent resolution with full width at half maximum values from 29 to 41 keV were recorded for M-PAN film and from 26 to 45 keV for M-PES film. Whereas uranium uptake increased with selective layer film thickness and varied with polymer chemistry/extent of modification, the peak energy resolution was independent of layer thickness and polymer chemistry within the experimental measurement uncertainties.Results from this work were used to modify PES and PAN ultrafiltration membranes with the same experimental conditions to develop uranium selective membranes. High-throughput uptake experiments were performed to study the effect of residence time on detection efficiency and isotopic resolution. This membrane-based detection method enables the rapid, fieldable screening analysis of radionuclides in water for environmental monitoring studies and nuclear forensics investigations.",17,2.0
"Many small molecule pollutants interact with human hormonal systems. These compounds, known as endocrine disruptors, are ubiquitous, as they are found in plastics, medications, and pesticides. Such chemicals are especially problematic because they have been linked to severe health problems, including cancer, infertility, and diabetes. We have developed two complementary electrochemical methods to rapidly detect and quantify these compounds. One technique involves the specific monitoring of Bisphenol A (BPA) using a DNA aptamer immobilized on an electrode. The second method involves the measurement of the biological activity of chemically dissimilar endocrine disruptors using a human hormone receptor as a component of an electrochemical sandwich assay. These electrochemical platforms require no specialized skills to implement and have enabled very sensitive detection (sub-ppb levels) of these environmental pollutants from complex solutions, such as baby formula.",17,3.0
"Graphene-based flexible electronic devices for sensor applications has been receiving growing interest due to unique properties of graphene.1 Various fabrication methods, such as photolithography, printing and chemical vapor deposition (CVD), has been used to produce high resolution and low feature size graphene-based devices.2, 3 However, most of these methods are complex and require multiple processing steps (i.e. stamping, vacuum drying and etching) as well as additional post treatments (i.e. high temperature (300-1000 °C) baking or laser annealing) that can thermally or chemically degrade polymer-based substrates limiting substrate material selection.2, 3 In addition, these methods can only be applied to 2D planar substrates and are not feasible to fabricate 3D circuits and some of them, such as sticky/adhesive tape peeling method, require high amounts of graphene consumption.4 Despite the recent progress in the field, there is still a need to develop novel fabrication approaches that can address the current limitations of the existing methods.In this study, we present two flexible electronics device fabrication methods based on simple polymer casting and microfluidics approaches.5, 6 The first method focuses on simple polymer casting based graphene transfer at room temperature that does not require any post-processing. Briefly, our method consists of two main steps; (i) the formation of graphene patterns/films on substrates/molds via conventional methods such as CVD, channel filling or ink-jet printing and (ii) direct casting of target substrate polymer solution on the substrates/molds with graphene micropatterns and direct graphene transfer to the target substrate via peeling off upon drying and film formation. This method simply relies on the differences in the surface energies and adhesive forces between the graphene/mold and graphene/target polymer substrate.5 Alternatively, the second method concentrates on a novel microfluidic approach to fabricate high resolution and low feature size graphene-based circuits/devices.6 This method involves controlled and selective filling of microchannels on substrate surfaces with a conductive binder-free graphene nanoplatelets (GNP) solution. The ethanol-thermal reaction of GNP solution at low temperatures (~75 °C) prior to microchannel filling (as called “pre-heating”) potentially reduces and homogenize GNP, which in turn enhances conductivity and reduces sheet resistance (down to ~0.05 kΩ sq-1). Therefore, harsh post-processing of GNP is eliminated, and room temperature fabrication is enabled, which allows the use of degradable substrates, including biodegradable and natural polymers. This simple process is followed by selective administration of conductive GNP solution to the predetermined microfluidic channels on flexible substrates in a controlled manner via a syringe pump to create high resolution and low feature size graphene circuits/devices. This microfluidic approach can also be used to create 3D circuits with different geometries, using combined approaches with 3D printing, which is difficult to obtain with other methods (i.e. inkjet, screen or aerosol printing) that work mainly with 2D planar substrates. 6We demonstrated that using these fabrication methods, we obtained high resolution and low feature size graphene circuits with different geometries, dimensions and 3D superficial microstructures. We investigated the effect of temperature and microfluidic parameters (such as concentration, volume and flow rate) as well as the type of substrate materials on the graphene structure and formed micropatterns. The XPS, Raman, SEM and TEM results validated the structural changes of graphene along with confirming the stable presence of graphene patterns on the target substrates.5, 6 The feature sizes of the graphene patterns could range from a few micrometers (down to ~15 µm in width and ~5 µm in depth) to a few millimeters with very small amounts of GNP used (~2.5 mg of graphene to obtain ~0.1 kΩ sq-1 of sheet resistance for 1 cm2). 5, 6 The generated graphene patterns demonstrated significant stability after multiple washing and bending cycles. These methods were also implemented using other conductive inks, such as conductive graphene-silver based composite ink.5, 6 Room temperature processing also provided precise control of 3D microstructural and mechanical properties (such as film porosity, pore size, elasticity etc.) of the target substrate materials, as demonstrated by SEM images.5, 6 Using these methods, we successfully fabricated pressure sensors and near field communication antennas. 5, 6 Our results demonstrated that the devices worked with high sensitivity and stability. In conclusion, the circuits/devices fabricated using this method can easily be implemented as wearable electronic, sensors and batteries, robotic components or motion detectors. In a broader sense, these simple, environmentally friendly and low-cost methods could pave the way for the fabrication of 2D and 3D electronic circuits/devices on different degradable flexible substrates using various graphene-based conductive inks. References:Jang, H.; Park, Y. J.; Chen, X.; Das, T.; Kim, M. S.; Ahn, J. H., Graphene-Based Flexible and Stretchable Electronics. Advanced Materials 2016, 28 (22), 4184-4202.Das, S. R.; Nian, Q.; Cargill, A. A.; Hondred, J. A.; Ding, S.; Saei, M.; Cheng, G. J.; Claussen, J. C., 3D nanostructured inkjet printed graphene via UV-pulsed laser irradiation enables paper-based electronics and electrochemical devices. Nanoscale 2016, 8 (35), 15870-15879.Das, S. R.; Srinivasan, S.; Stromberg, L. R.; He, Q.; Garland, N.; Straszheim, W. E.; Ajayan, P. M.; Balasubramanian, G.; Claussen, J. C., Superhydrophobic inkjet printed flexible graphene circuits via direct-pulsed laser writing. Nanoscale 2017, 9 (48), 19058-19065.Oren, S.; Ceylan, H.; Schnable Patrick, S.; Dong, L., High‐Resolution Patterning and Transferring of Graphene‐Based Nanomaterials onto Tape toward Roll‐to‐Roll Production of Tape‐Based Wearable Sensors. Advanced Materials Technologies 2017, 2 (12), 1700223.Uz, M.; Jackson, K.; Donta, M. S.; Jung, J.; Lentner, M. T.; Hondred, J. A.; Claussen, J. C.; Mallapragada, S. K., Fabrication of High-resolution Graphene-based Flexible Electronics via Polymer Casting. Scientific Reports 2019, 9 (1), 10595.Uz, M.; Lentner, M. T.; Jackson, K.; Donta, M. S.; Jung, J.; Hondred, J.; Mach, E.; Claussen, J.; Mallapragada, S. K., Fabrication of Two-Dimensional and Three-Dimensional High-Resolution Binder-Free Graphene Circuits Using a Microfluidic Approach for Sensor Applications. ACS Applied Materials & Interfaces 2020, 12 (11), 13529-13539.",17,4.0
"Crystallization is a complicated molecular recognition and self-assembly process that is completely managed by manipulating macroscopic process variables. Oftentimes, the crystallographic state of the resulting product is critial to its performance. The efficacy of products as diverse as pharmaceuticals and pesticides is often very sensitive to polymorph, for example. In addition, the particle size distribution depends on multiple process parameters in complicated, often nonlinear ways. As many crystallized products have become more advanced, with increasing demands on product performance, industrial crystallization has had to adapt. Pratitioners tend to come to the field from either process engineering or solid state chemistry. While it is possible to stay in one’s own “lane” and contribute to the field, most successful practioners today are at least conversant with both sides of the chemistry/engineering “divide”. Cross-training and collaboration often contribute greatly to successful crystalline product development and scale-up.  I come to industrial crystallization from process engineering, but have come to appreciate the necessity of understanding solid-state chemistry to adequately develop processes and products. I’ll illustrate the interplay between process engineering and solid-state chemistry with several examples from several industries, and review where I think continued collaboration can lead to crystalline product control and improvement.",18,0.0
"Tunable reactions can be used to control precisely the generation of supersaturation in reactive crystallization systems. Enzymes, with high reaction specificity and predictable rate enhancement, enable fine control over a number of reactions, and have been demonstrated in several reactive crystallization systems. In this study, penicillin G acylase (PGA) is used to synthesize and simultaneously crystallize cephalexin, starting from two cephalexin precursors, in a controlled manner. It was recently shown that cephalexin, which typically forms high-aspect-ratio needle-shaped crystals, can form lower-aspect-ratio crystals when supersaturation is generated in a slow, controlled manner (Li et al., 2019). These lower-aspect-ratio crystals have many desirable properties including faster filtration (by an order of magnitude) and improved powder flowability and handling. This study aims to show that enzyme control of cephalexin reactive crystallization can produce crystals with a more desirable aspect ratio while also providing the benefits of combined synthesis and separation (greater productivity, conversion). The mechanism of PGA synthesis of beta-lactam antibiotics such as cephalexin is known (McDonald et al. 2017); here an immobilized form of PGA is used to enable recycling of the enzyme. Use of process analytical technology (PAT) combined with a reaction-diffusion-crystallization model (Salami et al. 2020, in preparation) lends insight into the phenomena critical to successful implementation of this novel means of supersaturation control. In situ microscopy and focused beam reflectance measurement (FBRM) show that the amount of crystal surface area is important to both aspect ratio and secondary nucleation. The model suggests that the amount of enzyme, and unexpectedly the immobilized bead size, is critical in dictating the reaction timescale. In general, this study demonstrates how catalysts can lend control over the crystallization process, giving process designers an additional knob to turn in the synthesis of new routes to important pharmaceutical products. Li, M., et al. (2019). ""Optimizing the Aspect Ratio of Cephalexin in Reactive Crystallization by Controlling Supersaturation and Seeding Policy."" Transactions of Tianjin University 25(4): 348-356. McDonald, M. A., et al. (2017). ""Enzymatic reactive crystallization for improving ampicillin synthesis."" Chemical Engineering Science 165: 81-88. Salami, H., et al. (2020). “Model development for enzymatic reactive crystallization of β-lactam antibiotics, a reaction-diffusion-crystallization modeling approach.” Computers & Chemical Engineering in preparation",18,1.0
"One of the basic tenets of the classical theories of crystal nucleation and growth is the Szilard postulate, which states that molecules from the supersaturated phase join a nucleus or a growing crystal individually. Over the years, numerous crystals have been found to nucleate by assembly of monomers and grow by sequential association of monomers to growth sites. Recent experiments in bio- and geomimetic environments have spotlighted a partial deviation from the Szilard rule, a bimodal distribution of growth species that arises from the assembly of ordered or amorphous structures in the solution, which then incorporate into the crystal as a whole. Here we report a complete violation of the Szilard rule. In octanol solutions of etioporphyrin, a planar molecule whose crystals have attractive electronic and optical properties, the solute populates a continuum of quasi-parallel dimers that differ from the conformation in the crystal by shift and orientation. Molecular absorption and luminescence spectroscopies and all-atom molecular dynamics (MD) simulations reveal that the stabilities of the dimers are within the thermal energy and enforce free conversion between the distinct structures. The monomer concentration in the solution in below 0.1 %. Despite the structural complexity of the solute, the velocity of the steps on the (010) faces of etioporphyrin crystals (measured using time resolved in situ AFM) increases linearly with the total solute concentration. The observed linearity indicates that the molecular reaction at the growth sites in monomolecular and the majority of solute dimers seamlessly adopt the crystallographic conformation. By contrast, step growth on the other major face, (001), scales with the square of the solute concentration. We eliminate three potential mechanisms that may manifest as super-linear growth: inaccurate solubility determination, escalating kink density at elevated supersaturation, and adsorption of step pinners on the crystal surface. The disqualification of the three scenarios enforces the conclusion that the quadratic kinetic law indicates a bimolecular process of incorporation. We demonstrate that in contrast to numerous other solution grown crystals, solute molecules directly incorporate in the kinks and do not reach the growth sites after first adsorbing on the terraces between steps. The paucity of solute monomers precludes a collision between two monomers to form a crystallographic dimer as the mechanism of the crystallization reaction. MD evaluation of solute interactions with kinks on the (001) face reveal a molecular trap selective for solute dimers located on the approach to all kinks, which necessitates a collision with a second dimer to ease the captured solute towards the kink. The results with etioporphyrin demonstrate the lack of correlation between the solute state and the incorporation kinetics driven by the solute. In a broader context, these observations illuminate the immense diversity of unexplored crystallization scenarios that pave the long road to crystallization control.",18,2.0
"Reinforcement Learning (RL) is one of the three basic machine learning paradigms, alongside supervised and unsupervised learning. RL focuses on how a smart agent can learn the optimal policy from the maximization of cumulative rewards from its environment. The recent development of model-free RL has achieved remarkable success in various control tasks, where multiple applications have been reported in literature including parameter tuning for existent single PID control loops [1], supply chain management [2] and robotic control [3].In the process control domain, there is an extensive state of art that applies RL algorithms to different types of chemical processes [4,5]. Nevertheless, most of these studies focus on the implementation and application of a specific RL method. These studies usually do not consider comparison with other RL algorithms or with traditional methods in the process control area. Additionally, there is no guideline regarding the RL algorithm selection.In this work, we will provide a comprehensive investigation of RL applications in the process control area. The above practical issues would be addressed through the case study of a batch bio-reactor. Different types of RL methods including value-based, policy-based, and actor-critic algorithms will be compared in this case study. Specifically, their training stability, convergence, reproducibility and sample efficiency would be evaluated. Beyond the algorithm level evaluation, RL performance on common control scenarios, including control for optimization, control to set point, and control with state constraints would also be performed. Furthermore, the performance of the RL algorithms would be compared with a model predictive controller. Based on these comparisons, we will summarize some guidelines and suggestions on current model-free RL approaches in the control area, as well as some future perspectives regarding RL development and its application in the chemical industry. Reference[1] Badgwell, Thomas A., et al. ""Adaptive PID controller tuning via deep reinforcement learning."" U.S. Patent Application No. 16/218,650.[2] Oroojlooyjadid, Afshin, et al. ""A Deep Q-Network for the Beer Game: A Deep Reinforcement Learning algorithm to Solve Inventory Optimization Problems."" arXiv preprint arXiv:1708.05924 (2017).[3] Peng, Xue Bin, et al. ""Sim-to-real transfer of robotic control with dynamics randomization."" 2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018.[4] Spielberg, Steven, et al. ""Toward self‐driving processes: A deep reinforcement learning approach to control."" AIChE Journal 65.10 (2019): e16689.[5] Petsagkourakis, Panagiotis, et al. ""Reinforcement learning for batch bioprocess optimization."" Computers & Chemical Engineering 133 (2020): 106649.",20,0.0
"OR-Gym: A Reinforcement Learning Library for OperationsResearch ProblemsChristian Hubbs1,2, Owais Sarwar1, Hector Perez1, Nikolaos Sahinidis1, and Ignacio Grossmann11Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA 152132Digital Fulfillment Center, Dow Chemical, Midland, MI 48642Keywords: Machine Learning, Reinforcement Learning, Optimization, Scheduling, Stochastic Program- ming1 AbstractWe introduce OR-Gym, an open-source benchmark in the form of OpenAI Gym for developing reinforcement learning algorithms to address operations research problems [Brockman et al., 2016]. Reinforcement learning has been widely applied to game-playing and surpassed the best human-level performance in many domains, yet there are few use-cases in industrial or commercial settings.We seek to provide a standardized library for the research community who wishes to extend RL into commercial applications by building on top of the preceding work and releasing OR-Gym, a single library that relies on the familiar OpenAI interface for RL, but contains problems relevant for industrial use. To this end, we have incorporated the benchmarks in Balaji et al. [2019], while extending the library to address the traveling salesman problem, knapsack problem [Kellerer et al., 2004], multi-dimensional bin packing [Coffman et al., 2013], multi-echelon news vendor problem, portfolio optimization [Dantzig and Infanger,1993], and vehicle routing problem [Pillac et al., 2013]. These problems cover logistics, finance, engineering, and are common in many business operation settings. In each case, we select a prototypical version from the literature to benchmark reinforcement learning and other optimal approaches against.RL problems are formulated as Markov Decision Processes (MDP), meaning they are probabilistic, se- quential decision making problems. MDP’s are defined by the state, which informs an agent to select an action, which then receives a reward during the transition to the subsequent state. This framework is not widely used in the optimization community, so we make explicit our thought process as we reformulate many optimization problems to fit into the MDP mold without loss of generality.Many current RL libraries such as the canonical OpanAI Gym, have many interesting problems, but problems that are not directly relevant to industrial use. Moreover, many of these problems (e.g. the Atari suite) lack the same type of structure as classic optimization problems, and thus are primarily amenable to model-free RL techniques, that is RL algorithms that learn with little to no prior knowledge of the dynamics of the environment they are operating in. Bringing well-studied optimization problems to the RL community may encourage more integration of model-based and model-free methods to reduce sample complexity and provide better overall performance.It is our goal that this work encourages further development and integration of RL into optimization and the OR community while also opening the RL community to many of the problems and challenges that the OR community has been wrestling with for decades.We explicitly present the design decisions we made to translate these problems to MDP’s, as well as theresults of the benchmark algorithms. All code is open-source and available at www.github.com/hubbs5/or- gym.ReferencesB. Balaji, J. Bell-Masterson, E. Bilgin, A. Damianou, P. M. Garcia, A. Jain, R. Luo, A. Maggiar, B. Narayanaswamy, and C. Ye. ORL: Reinforcement Learning Benchmarks for Online Stochastic Op- timization Problems. 2019. URL http://arxiv.org/abs/1911.10641.G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. pages 1–4, 2016. ISSN 00217298. doi: 10.1241/johokanri.44.113. URL http://arxiv.org/abs/1606.01540.E. G. Coffman, J. Csirik, G. Galambos, S. Martello, and D. Vigo. Bin Packing Approximation Algorithms: Survey and Classification. In Handbook of Combinatorial Optimization, pages 455–531. Springer, Heidel- berg, 2013.G. B. Dantzig and G. Infanger. Multi-stage stochastic linear programs for portfolio optimization. Annals ofOperations Research, 45:59–76, 1993.H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems, volume 53. Springer Verlag, Heidelberg, 2004. ISBN 9788578110796. doi: 10.1017/CBO9781107415324.004.V. Pillac, M. Gendreau, C. Gueret, and A. L. Medaglia. A review of dynamic vehicle routing problems.European Journal of Operational Research, (225):1–11, 2013.",20,1.0
"The growing need for accurately modeling complex physical systems is increasing the complexity of high-fidelity models. Simpler, often analytical, computationally inexpensive surrogates or meta-models offer an attractive alternative. Surrogates are data-driven models that mimic input and output patterns in the data by means of response surfaces. Selecting a surrogate model that approximates a complex system most accurately is critical. A straightforward approach for this selection problem is to “try” several surrogates and select the best. Several other approaches exist in the literature. Genetic Programming (GP) has been used (Koza 1994; Streeter and Becker 2003; Lessmann, Stahlbock, and Crone 2006) to derive an optimum combination of operators and simple basis functions defining a surrogate model. Cozad, Sahinidis, and Miller (2014) developed a low-complexity, accurate model (ALAMO) that uses MILP optimization to identify the best mix of basis functions. Apart from these, MINLP formulations (Cozad and Sahinidis 2018) and extended GP (Kaizen Programming) (Rad, Feng, and Iba 2018) have also been used to good effect. All these works must be exhaustively applied to every new data set to determine the best surrogate. However, a smarter, faster, and learning-based approach is to first unearth patterns that match the meta-features or attributes of a data set with surrogate model performance. Such meta-learning can help select the surrogate for a future data set. Cui et al. 2016 and Garud, Karimi, and Kraft (2018) developed this basic idea into CRS and LEAPS2 frameworks respectively. Later, Davis, Cremaschi, and Eden 2018 studied the performance of surrogates with respect to sample sizes, input dimensions, and shapes of input functions. Although LEAPS2 addressed the limitations of CRS with respect to sample sizes, dimensionality, and surrogates, LEAPS2 has its shortcomings. It was trained only on noise-free synthetic data, so it may recommend an over-fitting model for real-world, noisy data. Moreover, LEAPS2 used an error-based metric that requires splitting the data into train/test sets. Furthermore, LEAPS2 includes both data-distribution based attributes such as local and global fluctuations, dimensionality, etc. along with statistical data-based attributes like mean, standard deviation, gradient, etc. Intuitively, it appears that only distribution-based attributes should determine surrogate performance, as it is the underlying trends or features in data that determine surrogate performance.In this work, we modified and broadened the scope of LEAPS2 in several significant ways. First, we incorporated noisy and real-world data sets to address a key challenge in surrogate modeling. Second, we added one more metric for surrogate selection, namely a complexity-based metric called AIC weight. This metric provides an alternative for surrogate selection when splitting the dataset into train/test sets is not feasible. Third, we essentially revamped the attribute set of LEAPS2 to use only those attributes that quantify the underlying features of “data-distribution”, rather than the data itself. Some of these new attributes quantify the degree and variations of non-linearity in the data, asymmetry, and flatness of response with respect to standard distribution. Thus, we now have fewer (11 vs 14) but intuitively more appealing attributes in LEAPS2. Fourth, we have improved the surrogate recommendation strategy by developing simple heuristics. Finally, we have updated our surrogate pool by adding 10 new surrogates in LEAPS2. Our improved LEAPS2 framework was evaluated with respect to the two metrics (Garud et al. 2018), namely “Total Degree of Success” (TDoS) that quantifies the success in recommending the best surrogates, and “Total Coefficient of Reward” (TCoR) that combines the success and computational savings in a single score. The new framework gives a TDoS = 91% and a TCoR = 42% for the error-based metric, and TDoS = 83% but a much higher TCoR = 63% for AIC weight on test data. However, they improved during the learning process. We tested the new framework on two case studies with real data, one on a compressor, and the other on COVID-19 data. In both cases, our improved LEAPS2 achieved a TDoS of 100%. This framework acts as a smart tool for surrogate selection to model complex physical systems.References:Cozad, Alison, and Nikolaos V. Sahinidis. 2018. “A Global MINLP Approach to Symbolic Regression.” Mathematical Programming 170 (1): 97–119. https://doi.org/10.1007/s10107-018-1289-x.Cozad, Alison, Nikolaos V. Sahinidis, and David C. Miller. 2014. “Learning Surrogate Models for Simulation-Based Optimization.” AIChE Journal 60 (6): 2211–27. https://doi.org/10.1002/aic.14418.Cui, Can, Mengqi Hu, Jeffery D. Weir, and Teresa Wu. 2016. “A Recommendation System for Meta-Modeling: A Meta-Learning Based Approach.” Expert Systems with Applications 46 (March): 33–44. https://doi.org/10.1016/j.eswa.2015.10.021.Davis, Sarah E., Selen Cremaschi, and Mario R. Eden. 2018. “Efficient Surrogate Model Development: Impact of Sample Size and Underlying Model Dimensions.” In Computer Aided Chemical Engineering, 44:979–84. Elsevier. https://doi.org/10.1016/B978-0-444-64241-7.50158-0.Garud, Sushant S., Iftekhar A. Karimi, and Markus Kraft. 2018. “LEAPS2: Learning Based Evolutionary Assistive Paradigm for Surrogate Selection.” Computers & Chemical Engineering 119 (November): 352–70. https://doi.org/10.1016/j.compchemeng.2018.09.008.Koza, JohnR. 1994. “Genetic Programming as a Means for Programming Computers by Natural Selection.” Statistics and Computing 4 (2). https://doi.org/10.1007/BF00175355.Lessmann, Stefan, Robert Stahlbock, and Sven F Crone. 2006. “Genetic Algorithms for Support Vector Machine Model Selection,” 7.Rad, Hossein Izadi, Ji Feng, and Hitoshi Iba. 2018. “GP-RVM: Genetic Programing-Based Symbolic Regression Using Relevance Vector Machine.” ArXiv:1806.02502 [Cs], August. http://arxiv.org/abs/1806.02502.Streeter, Matthew, and Lee A Becker. 2003. “Automated Discovery of Numerical Approximation Formulae via Genetic Programming,” 32.",20,2.0
"Over the past few decades, nonlinear model-based control is receiving significant attention as linear models are inadequate in describing inherently nonlinear and complex industrial processes. This requires developing methods for nonlinear model identification. Modeling using first-principles is desirable only when there is sufficient knowledge of the process available. For the processes that are complex and poorly understood, it is difficult to derive such models. In view of this, there has been an increasing interest in data-driven system identification for prediction and control purposes. Specifically, subspace identification methods such as multi-variable output-error-state-space (MOESP) [1], numerical algorithms for subspace state-space identification (N4SID) [2], and canonical variate analysis [3] are popular in the process control domain. Although these methods are successful in identifying state-space models for various industrial applications using input-output data, they do not provide a physical understanding of the process. More importantly, for adaptive modeling applications, it is advantageous to have a model that can provide interpretability of the changing dynamics, which will help guide in evaluating the process operating conditions to take appropriate actions. Lately, sparse identification of nonlinear dynamics (SINDy) has delivered promising results for various nonlinear processes [4]. The SINDy algorithm is based on the techniques of sparse regression and compressive sensing. It fits the input-output data to a library of candidate functions such that only the functions describing the original dynamics are identified. Thereby, using this approach, a parsimonious and interpretable model is obtained. Additionally, any prior knowledge about the process from, for example, physics and thermodynamic laws can be included in the candidate library to quicken the model identification process. Due to these reasons, there has been a growing interest in applying SINDy to a variety of applications, which includes identifying rational nonlinearities [5], sparse learning of reaction kinetics [6], reduced-order modeling of a complex process [7], model recovery from abrupt system changes [8], and for control [9].Despite the simplicity of SINDy algorithm, it is challenging to use SINDy for model-based control, especially at any instance of plant-model mismatch or process upset. This is because re-training the model using SINDy is computationally expensive and cannot guarantee to catch up with rapidly changing dynamics. Hence, we propose online adaptive sparse identification of systems (OASIS) framework that extends the capabilities of SINDy for accurate, automatic, and adaptive approximation of process models. The key novelty is to combine the usefulness of SINDy in discovering an interpretable model with a deep neural network (DNN) to adaptively model and control the process dynamics in real-time. The proposed method is implemented in two steps: system identification and controller design. For the system identification step, we utilize several sets of process historical data that are available for various input settings and identify their corresponding models using SINDy. Next, we train a DNN using the previously collected historical data sets and their respective SINDy models such that the DNN approximates the relationship between process data and SINDy models. We use this trained DNN to design a controller wherein the DNN updates the SINDy model by utilizing a new set of measurements at every sampling time to accurately predict the future process behavior. In this way, the OASIS method supports the application of SINDy for real-time model identification and control. We demonstrate the OASIS methodology on the model identification and control of a continuous stirred tank reactor. The closed-loop results showed that the proposed OASIS framework can be effectively used for adaptive modeling and control of nonlinear processes.Literature cited:[1] Verhaegen M, Dewilde P. Subspace model identification part 2. Analysis of the elementary output-error state-space model identification algorithm. International Journal of Control. 1992; 56(5):1211-1241.[2] Van Overschee P, De Moor B. N4SID: Subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica. 1994; 30(1):75-93.[3] Larimore WE. Canonical variate analysis in identification, filtering, and adaptive control. In: 29th IEEE Conference on Decision and Control, vol. 2. Piscataway, NJ: IEEE. 1990; pp. 596-604.[4] Brunton SL, Proctor JL, Kutz JN. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences. 2016; 113(15):3932-3937.[5] Mangan NM, Brunton SL, Proctor JL, Kutz JN. Inferring biological networks by sparse identification of nonlinear dynamics. IEEE Transactions on Molecular, Biological and Multi-Scale Communications. 2016; 2(1):52-63.[6] Hoffmann M, Frhner C, No F. Reactive SINDy: Discovering governing reactions from concentration data. The Journal of Chemical Physics. 2019; 150(2):025101.[7] Narasingam A, Kwon JSI. Data-driven identification of interpretable reduced-order models using sparse regression. Computers & Chemical Engineering. 2018; 119:101-111.[8] Quade M, Abel M, Nathan Kutz J, Brunton SL. Sparse identification of nonlinear dynamics for rapid model recovery. Chaos: An Interdisciplinary Journal of Nonlinear Science. 2018; 28(6):063116.[9] Brunton SL, Brunton BW, Proctor JL, Kutz JN. Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control. PloS one. 2016; 11(2):e0150171.",20,3.0
"Real-world decision making generally involves selecting the best among multiple possible options. It can be conjectured that these decisions are solutions to an underlying optimization problem which, in many cases, is unknown even to the decision maker. Uncovering these hidden optimization problems can act as a medium to generate insights into the decision-making process and predict future decisions. Examples where this can be useful are as diverse as learning the strategy of an expert decision maker operating a complex process, understanding the preferences of customers, or analyzing decision-making strategies in biological systems known to have evolved to behave optimally for their survival. Several of such decision-making problems can be formulated or approximated as linear programs (Burgard & Maranas, 2003; Saez-Gallego et al., 2016).In this work, we develop a data-driven framework for uncovering the unknown cost vector when the decision-making process can be represented as a linear optimization problem. This is referred to as inverse optimization (Ahuja & Orlin, 2001). Research on inverse optimization started with a single, deterministic observation and has more recently progressed to the more realistic setting of multiple noisy observations coming from different sets of input parameters (Aswani et al., 2018). The latter is often referred to as data-driven inverse optimization and is the focus of this work.The two main theoretical contributions of this work are as follows: (1) We propose a novel two-phase algorithm that performs denoising of observations and cost vector estimation separately. Our approach derives from a polyhedral understanding of the problem and is shown to be effective in alleviating the limitations of existing methods found to lead to severely restricted feasible sets of cost vector estimates. (2) We develop a rigorous adaptive sampling strategy that systematically reduces the set of feasible estimates and therefore increases the confidence in the resulting point estimate. Our extensive computational experiments show that the proposed approach can find the true cost vector (or the best possible estimate) with significantly less samples compared to naïve random sampling.ReferencesAhuja, R. K., & Orlin, J. B. (2001). Inverse optimization. Operations Research, 49(5), 771–783.Aswani, A., Shen, Z. J. M., & Siddiq, A. (2018). Inverse optimization with noisy data. Operations Research, 66(3), 870–892.Burgard, A. P., & Maranas, C. D. (2003). Optimization-based framework for inferring and testing hypothesized metabolic objective functions. Biotechnology and Bioengineering, 82(6), 670–677.Saez-Gallego, J., Morales, J. M., Zugno, M., & Madsen, H. (2016). A Data-Driven Bidding Model for a Cluster of Price-Responsive Consumers of Electricity. IEEE Transactions on Power Systems, 31(6), 5001–5011.",20,4.0
"Noninvertibility, an intrinsic property of certain discrete-time dynamical systems, is defined as non-uniqueness of the backward-time dynamics. Moreover, this noninvertibility concept could be also be adapted to neural network predictions that approximate the ground-truth fpr some unknown system [1]. In many cases, the governing equations for discrete-time systems could be treated as a discretized form of a continuous time, ordinary differential equation, which could be solved numerically by, say, explicit or implicit Euler integration methods. These methods, to some extent, are similar in architecture to residual neural networks (ResNets), which have recently seen wide use in machine learning [2].We begin our work with numerical simulations of chemical oscillatory dynamics. Extending previous work [1], we implemented an algorithm to determine the range of noninvertible regions for the Euler discretization map. Then, training a neural network to discrete time data, we show how noninvertibility arises naturally in neural networks, and how to quantify its occurrence. In addition to this study of the noninvertibility itself, we investigate its dynamical consequences: dynamic instabilities and pathological phenomena that occur when this feature of the discrete map interacts with the dynamics and bifurcations of the underlying continuous-time system. This study yields useful insights into the long-term prediction power of network architectures. Additionally, our work establishes connections between a network’s invertibility and the estimation of its Lipschitz constants, a connection which has value beyond dynamical systems applications [3].[1] R. Rico-Martinez, I. G. Kevrekidis and R. A. Adomaitis, ""Noninvertibility in neural networks,"" IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 382-386 vol.1.[2] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.[3] J. Behrmann, W. Grathwohl, R.T.Q. Chen, D. Duvenaud, and J. Jacobsen. Invertible Residual Networks. arXiv preprint arXiv: 1811.00995, 2019.",20,5.0
"Recent developments in the broad field of data-science have led to a series of breakthroughs in Machine Learning (ML) [1, 2] techniques. The Process Systems Engineering (PSE) community is having an important debate on the dangers of data-science taking over first-principles (i.e., thermodynamics, transport phenomena, kinetics and mass balances) [3]. The key reasons for doubting the value of ML in chemical engineering are: their black-box nature; poor extrapolating capabilities; lack of interpretability and unbounded uncertainty in predictions that may not satisfy physical constraints. “Hybridization” ensures that physics-based knowledge is not ignored, and the data-dependent models are more reliable because they learn from both data and physics [4]. The concept of Hybrid-Modeling is not new to PSE. Indeed, hybrid modeling techniques and applications have been growing in number since the early 90’s [5-7]. Following an explosion of advances in purely black-box ML techniques, merging fist-principle knowledge and ML is becoming the next big trend, since it can increase accuracy and interpretability with less data.When reviewing all potential ways to merge data-driven with physics-based models, a plethora of approaches have been proposed in different literatures, ranging from feature engineering, to simple additive models comprised of separate physical and data-driven equations, to advanced methods that embed physics within data-driven models using customized training and numerical techniques. In fact, one can quickly realize that similar concepts targeting the same fundamental idea are disguised under different terminology. The first aim of this presentation is to perform a thorough review of state-of-the-art advances in four different research areas: “Hybrid-Modeling” (HM), “Physics-Constrained ML” (PC-ML), “Model calibration” (MC) and “Multi-fidelity modeling” (MF).Specifically, HM contributions focus on identifying unknown or partially-known process mechanisms constrained by known first-principle equations (typically, a system of differential equations representing conservation balances)[4]. PC-ML is in many ways a synonym to HM, but typically refers to techniques for constrained training of deep ML models (i.e., adding physics-based loss terms, or changing weight parameters or the ML model structure) based on prior knowledge [8, 9]. PC-ML and HM techniques aim to maintain the advantages inherent to data driven models (low computational cost), while making models more generalizable and physically consistent. MC is the process of updating the parameters of physics-based models with statistical techniques (typically Bayesian methods) to compensate for the model-data discrepancy [10]. Finally, MF modeling is used when multiple types of data and/or models are available for the same system (ranging from highly accurate to low-fidelity) and these and the correlation between them are jointly used to generate overall more accurate models [11]. These terms are widely used in the PSE, ML, Statistics and Operations Research literatures, respectively. We have compiled the similarities and differences in methodologies, applications, types of data, types of physical knowledge and software implementations for all four areas, and will provide a comprehensive discussion on how the advances in each area can inform the rest, and what their collective relevance is for the PSE community.The presentation will conclude with a demonstration of customized tools for HM, PCML, MC and MF and combinations of these for a series of problems, such as modeling, design and parameter estimation of (bio)chemical reactors and power-grid modeling. Through these case studies, we will showcase the current capabilities, limitations and future challenges on the topic of merging data-driven techniques with first-principle knowledge, for the generation of tractable but accurate hybrid models that can be used for optimization, design and control applications.ReferencesQin, S.J. and L.H. Chiang, Advances and opportunities in machine learning for process data analytics. Computers & Chemical Engineering, 2019. 126: p. 465-473.Lee, J.H., J. Shin, and M.J. Realff, Machine learning: Overview of the recent progresses and implications for the process systems engineering field. Computers and Chemical Engineering, 2018. 114: p. 111-121.Venkatasubramanian, V., The promise of artificial intelligence in chemical engineering: Is it here, finally? AIChE Journal, 2019. 65(2): p. 466-478.Von Stosch, M., et al., Hybrid semi-parametric modeling in process systems engineering: Past, present and future Hybrid modeling Hybrid neural modeling Semi-mechanistic modeling Hybrid grey-box modeling Hybrid semi-parametric modeling Process operation/design. Computers and Chemical Engineering, 2014. 60: p. 86-101.Psichogios, D.C. and L.H. Ungar, A hybrid neural network-first principles approach to process modeling. AIChE Journal, 1992. 38(10): p. 1499-1511.Rico-Martinez, R., J.S. Anderson, and I.G. Kevrekidis. Continuous-time nonlinear signal processing: a neural network based approach for gray box identification. in Proceedings of IEEE Workshop on Neural Networks for Signal Processing. 1994.Thompson, M.L. and M.A. Kramer, Modeling chemical processes using prior knowledge and neural networks. AIChE Journal, 1994. 40(8): p. 1328-1340.Chen, T.Q., et al. Neural ordinary differential equations. in Advances in neural information processing systems. 2018.Raissi, M., P. Perdikaris, and G.E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 2019. 378: p. 686-707.Kennedy, M.C. and A. O'Hagan, Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2001. 63(3): p. 425-464.Peherstorfer, B., K. Willcox, and M. Gunzburger, Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization. SIAM Review, 2018. 60(3): p. 550-591.",20,6.0
"IntroductionThe attainable regions for critical quality attributes, such as Particle Size Distribution (PSD) and impurity levels, in the manufacture of Solid Oral Dosage Forms (SODF) can be highly dependent upon the type and operation of the crystallization process. In this work, we outline a step-wise workflow consisting of model validation, model-based technology transfer and process optimisation employed for the digital design of a continuous cascade cooling crystallization and wet milling process for manufacturing an Active Pharmaceutical Ingredient (API). Following this workflow, the mechanistic model was first validated using batch crystallization data and subsequently applied to describe the continuous crystallization process and with continuous and periodic wet milling, and explore the impact of varying process parameters, including duration of milling, rotor speed and frequency of milling operation during the crystallization process. The step-wise approach taken for model validation and its subsequent application was as follows:Model validation of crystallization kinetics for the pure API solid phase from batch de-supersaturation experimental runs.Application of the validated crystallization kinetics to describe the continuous crystallization of pure API in a three-stage cascade process, as shown in Figure 1 below (wet milling was not considered at this stage).Model validation of breakage kinetic parameters using batch data from a wet milling unit operated in a recycle with a batch crystallizer. A rotor-stator wet mill was utilised for this process, with a range of rotor frequencies and milling head generator configurations probed to assess the impact on the PSD over time and to fit the breakage kinetic parameters. Subsequent refinement of the breakage kinetic parameters utilizing continuous crystallization experiments, where intermittent/periodic wet milling was employed using an immersion wet mill in the first stage crystallizer.Scenario analysis of the continuous crystallization and periodic wet milling process for this API, looking at the impact of process parameters on the Critical Quality Attributes (CQAs) of the product, including product PSD and dimer phase formation (impurity).ConclusionsThe main conclusions of the work include the following:The continuous crystallization process was unable to achieve the desired product PSD without wet milling, due to very low levels of secondary nucleation in the system. Optimal PSD quantiles (d10, d50 and d90) predicted for the product were significantly higher than the target PSD quantiles required to achieve the desired product performance for this compound.Addition of a wet milling step in a recycle with the continuous crystallization significantly increased the lower end of the attainable region in terms of PSD quantiles and impurity level, allowing the process to comfortably achieve the desired range for product PSD quantiles.Mechanistic modelling approaches can be utilised to significantly reduce the development timelines, material consumption and efficiency of continuous API crystallization production processes, in particular cases where periodic operation of some units, in this case the wet mill, are employed.",22,0.0
"In pharmaceutical industries, continuous manufacturing of medicine has the potential to reduce capital, time and increase efficiency1. Continuous crystallization as the bridge between synthesis and formulation2, the process and quality control is vital such as aggregation of crystals can affect the following blending and coating. Segmented/slug flow crystallization is one of the effective continuous process demonstrated to allow enhanced control of organic crystal properties such as size and shape3–6. This presentation will elaborate in the following topics7: (1) generate pure-form uniform size crystals in slug flow rapidly; (2) explore the operation region (meaning no secondary nucleation nor aggregation) in slug flow crystallizer. The crystal forms at different conditions generated by both slug flow crystallizer and batch crystallizer are discussed and compared. Experimental results validated the proposed crystallizer reduced production time and equipment cost while increased the operational region significantly compared to batch crystallization.Reference(1) Schaber, S. D.; Gerogiorgis, D. I.; Ramachandran, R.; Evans, J. M. B.; Barton, P. I.; Trout, B. L. Economic Analysis of Integrated Continuous and Batch Pharmaceutical Manufacturing: A Case Study. Ind. Eng. Chem. Res. 2011, 50 (17), 10083–10092.(2) Lee, S. L.; O’Connor, T. F.; Yang, X.; Cruz, C. N.; Chatterjee, S.; Madurawe, R. D.; Moore, C. M. V.; Yu, L. X.; Woodcock, J. Modernizing Pharmaceutical Manufacturing: From Batch to Continuous Production. J. Pharm. Innov. 2015, 10 (3), 191–199.(3) Jiang, M.; Zhu, Z.; Jimenez, E.; Papageorgiou, C. D.; Waetzig, J.; Hardy, A.; Langston, M.; Braatz, R. D. Continuous-Flow Tubular Crystallization in Slugs Spontaneously Induced by Hydrodynamics. Cryst. Growth Des. 2014, 14 (2), 851–860.(4) Eder, R. J. P.; Schrank, S.; Besenhard, M. O.; Roblegg, E.; Gruber-Woelfler, H.; Khinast, J. G. Continuous Sonocrystallization of Acetylsalicylic Acid (ASA): Control of Crystal Size. Cryst. Growth Des. 2012, 12 (10), 4733–4738.(5) Jiang, M.; Papageorgiou, C. D.; Waetzig, J.; Hardy, A.; Langston, M.; Braatz, R. D. Indirect Ultrasonication in Continuous Slug-Flow Crystallization. Cryst. Growth Des. 2015, 15 (5), 2486–2492.(6) Lu, J.; Litster, J. D.; Nagy, Z. K. Nucleation Studies of Active Pharmaceutical Ingredients in an Air-Segmented Microfluidic Drop-Based Crystallizer. Cryst. Growth Des. 2015, 15 (8), 3645–3651.(7) Mou, M.; Jiang, M. Fast Continuous Non-Seeded Cooling Crystallization of Glycine in Slug Flow: Pure α-Form Crystals with Narrow Size Distribution. J. Pharm. Innov. 2020, 22–24.",22,1.0
"Crystallization of proteins could be a potentially cost effective and robust method to purify protein molecules in the pursuit of the next generation of protein based biopharmaceutic therapies. However, the step from traditional small-scale high throughput screening methods, to crystallizers which can produce high yields of pure crystalline protein material, is still in its infancy.Here, we present a platform for continuous oscillatory flow crystallization (COFC) (Figure 1) and its application for the crystallization of lysozyme[1,2]. A workflow is developed from µl screening experiments, to scaled up batch oscillatory flow crystallization (BOFC) and COFC experiments.The induction time for lysozyme is found to be inversely proportional to the initial protein concentration, with the same relationship found between the induction time and the frequency and amplitude of the oscillation. However, it is also observed that the decrease in induction time comes at a cost to the crystal size and quality. In turn, the high shear environment from increased amplitude and frequency of the COFC can also damage the fragile protein crystals, resulting in the formation of more fine particles and an undesirable size distribution. This led to further investigation of slug flow mixing in the continuous crystalliser, where the crystal size and shapes are compared to the particles produced from the osciallatory flow reactor. Further work around optimization of the crystallization parameters and crystallizer geometry using the gCrystal platform to find the optimum scale up conditions are also discussed. The use of seeding and potential templating materials is also covered with a view to expanding the use of this crystallizer to less well characterized protein materials.It is demonstrated that purification of proteins can be achieved by crystallization in a continuous mode. This route offers the possibility to revolutionise downstream separations of biopharmaceuticalsReferences[1] H.Y. Yang, W.Q. Chen, P. Peczulis, J.Y.Y. Heng (2019): Development and Workflow of a Continuous Protein Crystallization Process: A Case of Lysozyme, Cryst. Grow. Des; 19, 983-991.[2] H.Y. Yang, P. Peczulis, P. Inguva, X.Y. Li, J.Y.Y. Heng (2018): Continous Protein Crystallisation Platform and Process: Case of Lysozyme, Chem. Eng. Res. Des. 136, 529-53",22,2.0
"Cooling crystallization is a popular crystallization technique in pharmaceutical and fine chemical industries. The driving force for this process is the temperature gradient that consequently results in increasing the supersaturation ratio. However, it is very challenging to minimize the gradient range and maintain a constant supersaturation without sacrificing the mixing performance. In this study we have we have developed two techniques to enable temperature gradient within the crystallization zone using the previously developed continuous microfluidic mixer. For the first method, we have designed a cooling bath around the circular zone which has separated inlet and outlet. The cold stream is circulated around the zone using a peristaltic pump while the hot streams enter from the four main inlets of the device. Due to the temperature difference between the hot and cold streams, the solution within the crystallization zone will attain an equilibrium temperature lower than the hot streams. In the second method, saturated hot and cold streams will enter the circular zone and mixed to impose supersaturation inside the mixing zone. Both systems are modeled using COMSOL Multi physics and equilibrium temperature and imposed supersaturation are calculated respectively. These microfluidic devices are then 3D-printed using the Formlabs 3D-printer for experimental verification. Using these two systems we have performed cooling crystallization of L-Glutamic acid and L-Histidine using water as solvent. Here we have compared eight different supersaturation for each compound and measured the face specific growth rate followed by morphology and size distribution of the crystals. Comparative analysis has also been performed using a batch microtiter plate.",22,3.0
"In recent years, the world’s top pharmaceutical companies have started major research programs to switch from classical batch to continuous manufacturing of their active drug substances. There is a significant gap in technology for the continuous isolation. Alconbury Weston Ltd (AWL), a leading supplier of continuous crystallization and filtration equipment, has designed an innovative continuous carousel filter dryer (CCFD) which filters, washes and dries solids in small aliquots simultaneously in a continuous manner.The core technology is based on well-established Nutsche filtration, processing thin cakes in a fully automated system. A state-of-the-art vision system can be used to track the cake and liquid heights during filtration and washing, improving product purity, yields and quality attributes. The CCFD is a complete plant which replaces a traditional batch production facility and fits into a walk-in fume cupboard.The presentation will describe the challenges and issues observed in batch filtration systems and demonstrate how these challenges can be addressed using continuous filtration. The talk will demonstrate the collaborative efforts required for the development of a novel continuous processing technology through to a commercial product. AWL has worked with Strathclyde University and CMAC (Glasgow, UK) and Purdue University (Indiana, US) to investigate the advantages of continuous filtration. The presentation will include case studies carried out at both academic centres of excellence.",22,4.0
"Insulin injection is commonly used for the treatment of diabetes. Pulmonary delivery through inhalation of dry insulin powders is commercially also available and offers a more convenient route compared to injection.1 The manufacture of such powders is challenged by the need to produce crystalline particles with an aerodynamic diameter in the narrow range of 1 – 5 μm. Particles outside the optimal size range will not deposit well into the lungs. Amorphous particles produced from spray drying an insulin solution are limited by their fast dissolution,2 which leads to a therapeutic effect of only several hours. Furthermore, in general, crystalline particles have a better stability and aerosolization performance and different drug release characteristics compared to amorphous particles.3,4 Therefore, the manufacture of a dry powder with insulin crystals is of interest to potentially improve pulmonary delivery of insulin. The crystallization of insulin has been studied in various crystallizers.5–7 However, those processes generally do not deliver crystals in the optimal size range for pulmonary delivery. Therefore, there is a need for novel crystallization processes that can produce insulin crystals with tailored size for the application of dry powder inhalation more efficiently.The objective of this work is to develop and characterize a novel continuous crystallization process for insulin that is tailored for dry powder inhalation by integrating a segmented-flow crystallizer with spray drying for rapid solvent removal. Continuous crystallization has received growing interest in the fine-chemical and pharmaceutical industries due to typical advantages such as easier process control and a more consistent product quality compared to batch crystallization.8 The benefits of integrating continuous crystallization with spray drying for the manufacture of dry powders for inhalation has been demonstrated by earlier work from our group for different small-molecule active pharmaceutical ingredients.9 The present work is, to the best of our knowledge, the first demonstration of such process for a therapeutic protein.The process consisted of a tubular crystallizer and a spray dryer. A calibrated two-channel peristaltic pump was used to introduce the feed and the precipitant solution into the crystallizer at the same flow rate through a T-mixer. A gas flow controller was used to introduce nitrogen with controlled flow rate via another T-mixer to create a segmented flow, which offers the benefit of a narrow residence time distribution.10,11 The supersaturation was generated by both cooling and a change in pH. The tubular crystallizer, which consisted of two connected pieces of tubing of 14m long each, was immersed in a thermostatic bath to allow temperature control. At the outlet of the crystallizer, a buffer vessel was used for separation of nitrogen and to provide a steady flow to the spray dryer using another peristaltic pump. Finally, the dry powder was collected in a powder collector located below a cyclone. Initially, the crystallizer was operated stand-alone to identify optimal crystallization conditions. Subsequently, optimal crystallization conditions were applied for the integrated process. The morphology of the particles and their approximate size were characterized with optical microscopy and scanning electron microscopy (SEM). The mass median aerodynamic diameter (MMAD) and fine particle fraction (FPF) fraction was measured with a Next Generation Impactor (NGI).The recovery of insulin from stand-alone crystallization experiments was in excess of 90% when using a combined flow rate of 1.2 ml/min and an insulin concentration of 2.4 mg/ml. A lower recovery was obtained at higher combined flow rates, and thus shorter residence times, in the range of 1.8 ml/min to 2.4 ml/min. A constant recovery could be approached in a short period of about 10 to 15 minutes when running at a high initial concentration in the range of 2.0 mg/ml to 2.4 mg/ml. Such fast start-up time is consistent with plug-flow conditions, which is an inherent benefit of a segmented-flow crystallizer compared to a mixed-suspension mixed-product-removal crystallizer. The produced particles appeared crystalline from SEM and were close to the desired size range for pulmonary drug delivery. The particles produced with the integrated process under optimal conditions were characterized by a MMAD in the range of 2 μm to 6 μm and an FPF of 20 - 40%, which indicated a good potential for pulmonary drug delivery. In conclusion, the demonstrated benefits of the novel process include a short total residence time, high recovery from crystallization, and suitable product quality attributes for pulmonary drug delivery of insulin without any added excipients. 1. References1. Easa, N., Alany, R. G., Carew, M. & Vangala, A. A review of non-invasive insulin delivery systems for diabetes therapy in clinical trials over the past decade. Drug Discovery Today vol. 24 440–451 (2019).2. Bailey, M. M., Gorman, E. M., Munson, E. J. & Berkland, C. Pure insulin nanoparticle agglomerates for pulmonary delivery. Langmuir 24, 13614–13620 (2008).3. Shenoy, B., Wang, Y., Shan, W. & Margolin, A. L. Stability of crystalline proteins. Biotechnol. Bioeng. 73, 358–369 (2001).4. Basu, S. K., Govardhan, C. P., Jung, C. W. & Margolin, A. L. Protein crystals for the delivery of biopharmaceuticals. Expert Opin. Biol. Ther. 4, 301–317 (2004).5. Hirata, G. A. M., Bernardo, A. & Miranda, E. A. Crystallization of porcine insulin with carbon dioxide as acidifying agent. Powder Technol. 197, 54–57 (2010).6. Parambil, J. V., Schaepertoens, M., Williams, D. R. & Heng, J. Y. Y. Effects of oscillatory flow on the nucleation and crystallization of insulin. Cryst. Growth Des. 11, 4353–4359 (2011).7. Chen, F. et al. Crystallization of bovine insulin on a flow-free droplet-based platform. in AIP Conference Proceedings vol. 1820 030003 (American Institute of Physics Inc., 2017).8. Wood, B., Girard, K. P., Polster, C. S. & Croker, D. M. Progress to Date in the Design and Operation of Continuous Crystallization Processes for Pharmaceutical Applications. Org. Process Res. Dev. 23, 122–144 (2019).9. Hadiwinoto, G. D. et al. Integrated Continuous Plug-Flow Crystallization and Spray Drying of Pharmaceuticals for Dry Powder Inhalation. Ind. Eng. Chem. Res. 58, 16843–16857 (2019).10. Neugebauer, P. & Khinast, J. G. Continuous Crystallization of Proteins in a Tubular Plug-Flow Crystallizer. Cryst. Growth Des. 15, 1089–1095 (2015).11. Jiang, M. et al. Continuous-flow tubular crystallization in slugs spontaneously induced by hydrodynamics. Cryst. Growth Des. 14, 851–860 (2014).",22,5.0
"Oral modified release (MR) dosage forms are developed by altering the kinetics and/or site of drug release/absorption in order to achieve specific clinical objectives, such as improved patient compliance, optimized efficacy and reduced adverse events. This presentation introduces basic design principles and different MR delivery technologies. It also discusses the importance of understanding material, formulation, process and their interplays in rational product and process development along with case studies. ",23,0.0
"Although a drug might be developed and formulated for oral delivery, the same compound is commonly administered as a solution via an IV injection/infusion in order to determine absolute bioavailability to support early development. A common challenge to develop IV formulation is the low solubility of drug candidates, risking phlebitis due to precipitation upon injection. Other parameters that need attention include the pH variation (too high or too low) that can cause irritation, and osmolality differences that may cause significant fluid shifts between the intracellular and extracellular spaces. Thus, the development of a stable and safe IV formulation requires an adequate selection of solubilizers and evaluation of the safety of the solution prior to administration.In this study, a weak acid drug with poor water-solubility required a minimum of 40-fold increase in aqueous solubility for IV delivery. To achieve increased aqueous solubility, multiple strategies were assessed including screening of solvents and co-solvents, in situ salt, and pH control. Phlebitis risk was evaluated by in vitro static dilution using surrogate plasma and observation of flocculation in rat plasma by optical microscopy. In conclusion, a pH-buffered solution containing in situ sodium salt allowed for up to 133-fold increase in aqueous solubility of the compound showing no risk of precipitation while complying with pH and tonicity requirements for IV administration.",23,1.0
"During drug product development, science-based design of formulation and manufacturing process is essential for achieving and assuring consistent quality products. This presentation discusses rational product development based on the integrated product and process knowledge. It addresses the importance of fundamental understanding of physicochemical, mechanical and biopharmaceutical characteristics of API, formulation, process and their interactions during product design, development, scale-up and commercial manufacturing with case studies in ensuring consistent quality, robust drug delivery performance and process.",23,2.0
"Use of predictive technologies and in silico modeling can significantly enhance speed, efficiency and effectiveness of drug product and process design. This presentation will discuss several case studies, in which multidisciplinary sciences and engineering principles, coupled with high power computing, are used to guide formulation development, manufacturability assessment, and drug substance solid form risk assessment.",23,3.0
"Amorphous solid dispersion (ASD) is a highly effective drug delivery technology for enabling oral absorption of insoluble APIs. This presentation discusses basic principles of formulation design & characterization, and importance of understanding material, formulation, dissolution (LLPS) and stability in rational product development using case studies",23,4.0
"Biopharmaceutical Classification System (BCS) Class-II and Class-IV drugs (active pharmaceutical ingredients, APIs) have low aqueous solubility and potentially low bioavailability. To overcome the limitations, soluble polymeric drug dispersion as films by either solvent casting or hot-melt extrusion is emerging as a feasible technique, where lab-scale tablet can be made by 3D printing. Film based solid dispersion of drug formulation can side-step and replace current legacy-based process design through iterative trial-and-error in pharmaceutical industries and facilitate a step-by-step highly scalable and rapidly implementable predictive physicochemical modeling based on drug release, storage stability and mechanical structure of the film. However, product design requirements need to be transformed to mathematical design instructions (dosage, release, stability, aesthetics, multiple APIs) for optimized tuning of the drug release rate, stabilization and dosage flexibility with continuous manufacturing to be predicted in advance. Predictive modeling is greatly simplified for a film-based approach that provide dimensional control by regulating transport and phase behavior to a 1D layer, thus streamlining manufacturing and scaling process. It also provides the opportunity for a multilayer film-based formulation for faster transition from bench to clinical trials to commercial production.A mathematical modeling framework for the design and parameter optimization during the formulation of hydrophobic API-loaded soluble polymeric film formulations was implemented for the foundation for customer specific drug design in terms of dosage, release rate, storage stability (self-life) and mechanical properties (bending/cutting). Overall optimization initially involves two components: dissolution and stability models, which have to be solved simultaneously for targeted design requirements in terms of thickness, geometry, order, composition and number of layers. Unsteady-state Fickian diffusion describes both dissolution and stability modeling. During dissolution, the relative rate of water intrusion (swelling controlled) and polymer erosion from surface coupled with API diffusion primarily govern the observed release rates, correlated with expected different release mechanisms (immediate/slow). For stability against water/oxygen, barrier polymer layer is modeled by an unsteady-state diffusion of water/oxygen along with mass transfer (partition) in the surface and interface to achieve < 1% impurity formation within 12 months considering impurity formation kinetics. Polymer diffusivity, solubility, active layer and barrier layer thickness, area, mass transfer coefficients and relative rate constants can be estimated and optimized using the model. Physical properties are determined from thermodynamics. Prototype creation and characterization, experiments will provide physical parameters for release, uniformity, stability and dose scaling. Generic algorithm for optimization/parameter fitting is pursued, where inner loops consisting smaller modules (release, stability, structure) are encircled by outer overall optimization loop (onion model) until any of the constraints (total mass < 600 mg, total thickness < 17 mm, area <153 mm2, and volume <1071 mm3) are met. The model is adaptive to progressive complexity such as presence of plasticizer/binders, glass-rubbery transition, phase separation, polymer diffusion, aqueous boundary layer, barrier dissolution and multi-dimensional transport, which can be independently adjusted, adapted and improved. Crystal or particle dissolution kinetics can be incorporated in case of crystalline/particulate drugs. Finally, film flexibility testing using 3-points flexural bending to express required mechanical properties (flexural stress, strain and modulus) in terms of force, deflection and dimension can also be incorporated in the overall optimization scheme.",24,0.0
"Moisture plays a major role in determining the attributes of granules prepared by fluidized bed granulation (FBG). In this talk, we are presenting a semi-theoretical droplet-based evaporation rate model that was developed and incorporated into moisture mass-enthalpy balances to simulate the temporal evolution of bed moisture-temperature. Experimental data from a GPCG30 unit were used to fit the model parameters. With only two fitting parameters, the model demonstrated very good capability to describe the moisture-temperature evolution for a wide range of operating conditions. Then, in a global process model (GPM) approach, the evaporation parameters were fitted to multi-linear functions of inlet air temperature, binder concentration, and spray rate. The GPM was validated successfully by simulating a different data set which was not used in its calibration. As the GPM demonstrated a good predictive capability, it was further used to investigate the impacts of process parameters. Numerical simulations suggest that the proposed GPM predicts the experimentally well-established trends of moisture-temperature profiles in previously published data, proving the applicability of the GPM approach. This study has demonstrated the capabilities of simple process models as a practical approach to predict time-wise evolution of bed moisture-temperature profiles in industrial FBG modeling, while also pointing out their limitations.",24,1.0
"Amorphous solid dispersion (ASDs) are a rising strategy to overcome the poor solubility of drugs in water, since they can maintain supersaturation concentrations of the drug and thus increase its oral bioavailability. Many approaches are reported to predict the oral absorption of drugs, namely the combined use of biorelevant dissolution testing with in silico modeling.1 There are not many publications regarding the prediction of in vivo ASDs behavior2 and the current mathematical models are simplistic on describing supersaturation as the ratio between the bulk concentration and the concentration at saturation of the drug. This approach seems inadequate to describe supersaturation and can be a roadblock to accurate physiologically based pharmacokinetic (PBPK) modelling of ASDs.This work aims at describing the behavior of ASDs when dissolving into a biorelevant medium taking into consideration a combination of four events: dissolution, supersaturation, nucleation and crystal growth. These events were described by differential equations and constants with physical meaning.ASDs were produced by spray-drying following a full-factorial design of experiments, varying the drug load, the ratio of atomization and the outlet temperature of spray-drying. The behavior of these ASDs was evaluated by dissolution in biorelevant medium including a pH shift from FaSSGF medium (pH 1.2) to FaSSIF medium (pH 6.5) to mimic the conditions in the gastrointestinal tract.The first attempt combined models describing dissolution and precipitation by using differential equations but describing supersaturation as a simple ratio between the concentration of the drug and its concentration at saturation. However, this attempt failed to be a good descriptor of supersaturation, as the model did not reflect the experimental data from ASDs presenting significant burst release.In order to have the drug’s concentration at saturation described by differential equations, rather than by the ratio between concentrations, the Noyes-Whitney equation was modified taking into consideration the maximum concentration for each ASD, rather than being restricted by the concentration at saturation of the drug. Then, the behavior of ASDs was modeled by combining four differential equations: the Noyes-Whitney equation for dissolution, the modified Noyes-Whitney equation for supersaturation, and the nucleation and crystal growth equations for precipitation. This approach was able to describe ASDs with different behaviors (error between experimental and simulated data < 2%). This outcome was possible because the dynamic process of dissolution was based on dissolution, supersaturation and precipitation equations of the drug.Figure 1 shows the experimental and simulated data for ASDs produced by spray-drying with different behaviors: spring and parachute, absence of precipitation, absence of supersaturation, and delayed and incomplete drug release (produced by hot melt extrusion, HME).This work contributes for a better understanding of an ASD behavior describing the supersaturation mechanism, which directly impacts on the drug’s bioavailability and plasmatic concentration.References:1. Kaur, N., Narang, A. & Bansal, A. K. Use of biorelevant dissolution and PBPK modeling to predict oral drug absorption. Eur. J. Pharm. Biopharm. 129, 222–246 (2018).2. Mitra, A., Zhu, W. & Kesisoglou, F. Physiologically based absorption modeling for amorphous solid dispersion formulations. Mol. Pharm. 13, 3206–3215 (2016).",24,2.0
"Thin coatings of polymer films find numerous technological applications in various industrial sectors related to protective and functional coatings. Tablets in the pharmaceutical industry may be coated for branding, to improve appearance or taste, increase stability or to achieve sustained or delayed release of the drug. In this study, these coatings are initially cast as thin films of polymer solutions, which dry to form a solid film. The solidification process due to the removal of the solvent phase is often accompanied by shrinkage of the film. If the polymer adheres to the substrate, the film can only shrink in the film-normal direction since the substrate prevents contraction in the transverse plane. In such cases, drying is accompanied by the development of transverse tensile stress in the film. If the tensile stress exceeds a critical value, the polymer film may develop cracks, thereby compromising the integrity of the film. Alternatively, if the adhesion between the film and the substrate is weak, the film may debond instead of cracking. It is essential to understand these mechanisms to obtain defect-free films.In the present work, transverse stress was measured during the drying of the film using glass as a cantilever substrate for solutions of 10% and 20% total polymer solids (Figure 1, left panel). Two polymer systems used in this study were CA (cellulose acetate): PEG (polyethylene glycol) and CA (cellulose acetate): HPC (hydroxypropyl cellulose). It was found that final drying stress remains the same for both the polymer systems irrespective of concentration and coating layers except for 10% CA: HPC films. Transverse stresses in the latter films were found to be high and suggest a higher propensity for cracking (Figure 1, right panel). Cross-sectional SEM images were taken to relate the microstructure to the stress. Ultimate tensile strength of polymeric films was also determined using a tensile testing instrument. The tensile strength decreases with flaw size, in accordance with the Griffith’s criterion. A simple mathematical model was developed to predict the critical thickness and critical stress beyond which the film would crack.",24,3.0
"The Wurster process is commonly used to modify the release rate or the taste of granulated beads in the pharmaceutical industry. Hereby, beads are coated with one or multiple functional films that control the release of drug substances or mask a specific taste. The coating process takes place in the Wurster tube, in which the particles are transported centrally upwards followed by a subsequent downwards movement outside the tube. Thus, the particle flow resembles a torus-shaped recirculation pattern.From former studies, it is known that small droplets, a diluted coating solution and low spray rate improve the coating layer homogeneity1. On the other hand, such a process is energy and time consuming. Low spray rates and small droplets promote spray drying (premature droplet drying, inhibits spray deposition on particles), reducing the process efficiency. Additionally, the coating quality increases with higher spray rates1,2. From product quality point of view, the spray rate is capped by the amount of spray drying on the lower boundary and by overwetting, which would lead to agglomerations, on the upper boundary. Therefore, investigating the heat and mass transfer inside the coating device can help to find optimal process parameters.In our work, a fully integrated CFD-DEM environment is used to model the transport phenomena in a lab-scale Wurster coater. The commercial DEM code XPS is coupled to AVL-FIRE for the CFD simulation. State-of-the-art models for momentum, heat and mass transfer between continuous and discrete phase are integrated in a fully coupled manner3. Temperatures, vapor mass fractions and evaporation rates of a coating process with a multicomponent spray solution are calculated. The amount of spray drying is quantified and related to the input parameters of the process.van Kampen A, Kohlus R. Statistical modelling of coating layer thickness distributions: Influence of overspray on coating quality. Powder Technol. 2018;325:557-567. doi:10.1016/j.powtec.2017.11.031van Kampen A, Kohlus R. Systematic process optimisation of fluid bed coating. Powder Technol. 2017;305:426-432. doi:10.1016/j.powtec.2016.10.007Forgber T, Toson P, Madlmeir S, Kureck H, Khinast JG, Jajcevic D. Extended validation and verification of XPS/AVL-FireTM, a computational CFD-DEM software platform. Powder Technol. 2020;361:880-893. doi:10.1016/J.POWTEC.2019.11.008",24,4.0
"Research Interests The structuring of solvent at the crystal-solvent interface largely regulates the activation barrier for incorporation of solute into the crystal. Depending on the groups exposed on the crystal surface, solvent at the interface may be ordered or disordered. In general, consequences of solvent structuring on the molecular level interactions comprising crystallization are poorly understood. Organic solvents, in which the bonds that could support a defined fluid structure are weak and varied, present a particular challenge to understand the effect of solvent structuring and solvent-solute interactions on thermodynamics and kinetics of crystallization. We explore the thermodynamic parameters of crystallization for etioporphyrin I (a planar free base porphyrin) in a range of organic solvents from alcohols (octanol, hexanol and butanol), through a polar aprotic solvent (DMSO), to a carboxylic derivative (caprylic acid). Thermodynamic characteristics of crystallization along with absorption spectroscopy were used as probes to decipher the presence of solvent structuring and solute-solvent interactions for non-solvate crystals crystallizing from different organic solvents. Time-resolved in situ atomic force microscopy monitoring of the step dynamics on (010) face of etioporphyrin in two types of solvent, alcohols (octanol, butanol and hexanol) and DMSO, having varying aliphatic chain lengths and subsequent solvent viscosities demonstrate the impact of solute-solvent interactions and solvent viscosity on the barrier for solute incorporation into kinks. The relationship between step kinetic coefficient and solvent viscosity reveals that the crystallization of organic small molecules is not governed by diffusion limitation but rather by solute-solvent interaction at the kinks. The correlation between the step velocity and the solute concentration on the (010) face of etioporphyrin I for different solvents reveal that for solvents belonging to the same homologous series, solute incorporation scales with the solute diffusion coefficient. All-atom MD simulation for solvent structuring at the crystal interface emphasizes the importance of solute-solvent interaction at the crystal interface. These emerging insights offer guidance for polymorph selection, chiral separations, structure design and numerous other open questions relevant to myriad crystallization systems in the pharmaceutical and chemical industries.",25,0.0
"The activity of platinum group metals have placed majority of these catalysts far above other types of metal catalysts, turnover frequencies have typically landed them on top of the volcano curves of most chemical reactions. However, its biggest attraction is also the reason for its lack of practicality; two of the biggest problems involving platinum group metals include lack of selectivity control, and susceptibility to poison1. Hydrodeoxygenation (HDO) is a key chemistry in pushing forward more bio-derived feedstocks in the industry, succinic acid -through HDO- has been highlighted as a potential alternative to maleic anhydride for the production of 1,4-butanediol, gamma-butyrolactone and tetrahydrofuran. However, upgrading these carboxylic acids proves problematic over the preferred catalysts (noble metals) because of a lack of selectivity control. These active catalysts facilitate a multitude of undesired reactions such as hydrogenolysis, decarbonylation, decarboxylation, and methanation, which all lead to a slew of low-value products2. It is widely reported that the addition of oxophilic “promoter metals,” such as Sn can be used to tune the selectivity of noble metals during the HDO of carboxylic acids3; however, their mechanism of action is poorly understood. Another common problem in both upgrading and extracting energy (fuel cells) from carboxylic acids is formation of carbon monoxide, which has a tendency to inhibit reaction rates. A new approach has been brought to the forefront in modern catalyst operation, dynamic catalysis. This involves the use of an external stimuli in order to oscillate between energy state4; these different states will manipulate the rate determining steps (rds) in a reaction mechanism, bouncing between different rds’s has the potential operate at rates magnitudes higher than operating under a static condition. In this poster, we use two separate approaches for solving the practical limitations of platinum group metals.First, we use propionic acid HDO as a model system for tackling selectivity control over supported Pt catalysts. While succinic acid HDO might be more practical to an industrial setting, a simpler molecule is ideal for a fundamental study as it captures all the essential chemistries expected without a product stream being overcrowded. We will dissect HDO activity over monometallic Pt using experimental and computational techniques, providing macroscopic insight on reaction pathways and major inhibitors to HDO selectivity. This shall play a crucial role in understanding how the addition of an oxophilic promoter, Sn, can intrinsically tune the catalytic activity towards higher yields of HDO products.Second, we explore the effect of dynamic catalysis on carboxylic acid oxidation using electric fields. Formic acid oxidation will be used as a model reaction for the production of carbon dioxide, this is common reaction in fuel cells. Oscillating in between different applied potentials will allow us to observe this concept experimentally and the degree to which a rate increase can be achieved using a potentiodynamic approach while also overcoming the hurdle of product inhibition.Research Interests Oxygenated compounds have become a major corner stone in today’s industrial landscape, finding itself on the forefront of energy storage as well as chemical production. The expansion of modern catalysis is tantamount to efficiently carry out these chemical processes in order to meet social demands. As there are many aspects of a chemical process that can be approved upon, there are also many approaches one can take to solve current problems, and in some instances, a combination of approaches. Bimetallic catalysts have demonstrated it’s resistance to product inhibition though the production of oxygenated products over typical undesirable ones that can be formed over highly active catalysts3: undesired compounds include carbon monoxide, carbon dioxide and alkanes/alkenes. Dynamic catalysis has recently been brought to the forefront as a means of increasing catalytic turn over frequencies by switching between catalytic energy states, theoretical and experimental work has exhibited reaction rates orders of magnitude higher during dynamic operation versus static operation4-5. It has also been reported through a theoretical investigation, that dynamic catalysis also has the capability of tuning the selectivity of pathways by making alterations to the amplitude and frequency of the stimuli promoting the dynamic operation6. My research interests include leveraging the favorable properties of bimetallic catalysts along with the vast potential of dynamic catalysis to tune the selectivity while increasing the rates of reactions involving heavily oxygenated compounds. There have been past studies that unknowingly demonstrated this idea to be plausible in an experimental setting7-8, however without approaching the concept with the specific purpose of uncovering it’s mystery, there will be gaps in the overall understanding that will dampen it’s journey to real world applications. These research interests can find a place within many different industrial sectors such as energy extraction, biomass upgrading and pharmaceuticals; a refined solution to a commonly encountered problem.ReferencesGurbuz, E.; Bond, J. Q.; Dumesic, J. A; Román-Leshkov, Y. 261-288. Elsevier, 2013.Lugo-José, Y. K.; Monnier, J. R.; Williams, T. C. Appl. Catal. A: General 469, 410-418. 2014.Vardon, D. R., Beckham, G. T., et al. ACS Catal. 7, 9. 2017.M. Alexander, A.; Omar A., A.; Paul, D., Principles of Dynamic Heterogeneous Catalysis: Surface Resonance and Turnover Frequency Response. 2019.Gopeesingh, Joshua; Ardagh, Matthew; Shetty, Manish; Burke, Sean; Dauenhauer, Paul; Abdelrahman, Omar (2020): Resonance-Promoted Formic Acid Oxidation via Dynamic Electrocatalytic Modulation. ChemRxiv. Preprint.Ardagh, M. A.; Shetty, M.; Kuznetsov, A.; Zhang, Q.; Christopher, P.; Vlachos, D.; Abdelrahman, O.; Dauenhauer, P. J., Chem. Sci. 2020.Adžić, R. R.; Popov, K. I.; Pamić, M. A., Electrochim. Acta 1978, 23 (11), 1191-1196.Juan Victor Perales-Rondón; Adolfo Ferre-Vilaplana; Juan M. Feliu; Enrique Herrero. Journal of the American Chemical Society. 2014.",25,1.0
"Research Interests: In this poster, I present an overview of my research on building active colloids and colloidal systems, and manipulating their transport behavior through external (imposed gradient/ microenvironment) and intrinsic (materials/ size/ shape of colloid) means. As an aspiring industry candidate, I envision working at the interface of chemical engineering, biochemistry and material science to tackle industry challenges in formulations and formulation processes. By applying my colloidal and surface science expertise, I aim to understand the physico-chemical properties of food, pharmaceutics, and fine chemicals formulations to develop products that are optimized for customer needs.Research Experience:My research experience of 7 years encompasses areas of active colloids, emulsions and microfluidics. Prior to my PhD work at Penn State Chemical Engineering, I worked as a research engineer at the National University of Singapore for two years, where I also received my undergraduate degree from. Here is a brief description of my research as (1) a PhD student and (2) a research engineer/ undergraduate student.Colloids and active matter (2016 – Present) During my time at Penn State, I worked to understand the motility and diffusion behavior of soft colloids such as enzymes, proteins and vesicles. A major part of my research comprises of characterizing movement and determining the mechanism of movement of soft colloids. Through this experience I developed expertise in areas of colloidal synthesis and characterization, colloidal and interfacial science, and fluid dynamics.Developed enzyme-powered protocell nanomotors as potential biocompatible cargo transporters.Characterized a new mechanism of motor movement based on solute-membrane interactionsApplied microfluidic and microscopy techniques to study and characterize the behavior of active molecules and colloids in gradients of solutesExperienced in the techniques of microfluidic gradient generators, surface functionalization of diverse colloids such as polystyrene, silica, vesicles, proteins and enzymes and fabricating active colloidsCharacterization techniques like dynamic light scattering, zetasizer, confocal microscopy, fluorescence recovery after photobleaching, enzyme and protein purification, particle size and diffusion analysisMicrofluidic emulsion based processing of colloids (2013 – 2016) During my time at the National University of Singapore, I used microfluidic emulsion generators to synthesize micro/ nanoparticles for a variety of applications. The research I did during my undergraduate thesis project and as a research engineer culminated into a start-up spin off from my lab. Through this experience I developed expertise in areas of microfluidic emulsion generators, cargo encapsulation and materials formulation.Developed a comprehensive research strategy to understand and evaluate the effect of polymeric materials on crystallization of pharmaceutical formulations through microfluidic emulsion based processingEstablished and optimized the study of mucoadhesive polymers for particle based ocular drug delivery and quantified the extent of mucoadhesion for various biocompatible polymersDeveloped functional food structures of sodium alginate in the form of microparticles and thin fibres using microfluidic emulsion generation techniquesWorked on the flow based hydrogenation of organic compounds (hexene, nitrobenzene) through tube-based millifluidic reactors",25,2.0
"Research Interests This work deals with building predictive bulk property models, intended to facilitate the use of dry powder formulations in applications such as direct compaction and continuous manufacturing, for which adequate bulk properties such as packing and flow are required before commencement of production. Such property prediction models can shorten the formulation development stage, since formulators will have an approximation of the adequacy of bulk properties before experimental testing, or level of bulk property enhancement required. These models have additional significance to continuous manufacturing, where intermediate, ad-hoc property enhancement is not possible, and adequate bulk properties are required at the start of the continuous manufacturing line. In cases where bulk property enhancement is required, dry coating via nano-silica is a proven alternative to traditional dry or wet granulation, since dry coating is a relatively quick and simple step which can save time, money and resources. During dry coating, the surfaces of host pharmaceutical powders are covered with guest nano-silica particles, decreasing the inter-particle cohesion forces between host particles, and improving bulk properties. Hence, this work aims to build predictive models, which intake particle scale characteristics to predict bulk powder properties, as this will reflect changes at the particle level, such as dry coating based processing. Bond number, which is the ratio of inter-particle cohesion force to particle weight, has shown promising bulk property prediction results in recent studies. The current work will attempt to investigate the ability of Bond number models to predict bulk properties with larger data sets comprising of commonly used pharmaceutical powders, in addition to their mixtures. Results conclude that the current version of Bond number models are able to accurately predict powder bed porosity and powder flow for many commonly used pharmaceutical powders, in agreement with previous studies. However, there are several cases where Bond number model predictions are inaccurate, such as powder bed porosity of rough surfaced particles and binary mixtures where only one component is dry coated. Attempts are made to adjust the Bond number models using particle scale characteristics, such as particle shape descriptors, in order to improve prediction where it is necessary, while maintaining adequate prediction for other powders. Finally, experimental work done for model testing produced interesting trends which may be useful for pharmaceutical formulators, since they hold true for large data sets comprising of commonly used pharmaceutical powders.",25,3.0
"Research Interests:  Reaction Engineering, Computational Chemistry, Pyrolysis, Condensed Phase CatalysisFast pyrolysis is a thermal deconstruction technology that provides routes to break down macromolecules such as cellulose to smaller oxygenates that can be further upgraded to fuels and platform chemicals. Despite significant research in this area, fundamental insights into pyrolytic reaction mechanisms and the influence of the naturally present catalytic environments formed by hydroxyl groups and inorganic metals on kinetics are still lacking. This research utilizes first principles-based computations supported by experimental techniques to elucidate in detail activation (polymer degradation) mechanisms as well as the role of natural catalytic environments in enhancing activation. In relation to the natural hydroxyls, this research elucidates their influence on the kinetics of the transglycosylation reaction. It is shown that hydroxyl groups can act as proton shuttling agents and hence promote facile proton transfer reactions. Density functional theory (DFT) calculated activation energy for this route is in good agreement with activation energy measured by isotopic labeling experiments in the PHASR (Pulsed Heated Analysis of Solid Reactions) set-up (29.5 kcal mol-1 and 26.9 ± 1.9 kcal mol-1 respectively) thus showing that hydroxyl groups can open up seemingly catalytic pathways particularly at temperatures below 470°C. Ab initio molecular dynamics (AIMD) simulations are also used to provide insights into the unique hydrogen bonding networks formed by these hydroxyls. Specifically, the influence of reaction dynamics and entropic effects are captured using these AIMD simulations. In addition to this, influence of inorganic metals on the rates of cellulose activation is also explored. Specifically, this research elucidates the role of group II metals in disrupting hydrogen bonding networks and stabilizing charged transition states formed during cellulose activation. A detailed mechanism for cellulose activation in the presence of group II metals is presented with a DFT computed barrier that agrees well with experimental kinetics measured at 370-430°C. More generally, this research provides fundamental insights into the reaction environments that influence cellulose pyrolysis and can be used to devise strategies for engineering such environments.",25,4.0
"Research Interests Through my academic career at various universities and institutes, I have acquired a unique perspective in understanding and developing cutting-edge materials and have been exposed to various research areas, including organic chemistry, analytical chemistry, polymer chemistry chemical engineering, materials engineering and separation science. My research is highly interdisciplinary but focuses primarily on the rational design, synthesis and characterization of novel materials. The success in developing new materials, small molecules or any chemical entity in general for a certain application is usually achieved by a keen understanding of the structure-property relationships. Over more than 10 years, I have demonstrated excellence in manipulating material structure and composition to suit the requirements of the desired properties. This extensive experience could seemingly be transferred to the small molecules and new drug development by building the chemical structure-property relationship understanding needed to identify optimal structures for achieving required biological attributes and physical, chemical and pharmaceutical properties relevant to the application.My PhD. in materials chemistry and the seven years of professional experience in the U.S. Department of Energy National Labs, enable me to acquire a great breadth of scientific knowledge and experimental, communication, technical, and leadership skills, as summarized below.I have demonstrated excellence and extensive experience inDesign, synthesis, characterization and modification of small organic moleculesPorous Materials, polymers and polymer compositesNanoparticles synthesisDeep knowledge in the structure-property relationships and tuning of material properties to meet performance and reliability requirements.Fabrication of membranes, thin films, monolith, pellets and tabletsMaterials manufacturing and characterizationDevelopment of new chemical entitiesSynthetic organic chemistryMultistep synthesis of complex organic moleculesDevelopment of new synthetic routes (Organic and Inorganic)Analytical development and characterization using several chromatographic and biophysical characterization methodsSeparation ScienceElectrochemical techniquesDrug deliveryStructural analysisFormulation chemistryProcedures relating to purification and separationsWater treatmentHeterogeneous catalysisConducting work at scales spanning the range from laboratory scale to multi-kilogram scaleQuality control and complying environmental health and safety requirementsDelivering high-quality resultsIndependent planning and execution of experiments, interpretation of data and drawing conclusions from resulting dataMentoring of young chemists, scientists and engineers to share best practice and knowledge toward efficiently achieved successful outcomesMeeting project timelines and objectivesLeadership and StewardshipWorking in several multi-disciplinary project teamsManaging multiple projects and prioritiesHigh learning agilityCreative problem solvingOperating at pace and agile decision-makingCross-functional collaborationRemaining current with the latest technologiesWriting proposals, scientific papers, patents and reports and giving presentations on technical and scientific findings.Instrumentation skills:Fourier-transform infrared spectroscopy (FTIR), Thermogravimetric Analysis (TGA), Ultra Performance Liquid Chromatography (UPLC), Gas Chromatography (GC), Mass Spectrometer (MS), High-Performance Liquid Chromatography (HPLC), Nuclear Magnetic Resonance (NMR), Single-Crystal X-ray Diffraction (SCXRD), Powder X-ray Diffraction (PXRD), Scanning Electron Microscope (SEM), Energy-dispersive X-ray Spectroscopy (EDS), Transmission Electron Microscope (TEM), X-ray Photoelectron Spectroscopy (XPS), Surface Area and Gas Analyzer, High-Pressure Gas Sorption Analyzer, Glove Box (Gas or Vacuum), Breakthrough Analyzer (Hiden), Inductively Coupled Plasma - Optical Emission Spectrometry (ICP-OES), Ultraviolet-Visible Spectroscopy (UV-Vis), Pycnometer, Differential Scanning Calorimetry (DSC), Dynamic Mechanical Analysis (DMA), Schlenk Line, Gas Permeation Analyzers (Pure-Gas and Mixed-Gas), Electrochemical Impedance Spectroscopy (EIS)Scholarly Achievements:Citations: 879, h-index: 17, i10-index: 21, total publications: 28Peer-Reviewed Research Articles: 25 published, and 4 in preparation (6 as a corresponding author, 19 as a first author)Peer-Reviewed Review Articles: 3 published and 1 in preparation (2 as a corresponding author and 2 as a first author)A cover image for Cell Reports Physical Science - May issue, 2020Patents: 1 published and 2 in preparationResearch Grants: 2 PI (Granted; $322K in 2020 and $120K in 2015), 2 Co-PI (Granted; $400K in 2019 and $506K in 2018), 1 fellowship, 1 conference grant, 2 participation in proposal’s writingInvited Talks: 3 (MIT, UC-Berkeley Energy Frontier Research Center, Florida Institute of Technology)Conference Oral Presentations: 11Conference Poster Presentations: 6In the News: https://www.netl.doe.gov/node/9779; https://www.pnnl.gov/news-media/fast-and-easy-two-step-creates-new-porous-materials; https://www.pnnl.gov/news/release.aspx?id=4490; https://www.pnnl.gov/science/highlights/highlight.asp?id=4883; https://phys.org/news/2018-04-decreasing-magnetism-metallic-core-particles.html; https://www.pnnl.gov/science/highlights/highlight.asp?id=4460Leadership: Leading three projects in collaboration with NETL separation team membersLeading several projects in collaboration with other national labs and universitiesProfessor Ju Li (MIT), Dr. Praveen Thallapally (PNNL), Professor Brian Space (University of South Florida), Professor Jeffrey McCutcheon (University of Connecticut), Professor Moises Carreon (Colorado School of Mines), Professor Omar Farha (Northwestern University), Dr. Yoosuf Picard (Carnegie Mellon University) and Professor Jeffery Long (University of California, Berkeley), Dr. Tony Pham (University of Tampa), Dr. Nathalie Rosi (University of Pittsburgh)Mentored ten students (nine undergraduate students at USF and one Ph.D. student at PNNL)Presentations at international/national conferences (11 talks, 3 invited talks and 6 posters)Leading large discussion sessions for groups of 500+ (lectures at Alexandria University) and 200+ (Inorganic Chemistry lectures at USF) students.Session chairing in conferences: ACS Meeting, Fall 2019Conference organization: Castle Conference (USF) and International Conference on Chemistry Progress (Alexandria University, Egypt)",25,5.0
"Research Interests Hydrometallurgy, liquid-liquid separation, purification, actinide science, radiochemistryNuclear reactors provide an attractive option for significant baseload power generation with zero carbon emissions. However, treatment of operation radioactive byproduct is an ongoing challenge. The separation of highly radiotoxic, long-lived radioactive elements from used nuclear fuel (UNF) followed by their conversion into shorter-lived nuclides is instrumental in strengthening interest and public opinion towards nuclear power. Under current industrial processes utilizing liquid-liquid separation, the highly radiotoxic long-lived minor actinide elements, americium and curium, are not efficiently removed from UNF. The selective removal of the minor actinides is difficult due to the co-extraction of lanthanides, which significantly interfere with the treatment process. The Actinide Lanthanide Separation Process (ALSEP), consisting of the extracting organic solvent containing 2-ethylhexyl phosphonic acid mono-2-ethylhexyl ester (HEH[EHP]) and N,N,N’,N’-tetra(2-ethylhexyl)diglycolamide (T2EHDGA) in n-dodecane, was developed for the selective partitioning of minor actinides from the lanthanides. Due to the inherent radioactive nature of UNF, liquid-liquid separation systems must be resistant to ionizing radiation. Therefore, to improve our understanding under process operating conditions, the ALSEP extracting solvent was equilibrated with metal loaded acidic aqueous phase and subjected to gamma and alpha irradiation. Degradation dose constants revealed greater ligand degradation due to gamma than alpha irradiation, and equilibration with nitric acid did not significantly impact degradation kinetics. However, the presence of nitric acid appears to alter the degradation pathway by favoring the formation of higher molecular weight recombination products. Results revealed the greater radiolytic susceptibility of T2EHDGA over HEH[EHP] could disrupt the minor actinide recovery with increasing radiation exposure.",25,6.0
"Research Interests My primary research interests are in the application and development of multiscale modeling frameworks for the rational design of functional and adaptive polymeric materials to address challenges related to drug screening and drug delivery. Technological innovations over the last several decades have been powered solely by our ability to uniquely understand the relationship between the chemistry of materials and their emergent macroscopic properties. In fact, these have led to the development of several novel classes of bulk materials such as thermoplastics and thermosets, which are today used widely in both pharmaceutical and manufacturing industries. Recently, to diversify the design parameter space, efforts are being focused on engineering complex materials (for ex. polymer grafted nanoparticle) that comprise two or more constituents with different chemistries. The central idea here is to leverage the intriguing behaviors arising at the interfaces formed by the constituents so that several materials with different macroscopic properties but with little variations in their chemistry can be designed. Though this approach sounds promising, each of these materials is still engineered for a specific targeted application and thus, it would be highly valuable to have one material that can be tuned systematically by suitable external perturbations such that it dynamically alters its microscopic structure and provides different functionalities.I am particularly interested in identifying the various factors that can be used to realize such adaptive materials. Biology, especially, has umpteen number of examples where protein complexes self-assemble into different higher-order structures depending on several environmental cues such as concentrations, external forces, presence of salts, etc. For example, it is known that actin filaments inside cells form various structures like crosslinked networks, bundled filaments, branched filaments, and so on depending on their concentration and interactions with other proteins. I am interested in elucidating the molecular drivers of these delicate processes and translate them to computationally design synthetic materials with wide-ranging implications. My initial focus is on developing materials that can adaptively recapitulate cellular architectures for drug screening and targeting purposes. Such material when synthesized experimentally would enable rapid and high throughput screening of various drug candidates and identify potential drugs with increased efficiency. My approach will involve a systematic application of multiscale modeling techniques such as atomistic, coarse-graining, enhanced sampling, Monte-Carlo methods, etc. to develop computational models across varying length and time scales as required to understand these complex processes in detail.Research ExperienceMy research from Ph.D. to postdoc has always focused on elucidating the fundamental molecular mechanisms connecting a material’s structure with its properties. I have worked on several classes of materials ranging from bulk and interfacial polymeric systems to biological protein complexes using multiscale modeling techniques to uncover the molecular determinants of their macroscopic transport, volumetric, thermal, and mechanical properties. In one of the projects during my Ph.D., I identified the key connections between the chemistry of various polyacrylate gels and the transport of small molecules (water and ethanol) through them. In fact, I also studied how these relationships change with the introduction of an interface by supporting polyacrylate gels on another hard substrate. In these studies, I systematically characterized the effects of physical and chemical characteristics of polymers such as their flexibility, hydrophobicity, and interfacial interactions on the transport properties of penetrant molecules. These results are highly generalizable to other polymeric systems and can aid in the design of efficient membranes for separation applications. In my postdoctoral training, I worked on developing a first-of-its-kind particle-based reactive-diffusive coarse-grained model of actin filaments to unravel the various factors driving the formation of higher-order actin assemblies inside a cell. Specifically, I demonstrated how the application of external forces can alter the actin filament structure in turn affecting the overall functionalities of the cell. Although the exact mechanisms leading to the formation of these structures are not known completely, it is established that they impart cells with diverse functionalities such as their ability to move, divide, and transport cargo. A clear understanding of the mechanisms underlying these processes would enable one to engineer adaptive materials that can dynamically alter their structure and provide various functionalities. A key aspect of all my prior works is the application of multiscale computational modeling approaches such as Monte Carlo, atomistic molecular dynamics, coarse-graining, enhanced sampling, and Markov State models to seamlessly connect information across length and timescales thereby allowing to systematically investigate the emergent macroscopic behavior from microscopic mechanisms. In summary, I have more than 7+ years of experience in applying the above computational methods to study a wide variety of systems such as synthetic crosslinked polymer networks, hydrogels, supported polymers, polymer thin films, and protein complexes. I am also highly proficient in (1) implementing new theoretical methods as computational tools using C++/MPI parallel and (2) in automating simulation protocols and data analysis procedures using a combination of C++, MATLAB, Python, and Linux.Postdoctoral work: ""Multiscale Modeling of Actin Filament Networks"" under the guidance of Prof. Gregory A. Voth at the University of Chicago.Ph.D. Dissertation: ""Structural and Dynamic Properties of Penetrant Molecules in Unsupported and Supported Hydrated Gels"" under the guidance of Prof. Rajesh Khare at Texas Tech University",25,7.0
"Many industrial processes collect an abundance of data from different sensors. These sensors often measure a wide variety of physical properties in order to ensure that these parameters are adhering to expected values. This is essential to ensure plant and operator safety, increase economic benefits, and maintain product quality.A number of algorithms have been developed in order to improve existing fault detection and diagnosis performance. These algorithms integrate a number of different data-driven driven tools and methods. Multiscale wavelet-based representation of data can be used in order to handle data that is autocorrelated, non-Gaussian, and noisy (Bakshi, 1998). Hypothesis testing methods such as the Generalized Likelihood Ratio (GLR) technique can be used in order to provide the best possible detection for a fixed false alarm rate (Reynolds & Lou, 2010). Moreover, certain model-based methods have been developed in order to monitor process drifts and degradations in the process model, even when a process is operating under control (Sheriff et al., 2019).This work will discuss different hybrid monitoring algorithms that were developed, their features, and practical applications, which include fault detection, diagnosis and classification of the benchmark Tennessee Eastman Process (TEP), and online monitoring of fouling in industrial heat exchangers (Basha et al., 2020; Sheriff et al., 2017, 2018, 2019).Research Interests:  Process systems engineering with an emphasis on the development of machine learning-based methods for process modeling, estimation, fault detection, and control. The algorithms and tools developed are utilized in many applications to improve the operation of various chemical, environmental, biological, and electrical systems.ReferencesBakshi, B. R. (1998). Multiscale PCA with application to multivariate statistical process monitoring. AIChE Journal, 44(7), 1596–1610. https://doi.org/10.1002/aic.690440712Basha, N., Sheriff, M. Z., Kravaris, C., Nounou, H., & Nounou, M. (2020). Multiclass data classification using fault detection-based techniques. Computers and Chemical Engineering, 136, 1–11. https://doi.org/10.1016/j.compchemeng.2020.106786Reynolds, M. R., & Lou, J. (2010). An Evaluation of a GLR Control Chart for Monitoring the Process Mean. Journal of Quality Technology, 42(3), 287–310. https://doi.org/10.1080/00224065.2010.11917825Sheriff, M. Z., Karim, M. N., Nounou, H. N., & Nounou, M. N. (2018). Process monitoring using PCA-based GLR methods: A comparative study. Journal of Computational Science, 27, 227–246. https://doi.org/10.1016/j.jocs.2018.05.013Sheriff, M. Z., Mansouri, M., Karim, M. N., Nounou, H., & Nounou, M. (2017). Fault detection using multiscale PCA-based moving window GLRT. Journal of Process Control, 54, 47–64. https://doi.org/10.1016/j.jprocont.2017.03.004Sheriff, M. Z., Nounou, H., Nounou, M., & Karim, M. N. (2019). Monitoring process degradation through operating regime based process monitoring. AIChE Spring Meeting and Global Congress on Process Safety: Process Control Monitoring and Analytics.",25,8.0
"Research Interests My core research interests consist of data and complex systems under the general theme of decision-making. How to make better decisions is a fundamental question for individuals, organizations, and societies, and there is no easy answer. Systems research, especially process systems engineering (PSE), with the help of artificial intelligence (AI), is the approach I plan to use to tackle some of the biomanufacturing decision-making challenges.The production and sale of monoclonal antibody (mAb) therapeutics are a hundred-billion-dollar and expanding biopharmaceutical industry to treat cancer, immune disorder, cardiovascular disease, and inflammatory diseases. However, mAbs are currently produced commercially in fed-batch cultures using media recipes and predetermined operating protocols based primarily on heuristics. Such a heuristic-based approach often leads to suboptimal production outcomes and requires costly experimentation to improve existing protocols. On-line control of mAb productivity and product quality metrics is not widely adopted for several reasons. For instance, measurement of critical quality attributes (CQAs) such as glycan distribution (the result of glycosylation) is available only after the process ends, making it impossible to adopt conventional feedback control. Cell growth, metabolism, antibody formation, and glycosylation are complex, multi-scale dynamics that are difficult to model. While currently available models can predict mAb synthesis or glycosylation dynamics accurately under specific process configurations, adapting them to new processes remains challenging.It is tempting to contemplate adopting a purely data-driven approach; however, the unique challenges associated with modeling biopharmaceutical processes make such an approach less likely to succeed. On the one hand, the biopharmaceutical industry is not data-rich because it is expensive to generate data, and existing data are usually sparse. As a result, models trained using these data may not extrapolate well outside the training data. On the other hand, domain knowledge and mechanistic understanding of biochemical processes can compensate for the lack of data. These two considerations argue for a hybrid approach to the development of a model that is suitable for predicting and for model-based control of CQAs. Meanwhile, PSE techniques such as Kalman filtering and AI techniques such as artificial neural networks can be used to estimate CQAs based on other measurements and signals available on-line. The development of high-fidelity, hybrid, adaptable models and state estimation techniques allows us to apply existing PSE tools to design more efficient biomanufacturing processes and to control them.",25,9.0
"Research Interests Research AbstractGiven the numerous advantages including improved process efficiency and product quality, continuous manufacturing (CM) has been an active area of research in the pharmaceutical industry. This research focuses on various experimental and modeling strategies, aimed towards process understanding and development of a virtual predictive framework of the overall process. One of the major bottlenecks for successful development of such framework and subsequent commercialization of CM technology is the lack of fundamental understanding of dynamic complexities arising during powder flow. To address this bottleneck, prominent challenges involve detailed understanding and systematic integration of powder dynamics in process operation and system analyses. This is paramount for development of a predictive framework with real-time capability.Mechanistic, first-principle based particle-scale simulations like Discrete element modeling (DEM) are increasingly employed to explore and extract particle-level information of powder flow and particle dynamics. Such tools are used to investigate powder mechanics during various process conditions and develop predictive models. During the development of such models, there is a balance that needs to be attained between the computational complexity and the model accuracy. Particle-scale simulations, when used for model development, can significantly improve model accuracy, however, at the cost of computational complexity. The computational requirement further increases when integrated with other unit operations in the process flowsheet. Alternatively, computationally cheap models might fail to capture detailed powder dynamics. Thus, there is a need to develop computationally efficient models using mechanistic particle-scale simulations, to be used for real-time prediction within integrated process flowsheets. To further improve the accuracy of these flowsheets, there is a need to develop and integrate real-time material tracking to explicitly track materials along time for lot-to-lot delineation. The process flowsheets, thus developed, can truly be dynamic in nature, while incorporating complexities of powder flow.Following this ideology, we propose to develop computationally-efficient models of unit operations in combination with DEM, using reduced-order and approximation strategies. Unit operation models, thus developed, are integrated together, within process flowsheets. Dynamic residence time distribution models are also proposed to enable real-time material tracking. Using the improved flowsheet models, we further plan to enhance system analyses tools like sensitivity, feasibility and flexibility analysis to include process dynamics while accounting for uncertainty, due to the variable nature and material properties of pharmaceutical powders. The proposed work would, thus, play a crucial role towards the development of a predictive framework for manufacturing lines, while capturing detailed process and powder dynamics.",25,10.0
"Research Interests:  formulation of colloidal dispersions, crystallization of nanoparticles, characterization of defects.My PhD research focuses on quantifying the relationship between crystal quality of self-assembled colloidal crystal and their optical properties, especially structural color intensity. Structural color is of interest in artificial materials because it arises from the periodicity of crystal structures, and it has better environmental stability than dyes. I fabricate colloidal crystals with structural color by controlling crystallization of colloidal particles via evaporative self-assembly. To study the connection between crystal film quality and structural color intensity, I investigate the crystal reflectivity as a function of film thickness, defect density, and impurity concentration using a combined experimental and computational approach. In the experiments, I formulated a series of colloidal dispersions of submicron-sized polystyrene spheres. By controlling the initial volume fraction of the colloidal dispersion, I varied the final thickness of crystal films after solvent evaporation and measured the thickness profiles using a profilometer. By introducing differently sized particles that function as impurities, I manipulated the defective microstructures of colloidal crystals and characterized different kinds of defects by scanning electron microscopy (SEM). I further analyzed microscope images with MATLAB to quantify crystal quality. To evaluate reflective structural color, I measured the reflection spectra of crystal films using a UV-Vis spectrophotometer and analyzed the reflection peak intensity. In the simulation, I collaborated with a colleague to model crystal structures with different quality via molecular dynamics. Then I simulated the reflection spectra of modeled crystals using the finite-difference time-domain (FDTD) method. The results show that film thickness profoundly affects structural color intensity. The peak reflection of the structural color increases approximately linearly with the film thickness, ultimately reaching reflection saturation. Self-assembled colloidal crystals cannot reach 100% reflection because they contain a variety of defects. Reduction in structural color peak reflection scales with increased defect density and impurity concentration; vacancies have a particularly detrimental effect. These quantitative relationships I have developed serves as a specific platform to guide the design of optical materials and support defect engineering in colloidal crystals.",25,11.0
"Research Interests: I am broadly interested in numerous areas of industrial R&D, which includes:Cell BiologyCell Culture DevelopmentPreclinical R&DBiotechnology R&DAgricultural R&D Related to Animals, Especially CowsR&D related to Skincare, Makeup, and BeautyBioinformatics/Genomics/Gene TherapyHistologyOncologyVaccine Bioprocess R&D­AntibodiesDevelopabilityPharmacometricsReports and Regulatory DocumentationBasic and/or Translational ResearchUpstream and Downstream R&DResearch Abstract:The onset of pregnancy in mammals begets dramatic changes in the mammary gland to prepare for breastmilk production. Tight junctions, intercellular junctional structures that regulate paracellular transport, are crucial for regulating pregnancy-induced morphogenesis in the mammary gland. Strong tight junctions with a high “barrier function” prevent milk from leaking out of the lactating gland into the surrounding interstitium, and weak tight junctions with a reduced barrier function coincide with involution (shrinkage) of the mammary gland during weaning. Although it is known that hydrostatic pressure exerted by accumulated milk helps to weaken tight junctions during involution, the exact mechanism is unclear. To identify how pressure regulates tight junctions, we custom-built a novel cell culture system to expose mammary epithelial cells to different pressures, up to 1.5 kPa, on their apical (luminal) sides. After exposure to pressure for 6 and 24 h, we observed significant decreases in transepithelial electrical resistance (TEER), a measure of barrier function, for pressures greater than 1 kPa (Figure). Additionally, we found that exposure to pressure decreases the expression of E-Cadherin at intercellular boundaries. Lastly, we found that exposure to pressure has no apparent effect on either cell proliferation or death. Ultimately, these findings may have implications for better understanding mechanisms underlying complications related to breastfeeding, such as mastitis.",25,12.0
"In the search for cheaper and energetically favorable materials for sustainable applications, there is a desire to catalyze reactions at room temperature and pressure with high active surface area for a number of chemistries. Designing new catalysts in turn improve photovoltaics, chemicals, enzymes, polymers, oil and gas, and automobile industries. MXenes, a new class of 2D materials similar to graphene, are tunable through physical and chemical modifications to their surfaces and have been considered as catalysts for chemical reactions for a decade now.1,2 For the past decade, learning methods have been used to predict and infer chemical and physical properties through analyzing datasets and building atomistic potentials.3In this poster, I will discuss approaches to performing high throughput computational screening on 2D materials to generate catalytic data based on these chemical and physical alterations to their structure. Computational based discovery requires the use of combined efforts of generating data and applying advanced techniques to understand and make data useful. Previous studies demonstrate our ability to compute data using density functional theory on MXenes for a number of chemical reactions crucial to reaching a clean energy economy.4–6 With this newly generated data, I perform data analyses of both computed and tabulated features through (a) examining correlations in the features and relationships to our target properties and descriptors of chemical reactions, based on changes to the material’s stoichiometry and geometry; and (b) using data science pipelines to generate simple models for structure-property relationships and infer features that point to how our chemistries change due to materials tuning. The methods involved training machine learning models ranging from linear regression and lasso method to neural networks and decision trees through hyperparameter tuning, validation tests, and feature selection through singular value decompositions.This work provides not only invaluable data to the 2D materials and catalysis communities but crucial analyses and fundamental understanding into how we can design materials and substances with high chemical activity through altering their electronic structure. The study also extends as a guideline to the scientific community on how to treat structured data in an appropriate manner to extract crucial insights needed to design new systems and improve upon existing ones. As a fifth year PhD candidate, the goal is to share how analytical, technical, and professional experience as a researcher, healthcare consultant, masters and undergraduate student mentor, and involved graduate student form the perfect candidate for business development and data related roles in industry.Research Interests:  materials design, catalysis, data science, electrochemistry, density functional theoryTeaching Interests: active learning, flipped classrooms, electronic note-taking, physical chemistry, reactor design, kineticsReferencesGogotsi, Y. & Barsoum, M. W. Two-Dimensional Transition Metal Carbides. ACS Nano 6, 1322–1331 (2012).Seh, Z. W. et al. Two-Dimensional Molybdenum Carbide (MXene) as an Efficient Electrocatalyst for Hydrogen Evolution. ACS Energy Lett. 1, 589–594 (2016).Kitchin, J. R. Machine learning in catalysis. Nat. Catal. 1, 230–232 (2018).Handoko, A. D. et al. Tuning the Basal Plane Functionalization of Two-Dimensional Metal Carbides (MXenes) To Control Hydrogen Evolution Activity. ACS Appl. Energy Mater. 1, 173–180 (2018).Johnson, L. R. et al. MXene Materials for the Electrochemical Nitrogen Reduction-Functionalized or Not? ACS Catal. 10, 253–264 (2020).Jin, D. et al. Computational Screening of 2D Ordered Double Transition-Metal Carbides (MXenes) as Electrocatalysts for Hydrogen Evolution Reaction. J. Phys. Chem. C 124, 10584–10592 (2020).",25,13.0
"Research Interests A significant challenge in chemical design is the large number of parameters that can be manipulated. Identifying an optimal set of parameters empirically by trial-and-error approaches is cost-prohibitive, inspiring the development of computational approaches, such as molecular simulations, to help garner insight into how these parameters relate to system properties. My research is focused on combining classical molecular dynamics (MD) simulations and machine learning techniques to systematically tune chemical properties for two relevant application areas. In both applications, I develop accurate predictive models that correlate simulation-derived descriptors to experimental data measured by collaborators, enabling the high-throughput screening of parameters using computationally efficient methods.Application 1 is motivated by the ability of gold nanoparticles (GNP) to target biomolecules, such as receptors on cancer cells, by attaching target-specific molecules (i.e. ligands) on the surface of the particle. Due to the vast number of tuning parameters of the GNP (e.g. core size, shape, and ligand selection), it is unfeasible to experimentally screen GNPs. Therefore, I developed a generalized workflow for developing GNP systems using MD simulations for any arbitrary gold core shape and size, and ligand selection. I used this workflow in conjunction with unsupervised machine learning clustering algorithms to characterize GNP surface properties. These properties dictate GNP adsorption onto lipid bilayers, which I further interrogate using enhanced sampling techniques (e.g. umbrella sampling). These tools could be used to systematically narrow down optimal GNP parameters for drug delivery and cell therapy applications.Application 2 focuses on converting biomass (renewable, organic material from plants) into transportation fuels or commodity chemicals. Biomass conversion is performed through acid-catalyzed reactions in aqueous solutions that are hindered by low reactivity. One way to improve reactivity is to modify the solvent composition by mixing water with organic cosolvents. Thus, I developed MD simulations to understand and predict the effects of adding an organic solvent on biomass conversion reactivity. I coupled MD simulations with convolutional neural networks, a machine learning model used to classify images, and generalized the reaction rate predictions across 84 reactant-solvent combinations; enabling fast solvent screening for enhanced reaction rates.My research interests include developing new materials or chemical processes using computational tools, such as molecular simulations (e.g. molecular docking, multiscale modeling) in conjunction with machine learning techniques (e.g. clustering, neural networks). By joining industry, I aim to broaden my ability to generate efficient and accurate computational models that could accelerate the discovery of new, beneficial products.",25,14.0
"Materials simulation for manufacturingResearch Interests: Manufacturing in the pharmaceutical industry can benefit from particle-based simulations. The efficiency of the unit operations handling powders and pills depend on how those powders and pills interact and flow. Simulations often model such materials as hard spheres. Adhesion, cohesion and (sliding, rolling and twisting) friction are present however, and impact their rheology. Particle-based simulations connect those interactions with emergent behavior. On a smaller scale, molecular simulations have been effective in drug discovery, yet advances in methods and computational power make simulations well poised to address formulation challenges in the pharmaceutical industry. Simulations of nanoparticle, amphiphile and small drug undergoing directed- and self-assembly give microscopic insight and design rules for formulations. I am interested in research that adds value by increasing efficiency in energy, waste and development time. I am eager to identify and support those opportunities as customer needs and market forces changes company value areas. I am qualified for such research because of my background in soft-matter and particle-based simulations.",25,15.0
"Single-cell genomics is a rapidly advancing field and is generating new knowledge of complex biological systems, ranging from microbial ecosystems diversity, through whole-organism clone tracing, to understanding early mammalian embryogenesis. During my graduate school, I have contributed to the field in two ways: developing a mathematical model to reconstruct cellular lineage in pre-implantation mouse embryos and creating an efficient and cost-effective mRNA enrichment method for prokaryotic cells.Cellular lineage reconstruction: Lineage reconstruction is central to understanding tissue development and maintenance. While various tools to infer cellular relationships have been established, these methods typically involve genetic modification and have a clonal resolution. I created scPECLR, a probabilistic algorithm to endogenously infer lineages at a single cell-division resolution using 5-hydroxymethylcytosine (5hmC). When applied to 8-cell mouse embryos, scPECLR predicts the full lineage trees with greater than 95% accuracy. Furthermore, I developed a protocol to detect both 5hmC and genomic DNA from the same single cell. Information from genomic DNA, in combination with scPECLR, could allow us to identify cellular lineages more accurately and expand the reconstruction to even larger trees, where the number of possible tree topologies increase to more than 1026. In addition, I showed that scPECLR can be used to map chromosome strand segregation patterns during cell division, thereby providing a strategy to test the controversial “immortal strand” hypothesis in stem cell biology.Low-input bacterial mRNA sequencing: RNA sequencing is a powerful approach to quantify the genome-wide distribution of mRNA molecules in a population to gain understanding of cellular functions and phenotypes. However, compared to mammalian cells, mRNA sequencing of bacterial samples is more challenging due to their 100-fold lower RNA quantities and the absence of a poly-A tail that typically enables an enrichment of mRNA. To overcome these limitations, I conceived an economical and effective mRNA enrichment method called EMBR-seq. The method results in greater than 80% of the sequenced E. coli RNA molecules deriving from mRNA, which originally contributes to lower than 5% of total RNA in a cell. Moreover, EMBR-seq successfully quantifies mRNA from 20 picogram total RNA, a level 500-fold lower than required in existing commercial kits, without introducing technical biases and at a fraction of the cost. Lastly, due to its simplicity and efficiency, EMBR-seq could potentially be extended to a single-cell resolution to advance developments in bacterial mRNA sequencing and improve understanding of microbial systems.Research Interests:  bioengineering, molecular and computational biology, and bioinformatics",25,16.0
"Research Interests The morphologies metal electrodeposits adopt during the earliest stages of electrodeposition are known to play a critical role in the recharge of electrochemical cells that use metals as anodes. Lithium (Li) metal is among the most promising anode materials for high-energy and light-weight rechargeable batteries due to its extremely high theoretical specific capacity (3860 mAh g−1), the lowest negative electrochemical potential (−3.040 V versus standard hydrogen electrode), and low density (0.534 g cm−3). The propensity of Lithium Metal anode to form low-density mossy morphologies, loosely termed dendrites during the charging phase has impeded its practical application in Metal Batteries. The dendrites proliferate in the electrode space to short-circuit the battery producing thermal runaway, meanwhile lowering the coulombic efficiency (CE) and reversibility of the Lithium anode in electrolyte media. Studies dealing with understanding the nucleation and growth dynamics of reactive metals are cardinal to understand the origin of instabilities at early stages of electrodeposition that may subsequently lead to dendrites at a later stage. Here we report results from a combined theoretical and experimental study of the early-stage nucleation and growth of electrodeposited lithium at liquid-solid interfaces. The spatial characteristics of Lithium electrodeposits are studied via Scanning Electron Microscopy in tandem with Image analysis. Comparisons of Li nucleation and growth in multiple electrolytes provide a comprehensive picture of the initial nucleation and growth dynamics. We report that ion diffusion in the bulk electrolyte and through the Solid Electrolyte Interphase (SEI) formed spontaneously on the metal play equally important roles on Li nucleation and growth. We show further that the underlying physics dictating bulk and surface diffusion are similar across a range of electrolyte chemistries and measurement conditions and that fluorinated electrolytes produce a distinct flattening of Li electrodeposits at low rates. These observations are rationalized using X-ray Photoelectron Spectroscopy (XPS), Electrochemical Impedance Spectroscopy (EIS), and contact angle goniometry to probe the interfacial chemistry and dynamics. Our results show that high interfacial energy and high surface ion diffusivity are necessary for uniform Li plating.",25,17.0
"Advances in pharmaceutical discovery and increasing global healthcare demands call for innovative technologies that can produce material at high throughput while retaining quality obtained at the bench scale. Nanomaterials are being increasingly used in pharmaceutical production. The properties of nanomaterials are highly dependent on the mixing dynamics of their synthesis process. Macromixing in conventional batch syntheses increases the overall mixing time, leading to variability within batches and a wide particle size distribution. These problems can be circumvented by using microreactors that provide a small mixing time because of their associated micromixing and mesomixing. This work proposes a continuous, scalable jet-mixing reactor that is used for the synthesis of several metal and metal-oxide nanomaterials. Initially, silver nanoparticles (Ag NPs) are synthesized at ambient conditions as proof-of-concept. It is observed that Ag NPs synthesized using jet-mixing have a particle size distribution narrower by 4.5% and a 20% increase in shelf life as compared to their batch-synthesized counterpart. The jet-mixing reactor also demonstrates material economy by requiring a capping agent concentration that is four times lower than that required in batch. Next, the jet-mixing process is developed to incorporate inert conditions to synthesize pure-phase copper nanomaterials (Cu NPs) that tend to oxidize readily under ambient conditions. Cu NPs with 88% phase purity are synthesized using jet-mixing. Lastly, the jet-mixing reactor is used for the inert synthesis of multi-component core@shell Pd@TiO2 nanomaterials. The materials obtained from batch and jet-mixing synthesis are tested as catalysts for the hydrodeoxygenation (HDO) of furfuryl alcohol. It is observed that the selectivity for the HDO product, 2-methyl furan, obtained using the jet-mixing-synthesized material is comparable to that obtained by batch synthesis. Jet-mixing is capable of a productivity of 50 g/cc min of the catalyst as compared to batch that yields 0.5 g per run, demonstrating the scalability of the synthesis. Overall, these studies demonstrate that jet-mixing is a scalable, flexible tool for the synthesis of several types of nanomaterials and is readily extendable to a variety of synthesis conditions. Currently, studies are underway to characterize the mixing dynamics of the reactor to enable process scale-up.Research Interests:  Pharmaceutical scale-up, batch-to-continuous processing, process development, product development, nanomaterial synthesis",25,18.0
"The overall goal of my graduate work is to use layer-by-layer film assembly to answer fundamental questions about growth factor-mediated healing of craniomaxillofacial bone defects. The aim is to determine optimal release kinetics for delivery of growth factors and optimal combination of angiogenic and osteogenic growth factors from a cell-free, synthetic, biodegradable scaffold. Current work has focused on engineering layer-by-layer films that release clinically-relevant osteogenic growth factor, BMP-2, in low doses over periods of time ranging from days to weeks. In this work, we developed and optimized diffusional barrier tools to modulate release of the protein from a degradable polymer film. We completed preliminary in vivo pilot studies which indicate that release kinetics may influence bone growth in a critically sized rat calvarial defect. We plan to perform a follow-up large scale study in the near future including microCT tracking of bone mineral density and bone volume, histological analysis to examine maturity and quality of new bone, and mechanical testing to determine osseointegration of new bone. We are also currently working to translate these systems to 3D printed macro porous scaffolds for rabbit mandibular defect studies. Our future plans include translation of the layer-by-layer film diffusional barrier tools to dual-growth factor systems and subsequent in vivo analysis to determine the effect of angiogenic growth factor type and relative delivery windows of angiogenic and osteogenic growth factors on craniomaxillofacial bone healing.Research Interests I am very interested in the intersection of engineering and medicine, with a strong emphasis on applications-based research. Specifically, I have really enjoyed exploring how chemical engineering fundamentals can play a role in drug delivery. I am open to new experiences and would be interested in a research & development role as well as a more management-focused track.Relevant skillsLayer-by-layer film assembly, in vivo surgical methods, IVIS imaging, microCT imaging, histology, ELISA and other protein assays, GPC, NMR, HPLC, tissue culture, step polymerization",25,19.0
"Screw feeders are the critical first unit operation in continuous manufacturing of drug product (CMDP) processes, influencing the mass flow rate of pharmaceutical powders downstream. Thus, industrial leaders are keen on accurately modeling feeders. Existing flowsheet models simulate the average mass flow rate [1]–[3], neglecting the stochastic behavior of the mass flow. Custom Discrete Element Method models realistically simulate particles’ behavior but require prohibitively long computation times to simulate minutes of operation [4]. To better design processes and controllers, a quick-to-solve flowsheet model that can simulate the stochastic nature of real screw feeders is necessary. This is precisely the gap addressed by this work.This work describes the novel characterization and modeling of the stochastic nature of a screw feeder’s mass flow rate using statistical time series analysis and a deterministic flowsheet model. First, experimental data was used to estimate the parameters of a hybrid mechanistic-empirical screw feeder model, based on Bascone et al. 2020 [3]. Next, the stochastic residual of the mass flow was isolated by subtracting the flowsheet model’s deterministic mass flow from the feeder-reported mass flow. Then, each experiment's stochastic residual was fit to an autoregressive moving average model (ARMA) [5], characterizing the mass flow variation. Finally, a predictive model mapping powder properties and operating conditions to ARMA model parameters was developed. This predictive model was integrated with our deterministic feeder model, yielding a novel mechanistic-empirical-stochastic flowsheet model that simulates realistic, high-variance mass flows and is suitable for the development of CMDP processes and controllers.Research Interests:  Dynamic Systems, Multivariate Analysis, Constrained Regression, Optimization, and Technical Software Development[1] Y. Yu, “Theoretical modelling and experimental investigation of the performance of screw feeders,” PhD thesis, 1997.[2] F. Boukouvala, V. Niotis, R. Ramachandran, F. J. Muzzio, and M. G. Ierapetritou, “An integrated approach for dynamic flowsheet modeling and sensitivity analysis of a continuous tablet manufacturing process,” Computers & Chemical Engineering, vol. 42, pp. 30–47, 2012.[3] D. Bascone, F. Galvanin, N. Shah, and S. Garcìa-Muñoz, “A hybrid mechanistic-empirical approach to the modelling of twin screw feeders for continuous tablet manufacturing,” Industrial & Engineering Chemistry Research, 2020.[4] P. Toson and J. G. Khinast, “Particle-level residence time data in a twin-screw feeder,” Data in brief, vol. 27, p. 104672, 2019.[5] G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: Forecasting and control. John Wiley & Sons, 2015.",25,20.0
"Research Interests My research interests include the following: Micro-capsules and microfluidics, modeling and simulation, pharmaceutical drug process development and manufacturing, drug micro-encapsulation and drug delivery, data science and automation.Abstract:In recent years, various studies have investigated the flow dynamics of a single capsule in straight microfluidic channels such as cylindrical, square and rectangular channels, constrictions and expansions. These studies have provided a better understanding for capsules utilized in numerous engineering and biomedical applications including fabrication of single and multi-compartment micro-capsules with desirable properties, targeted drug delivery and of course its similarity to blood flow in vascular capillaries.In the present work, our interest is concentrated on fabrication of multi-compartment micro-capsules based on interactions of single capsules. In our computational investigation, we consider spherical capsules made of a strain-hardening membrane with comparable shearing and area-dilation resistance, while we study a wide range of capsule sizes and flow rates. Under these conditions, we have identified several merging formation patterns which depend on the single capsule size. The conditions for capsules separation and the effects of lubrication physics will also be presented. Our work provides useful physical insight on the possible fabrication of multi-compartment micro-capsules via further cross-linking for drug delivery of multiple drugs as well as on erythrocytes aggregation in the micro-circulation.",25,21.0
,25,22.0
"Research Interests Hydrophobicity is an important interfacial property that determines the association of nonpolar materials in water and is highly correlated with biologically relevant properties, including immune response, protein-protein interactions, and cell uptake. Accurately predicting hydrophobicity is vital to the design of new materials with favorable properties for applications in drug delivery, biosensing, and antifouling. My research interests lie in developing and applying molecular simulations techniques and data science methods to understand how surface properties of biomaterials influence interfacial phenomena and intermolecular interactions. Then applying these newfound understandings to developing predictive models based on physical insights to rapidly screen drug and gene agents to identify candidates with desirable physicochemical properties. These models would be valuable to improving the efficiency and efficacy of new drug therapeutic design and discovery.Research ExperienceMy research focuses on modeling the interface between water and self-assembled monolayer (SAM) protected gold nanoparticles using molecular dynamics (MD) simulations to understand how physical and chemical properties influence the hydrophobicity of the SAM surfaces. My research project can be divided into three fundamental questions: (1) How do physical surface properties influence hydrophobicity? (2) How do chemical surface properties influence hydrophobicity? and (3) How do physical and chemical surface properties cooperatively influence hydrophobicity? The goal of answering these questions is to better understand how surface properties alter hydrophobicity; thus, providing a design framework to engineer novel biomolecules with fine-tuned hydrophobicity. To achieve this goal, I have developed a simulation methodology that applies umbrella sampling (an enhanced sampling MD technique) to measure the hydrophobic force exerted between two SAM surfaces. I collaborated closely with experimentalists to ensure the model accurately predicts hydrophobic forces. To screen a larger range of SAM chemistries and compositions, I applied a more efficient enhanced sampling technique (Indirect Umbrella Sampling) to predict the hydrophobicity. This method required approximately ten times less simulation time than classical umbrella sampling. Next, I applied traditional analysis techniques, such as time-averaged measurements, as well as more data-centric methods, including principal component analysis and multivariate linear regression, to unbiased MD simulation data to interrogate how interfacial water structure, physical and chemical surface properties, and hydrophobicity are related. Finally, in collaboration with data-science experts, we leveraged the predictive power of machine learning algorithms to create a model that predicts hydrophobicity 400 times faster than the most efficient enhanced sampling MD simulation methods.",25,23.0
"Research Interests My research interests encompass from understanding crystallization fundamentals to material design. In this regard, during my Ph.D., I have focused on understanding underlying fundamental mechanisms in zeolite crystallization, which is critical in developing optimal heterogeneous catalysts for different industrially relevant reactions, e.g., methanol to hydrocarbon (MTH) reaction, hydrocarbon cracking, Friedel craft alkylation, etc.Introduction: ZeolitesZeolites are crystalline microporous aluminosilicates consisting of tetrahedra of SiO4 and AlO4- linked by O atoms, producing three-dimensional networks of channels and cavities of molecular dimensions with ordered geometries and connectivity (Fig. A). Due to their unique properties, such as exceptional hydrothermal stability and tunable acidity, zeolites have extensive applications in petrochemical refining, production of fine chemicals, biomass conversion, separations, emissions control, and drug delivery. More than 245 zeolite frameworks have been synthetically realized; however, only ca. 20 of these structures are used commercially owing in part to the complexity of zeolite synthesis and the economic costs required to expand the list of viable framework types. Thus, it is essential to understand underlying mechanisms in zeolite crystallization.Zeolite crystal growth occurs by a combination of two general mechanisms (scheme in Fig. 1): a classical pathway involving monomer addition (Fig. B, 1a), and nonclassical pathways (Fig. B, 1b – 3a) that occur by the addition of species (or precursors) more complex than a monomer. For zeolites, growth solutions contain a diverse mix of precursors: oligomers (Fig. B, 1c), amorphous particles (Fig. B, 2a), and small crystallites (Fig. B, 3a). [1] There are many unanswered fundamental questions regarding the role of precursors in zeolite crystallization. I have focused on studying different paradigms of zeolite design which I will discuss briefly below:Understanding the role and prevalence of defects in zeolite crystallization by imaging through in-situ atomic force microscopy (AFM) of 2-dimensional surface growth Atomic force microscopy (AFM) is a powerful technique to study surface dynamics. Usually, zeolite synthesis occurs at high temperature (60-200°C) and high pH conditions, which render in-situ observations challenging. Many groups have utilized ex-situ AFM to study zeolite crystal growth, but without real-time measurements, the actual mechanism remains elusive. In this regard, our research group has pioneered the technique of in-situ atomic force microscopy (AFM), which enables the visualization of crystal growth with unparalleled spatiotemporal resolution. I have utilized in-situ AFM to investigate the crystallization of zeolite A which has commercial applications in the processes involving adsorption and ion exchange (Fig. A) to understand the synthesis conditions and final property relationships for the predictive control on the properties of zeolites.I identified multiple pathways of classical growth where the most common mode of layer generation stems from large (type I) protrusions (Fig. C), which are presumed to be defects located at the apex of hillocks. Secondary growth of these surfaces reveals the continued generation of new layers from the edge of defects, leading to the sustained growth of hillocks. Direct observation of type II defects (Fig. C) was enabled by the identification of conditions leading to 2D layered growth.Parametric analysis of growth conditions also reveals that the selection of the silicon source, most notably colloidal silica, is paramount in the generation of type II features, which are deemed to be the remnants of undissolved amorphous silicates. The ubiquitous observation of defects (types I and II) indicate their prevalent role in zeolite A crystallization. [2] Moreover, the presence of type II defects has practical implications as they can reduce acid site concentration and/or restrict diffusion in nanopores, which would negatively affect their performance in applications of catalysis and adsorption (in stark contrast to the rational design of nanocomposites where particle occlusion often enhances performance, e.g., metal@zeolite bifunctional catalysts). Previously undetected defects in zeolite A are associated with media containing undissolved silica, which is common in zeolite synthesis, suggesting the occlusion of amorphous silica is a phenomenon that likely extends to other framework types prepared by similar sol-gel methods.Seed-Assisted zeolite synthesis: The impact of seeding conditions and interzeolite transformations on crystal structure and morphology Seed-assisted zeolite synthesis is a simple and economically viable approach to alter crystal habit. Our findings reveal that the nature of the seed has a significant impact on the final product; and that only a small quantify of seeds is needed to dramatically reduce crystal size without any unintended modification of framework composition (Si/Al ratio) (Fig. D). [3] A phenomenon that is closely tied to seeded growth is that of interzeolite transformations where the nucleation of a new crystal occurs after the formation of an initial, more thermodynamically metastable structure. Interzeolite transformations are analogous to crystal polymorphism but in this case, two crystals involved in the transition differ in structure as well as in chemical composition. This process is akin to the Ostwald rule of stages where two or more distinct transitions can occur. The main objective of this study is to establish an improved understanding of how seeding influences zeolite crystallization and elucidate the factors driving interzeolite transformations.In this work, I examined the relationships between parent (product) and daughter (seed) crystals in seeded growth assays using a variety of seed properties and growth solution compositions. In many instances, we observe that seeded growth results in interzeolite transformations where the timescales are dependent upon the conditions selected, and the rationale for the observed trajectory is not well understood or easily predictable. In general, our observations indicate that the growth solution plays a vital role in controlling the kinetics of interzeolite transformations. The thermodynamic driving force for these transformations cannot be easily described by a single variable, such as the molar volume of zeolite structure as prescribed by previous hypotheses. The dissolution of seeds presumably leads to species that initially facilitate the nucleation of an identical structure; however, over the course of synthesis, the composition of the growth solution changes, thereby altering the chemical potential (i.e., driving force), leading to the nucleation of a new phase. This highlights the importance of kinetics in seeded zeolite syntheses wherein the previous hypotheses must consider additional factors that include (but are not limited to) synthesis time, temperature, and the compositions of seeds and the growth medium. [3]ReferencesOlafson, K. N., Li, R., Alamani, B. G., & Rimer, J. D. (2016). Engineering crystal modifiers: bridging classical and nonclassical crystallization. Chemistry of Materials, 28(23), 8453-8465.Choudhary, M. K., Jain, R., & Rimer, J. D. (Submitted). In situ imaging of 2-dimensional surface growth reveals the role and prevalence of defects in zeolite crystallization.Jain, R., & Rimer, J. D. (2020). Seed-Assisted zeolite synthesis: The impact of seeding conditions and interzeolite transformations on crystal structure and morphology. Microporous and Mesoporous Materials, 110174.",25,24.0
"Research Interests In this study, we successfully built a model for mimicking intestinal mucosal system for testing drug delivery. For this project, Intestinal stem cells was used, and Microfold cells were first differentiated by using 200 ng/ml RANKL,later on, this result were confirmed by different methods, which including qPCR, immuno-staining and western blot. Gold Nanocage was used for modeling drug transportation. In the future study, sigma 1 protein will be linked to gold Nanocage, making the Nanocage an idealistic nanomaterials for transporting antigen through M cells to tiger immune responses. ",25,25.0
"Superhydrophobic coatings have many applications. Examples include self-cleaning and anti-ice surfaces, anti-fouling marine coatings, and anti-bacterial coatings for medical devices. To develop a superhydrophobic coating, low surface energy and high surface roughness in micro/nano scale are required. Fluoropolymers can decrease the surface energy of a coating significantly by orienting themselves in the coating-air interface. To induce surface roughness, different strategies including solvent evaporation, non-solvent induced phase separation, vapor induced phase separation, solvent/co-solvent induced phase separation, and polymerization-induced phase separation are applied. Incorporating nanoparticles into coatings is another strategy for increasing the surface roughness of coatings. Higher roughness means higher pockets of air between a liquid droplet and a solid surface, leading to a higher contact angle.Silica is a nanoparticle widely used in high-performance coatings. Silica in a coating provides the coating with high thermal resistance, improved mechanical properties, resistance to corrosive solvents and so on. In case of covalent bonding between the silica surface and the polymeric matrix, these improvements in properties are much more significant .Acrylate monomers with a long alkyl chain are highly hydrophobic and undergo free-radical polymerization. Such polyacrylates have already been used for the fabrication of implants thanks to their high anti-bacterial activity and biocompatibility. In this paper, we present results from in-situ polymerization of a mixture of silica and a hydrophobic acrylate monomer. Different grades of silica with different surface chemistry were used for the polymerization. After the polymerization, the nanocomposites were coated on glass substrates via spin coating. The water contact angles of the coatings were measured by the goniometry technique. Other characterization techniques including TGA, EDS, and SEM are also applied to fully characterize the products.Research Interests: Polymerization sciencePolymer nanocompositesMathematical modeling of polymerization reactorsPolymer coatings and thin filmsMXene synthesis and surface modificationPolymer melt processing",25,26.0
"Research Interests Synucleinopathies are a group of neurodegenerative disorders such as Parkinson’s disease (PD), multiple system atrophy (MSA) and dementia with Lewy-bodies (DLB) that affects millions of people worldwide. They are characterized by abnormal aggregation or fibrillation of a-synuclein protein inside the nerve fibers or glial cells. These disorders are clinically characterized by memory and cognitive impairment, decline in motor or autonomic functions. α-synuclein is an intrinsically disordered protein which under pathological conditions undergoes structured transformation to oligomers containing β-pleated sheets that assists in its fibrillation process. The central amino acids from 35 to 97 (63 residues) play a major role in fibril formation and take up a unique Greek-key topology. The characterization of conformations accessible to monomeric α-synuclein is crucial in understanding the pathway to fibrillation. In this study, we use replica-exchange molecular dynamics methods to determine the folding landscape of α-synuclein fragment (35 – 97). We are able to obtain microscopic level information and thus, gain better understanding of protein folding by using an all-atom peptide model. The results provided also include the effect of temperature on these properties and the secondary structure analysis.",25,27.0
"Research Interests: EnergyBatteriesData ScienceWithin the framework of green chemistry, Deep Eutectic Solvents (DES) have been identified as promising candidates for use in many applications, including battery electrolytes. DES are characterized by two or three materials that associate with each other through hydrogen bond interactions, resulting in a eutectic mixture whose freezing point is below that of the individual materials. This design space is overwhelmingly large and poses a challenge for screening a vast and diverse set of materials. Here we present a strategic approach consisting of high throughput experimentation (HTE) coupled with data science driven analysis to identify exceptional DES candidates based on key physiochemical and electrochemical properties. Much of our HTE adopts methods that are already used frequently in the biotech and pharmaceutical industries, most notably performing parallel syntheses and analyses in 96-well-plate formats. DES samples are first synthesized using an open-sourced automated liquid handling robot. DES melting points are then determined by monitoring the melting process with an infrared camera and identifying the temperature at which the thermal conductivity of the samples changes abruptly. The solubility of battery redox-species is determined via UV-VIS well-spectrophotometers. Finally, the electrochemical stability window and cycling properties of DES electrolytes are measured in high-throughput by using screen-printed electrodes on 96-well plates adapted for use with a standard potentiostat. The ability to rapidly and efficiently collect data also creates a need for the development and use of automated processes for data analysis, which have been developed in an open-sourced format by our group. This approach to HTE also allows for the incorporation of data science techniques, such as feature extraction and machine learning, that further aid in probing a design space that is ultimately too large for experimental methods alone.",25,28.0
"Research Interests Capecitabine is an anticancer chemotherapy prodrug and is widely used in pharmaceutical industry to treat patients with metastatic colorectal cancer. Through two-step enzyme catalyzed degradation, capecitabine is converted to 5FU, which is an effective drug. Despite the longer half-life of the prodrug, current problems are high dose and toxicity. Many efforts had been made to increase the efficacy of the drug in terms of controlled and sustained release and thus, lower the dosage. Specifically, liposomal nanoparticle is an attractive approach to achieve those goals; however, the unique molecular structure of the prodrug makes it difficult to prepare stable nanoformulations with high drug loading and long-term stability.In this study, we modified the prodrug molecule by attaching a cetyl chloroformate to the 5- fluorouracil head group through a simple one-pot reaction. The new prodrug (5FU-PAL) has bigger hydrophobic domain and behaves as an amphiphilic molecule at the interface. We have optimized a liposomal formulation of 5FU-PAL for prolonged blood circulation and sustained release based on the understanding of molecular interactions between 5FU-PAL and lipids, which was achieved by using synchrotron X-ray reflectivity and grazing incidence X-ray diffraction (GIXD) integrated with a Langmuir trough. Monolayer packing of 5FU-PALwith saturated neutral lipid dipalmitoyl-phosphatidylcholine (DPPC), positively charged lipid 1,2-stearoyl-3-trimethylammonium-propane (DSTAP), negatively charged lipid 1,2-dipalmitoyl-sn-glycero-3-phospho-(1'-rac-glycerol) (DPPG), unsaturated neutral lipid 1,2-dioleoyl-sn-glycero-3-phosphocholine (DOPC) and unsaturated positively charged lipid 1,2-dioleoyl-3-trimethylammonium-propane (DOTAP) were investigated. It was found that 5FU-PAL interacts strongly with positively charged lipid, especially in the head group region where the electron density of positively charged lipid increased upon adding the prodrug while the electron density of neutral and negatively charged lipid decreased. Furthermore, adding the prodrug molecule caused the packing at the air-water interface to become tighter. In addition, 5FU-PAL at the air-water interface folded into multiple layers with highly ordered crystal structures. The studied could precisely quantify the maximum drug loading in the liposomal formulation with a stable structure.",25,29.0
"Systems engineering approaches for bioprocess development and biomanufacturing can be integrated in bioprocess development and biomanufacturing during the commercialization stage of biopharmaceuticals into the engineering marketplace. Lean tools can be used as engineering techniques that can be part of management of bioprocess development and biomanufacturing projects in order to reduce different types of waste. Online or Kanban (scheduling) system allows manufacturing managers and process development engineers to create a list of requests for troubleshooting, testing and on-site inspections. Failure mode and effects analysis can be applied to reduced hazards while allowing flexibility in the layout of materials and equipment for developing a pharmaceutical production process. Life cycle analysis, techno-economic analysis, risk identification and multi-criteria decision analysis are also tools that can be applied in scenarios when single-use disposables are compared with stainless steel tanks for pilot-scale process development or large-scale biomanufacturing. With regards to the developing the workforce for this industry, mentorship and leadership are also important towards the career advancement of chemical engineers in the biopharmaceutical product development and biomanufacturing businesses. After working in the biopharmaceutical industry as a process development engineer for almost five years, I realized that building a career as a young professional is a combination of education, work experience, industry certifications, and establishing a professional network. In my poster, online resources are presented for young professionals, those with less than ten years of industrial experience, that would be useful in exploring career paths, expanding their professional network, and building relationships as they move into various positions during their careers. These resources are also helpful for researchers involved in industrial psychology, management science and human resource development.Research Interests:  Systems Engineering, Management Science, Biopharmaceutical process development, Biomanufacturing",25,30.0
"Cadherin transmembrane proteins are responsible for cell adhesion in a number of tissues and therefore, modulate a variety of biological processes, such as tissue morphogenesis, cell motility, force transduction, and macromolecular transport across tissue boundaries. Furthermore, deficiencies in cadherin-mediated adhesion are correlated with several diseases, such as a number of skin diseases and cancers. Cell-cell adhesion is facilitated via adherens junctions, consisting of adhesive, trans-interactions, and lateral, cis-interactions, but the molecular role and interplay of these interactions is a topic of discussion. Here we used single-molecule microscopy to show the drastic, double cooperativity, of cis- and trans-interactions and how this can facilitate large junction formation using a biomimetic lipid bilayer cell adhesion model. Notably, the addition of cis-binding capability resulted in a 25-fold increase in trans-binding lifetimes between epithelial-cadherin extracellular domains. The nature of this cooperativity appears to be primarily due to allostery, as opposed to avidity. It has been thought the primary role of cis-interactions is to provide molecular ordering within junctions, increasing their size, but junctions still may form in the absence of cis-interactions. Furthermore, cis-interactions have been predicted to cause an elevated cadherin surface density in the adhesive zone between cells, potentially increasing adhesive binding strength though cluster avidity. We have directly observed strong cooperativity between cis- and trans-interactions, providing vital insight into the molecular mechanism of junction formation, and therefore, cadherin-mediated cell adhesion.Research InterestsBiomolecules at interfaces, statistical modeling, data science, single-molecule microscopy,",25,31.0
"Research Interests Throughout my PhD I have worked on the advancement of modeling and control for the continuous manufacturing of pharmaceuticals. I have been the primary contributor on three main projects and a secondary contributor on a few others.The first of these projects focused on the development of a virtual plant for the manufacturing of a small molecule active pharmaceutical ingredient. The plant was used to inform process design and process control for the manufacturing system.The second of these projects focused on the development of a model for the production of recombinant proteins in the host, Pichia pastoris. The model was validated with existing literature and used to understand long-term process stability and explored methods to analyze and increase this stability.The third of these projects involves on construction of a testbed for the continuous manufacturing of monoclonal antibodies. The system is heavily instrumented so that diverse datasets can be generated for process model and control strategy development. For this, I have worked on data integration, control strategy development, and process modeling for the system.Outside of these projects, I have a working knowledge of viral vector (specifically recombinant adeno-associated viruses) and vaccine manufacturing. I am looking for a job to leverage my experience in process modeling and control for the advancement of pharmaceutical manufacturing processes.",25,32.0
"Research InterestsIntermittent (“hopping”) surface diffusion of poly-L-lysine in a nanoslit was studied using single-molecule tracking microscopy. Three surface chemistries were employed to understand the interplay of long-range electrostatic attraction and short-range interactions: an amine-functionalized silica surface, an oligo(ethylene oxide) (OEG) modified surface, and an equally mixed surface. Diffusion increased rapidly with slit height until saturating for values <30 nm. While diffusion at a semi-infinite interface was significantly faster for OEG surfaces, the diffusion increased most rapidly with slit height for amine-functionalized surfaces, resulting in surface diffusion that was virtually independent of surface chemistry in gaps <15nm. Kinetic Monte Carlo simulations, using parameters obtained empirically from diffusion at a single interface, suggested that these trends were primarily due to strong H-bonding interactions between PLL and amine surface ligands, which led to increased rates of re-adsorption after hops and longer waiting periods between flights, and that long-range electrostatic attraction had a minor influence.",25,33.0
"Research Interests Although computational models have been instrumental in advancing the areas of renewable energy production and storage, traditional catalyst design primarily relies noticeably on human intuition. As such, the usual timeline for discovery, development and deployment of novel catalytic materials and chemical process is typically long-term and capital intensive. A promising solution to address this shortcoming is integration of machine learning (ML) and artificial intelligence (AI) methods, which are known to improve iteratively through experience and more data, with human intuition and creativity. I am fascinated by this data-driven design paradigm possible through the digitization of traditional R&D pipeline using AI/ML methods; enabling teams to undertake diagnostic, predictive, and prescriptive analytics which form an integral part of Industry 4.0. My Ph.D. research at Purdue University is focused on developing multi-scale catalyst models that capture the essential structure-functional properties of the real-world catalyst. These models include a combination of atomic-scale quantum mechanical simulations with subsequent reactor design and high throughput machine-learning based strategies to predict material properties. I am using these models to investigate the active site and reaction mechanism for water-gas shift and NOx decomposition in vehicular exhaust. The proposed theory models, as validated through our experimental collaborators, offer an improved molecular-level mechanistic understanding of the reaction. Besides this, I am involved in developing a grand-canonical genetic algorithm to generate catalyst models which allow an improved active-site description thereby reducing the theory-experiment disparity.Alongside developing theoretical models, I am investigating accelerated screening strategies for ranking catalyst candidates. These strategies are being developed for high-entropy alloys (HEA), which typically comprise of five or more elements. HEAs have attracted considerable interest in recent years due to their superior oxidation resistance relative to that of conventional alloys catalysts. I am using crystal-graph convolutional neural network (CGCNN) to model and rank catalyst stability with reduced computational expense compared to traditional electronic-structure optimization methods. Appropriate featurization of the chemical moieties to encode physical-chemical properties is a crucial step in developing a ML-Al model. Crystal graphs are a promising avenue to encode these structural and chemical underpinnings. In addition to generating a surrogate model, I am using the latent-space encodings generated through the CGCNN to discover lower dimensional manifolds that can be used for accelerated optimization of the target property.Besides my graduate research, I am currently interning at Dow Chemical Company (Lake Jackson, TX) in the Chemometric and AI department. I am part of a cross-disciplinary team comprised of process system engineers, applied statisticians, and organic chemists. My primary projects are 1) developing process models for anomaly detection to improve unplanned event identification and 2) building a GPU-accelerated cloud-based deep-learning framework to screen large database (on the scale 109) of organic molecules to optimize for cost, reactivity and stability. ",25,34.0
"Research Interests - My Ph.D. thesis research focuses on: (i) developing new tools for trapping and manipulating micro and nanoparticles in free solution using only fluid flow, and (ii) understanding the physics and emergent properties of vesicle suspensions using these control-based techniques. To this end, I have developed a millisecond time nonlinear model predictive control algorithm with real-time image processing (written in C++, LabVIEW and MATLAB) to control the center-of-mass and orientation of single and multiple anisotropic particle using only fluid flow. In tandem, I study the dynamics of lipid vesicles using fluidic trapping tools and advanced statistical tools. My research has direct implications to the processing of food active ingredient encapsulants, and personal care products such as shampoos and detergents. Overall, my Ph.D. research entails a distinct combination of numerical, experimental and theoretical/modeling skills involving fluid dynamics, process control techniques, and numerical methods that has provided me with the confidence to address a wide spectrum of challenges faced by modern society. Abstract Text- In this work, we study the shape dynamics of vesicles in precisely defined steady or time-dependent flow fields using a Stokes trap. Vesicles are membrane-bound soft containers that are often used for triggered release or reagent delivery and play an integral role in key biological processes such as molecular transport in cells. Giant unilamellar vesicles (GUVs) have been used as model systems to study the equilibrium and non-equilibrium dynamics of simplified cells that do not contain a cytoskeleton or polymerized membrane commonly found in cells. A grand challenge in the field of membrane transport lies in understanding how interfacial mechanics and fluid dynamics on the inside or outside of a soft vesicle contributes to the overall shape instabilities. Here, we study the dynamics of single floppy vesicles under large strain rates (~20 s-1) using a Stokes trap, which is a new technique developed in our lab for controlling the center-of-mass position of multiple particles or single molecules in a free solution. In this way, we directly observe the vesicle shape and conformations as a function of reduced volume,which is a measure of vesicle’s equilibrium shape anisotropy. We observe the formation of asymmetric dumbbell shapes, pearling, and wrinkling and buckling instabilities for vesicles depending upon the nature of flow and amount of membrane floppiness. We report the precise stability boundary of the flow-based phase diagram for vesicles in Capillary number (Ca)-reduced volume space, where Ca is the ratio of the bending time scale to that of flow time scale. We further probe the stability boundary at two different viscosity ratios to understand how the onset of dumbbell shape instability in vesicles depends on viscosity ratio. We also present results on the long-time relaxation dynamics of vesicles from high deformation back to their equilibrium spheroidal shapes after the cessation of flow. We study vesicles with shapes ranging from symmetric to asymmetric dumbbells with a long thin tether (extremely large fractional extensions with flattened thermal fluctuations), and we report on the influence of initial conditions in determining dynamic behavior. Using this approach, we study the non-equilibrium stretching dynamics of vesicles, including transient and steady state stretching dynamics in extensional flow. Our results show that vesicle stretching dynamics are a strong function of reduced volume and viscosity ratio, such that the steady-state deformation of vesicles exhibits power-law behavior as a function of reduced Capillary number. We identify two distinct relaxation processes for vesicles stretched to high deformation, revealing two characteristic time scales: a short time scale corresponding to bending relaxation and a long-time scale dictated by the membrane tension. We further discuss a model to estimate the bending modulus of lipid membranes as a function of vesicle reduced volume from the steady state stretching data. Overall, our results provide new insights into the flow-driven shape-dynamics for vesicles using new experimental methods based on the Stokes trap.",25,35.0
"Fuel cell technologies are regarded as the key for clean and sustainable energy generation; however, their efficiency is limited by the cathodic oxygen reduction reaction (ORR). Pt supported on carbon black, Pt/C, has long been recognized as the standard bearer in this field; however, its high cost and scarcity really cause challenges in wide applications. Therefore, my research interest is to search, develop, and design stable and efficient catalysts for ORR using computer modeling. In this work, molecular dynamics (MD) and density functional theory (DFT) are combined to study and design complex, multifunctional catalytic materials. To understand the carbon-supported Pt electrocatalysts for both cathodic (ORR) and anodic (MOR) reactions, MD simulations were performed to show that nanoscale Pt particles can be effectively stabilized at the open edges of the vertically aligned carbon nanofibers (VACNF). DFT calculations were then carried out to determine and predict the overpotential and catalytic activities. Moreover, DFT calculations were used to reveal the electronic structures, i.e. bader charge, which can be used as a descriptor to tune ORR activity. Further, scaling relationship of electrochemical potentials versus adsorption energies on reaction intermediates or charges on the active sites can be established and employed as training set for machine learning to screen, search, and design catalysts that has higher activity for ORR.Research Interests My research interest is catalysts design using first-principle-based method, i.e. density functional theory, to achieve high activity for ORR. I'd like to focus on building reasonable molecular model, investigating reaction mechanism and thermodynamics, characterizing material properties, finding catalytic trend, and designing new catalyst materials.",25,36.0
"Research Interests:  I am a methodical and ambitious research student with interests in tissue engineering, immunology and biomedical devices. For my dissertation, I am working on developing regenerative approaches for repair of soft tissues that employ light-activated immunomodulatory biomaterials and therapeutic delivery to enhance tissue repair.Sutures, staples and tissue glues remain the primary means of tissue approximation and vessel ligation. Despite widespread use, conventional sutures do not immediately seal approximated tissue and are susceptible to bacterial leakage, wound re-opening and infection. Laser-activated tissue sealing is an alternative approach which conventionally employs light-absorbing chromophores and nanoparticles for converting near infrared (NIR) laser to heat. The local increase in temperature engenders interdigitation of sealant and tissue biomolecules, resulting in rapid tissue sealing. We developed laser-activated nanosealants (LANS) in which, gold nanorods (GNRs) or indocyanine green (ICG) dye are embedded within a biopolymer matrix (collagen, silk or elastin-like polypeptides). We also fabricated novel laser-activated tissue-integrating sutures (LATIS) that synergize the benefits of conventional suturing and laser sealing. These laser-activated approximation devices demonstrated higher efficacies for tissue biomechanical recovery and repair in a full-thickness, dorsal surgical incision model in immunocompetent mice compared to commercial sutures and cyanoacrylate skin glue. Localized delivery of modulators of tissue repair, including histamine and copper, further improved healed skin strength following laser sealing and application. LANS films loaded with antibacterial drugs were able to significantly lower MRSA loads at the wound compared to antibacterial sutures, indicating a multifunctional strategy that synergizes rapid sealing with combating surgical site infections. In addition to incisional wounds, histamine co-delivered with silk LANS films accelerated the closure of full thickness, splinted excisional wounds in immunocompetent BALB/c mice and genetically obese and diabetic db/db mice, resulting in faster closure than Tegaderm wound dressing. Histological and immunohistochemistry analyses showed LANS-histamine treatment reduced dermal gap, promoted angiogenesis (CD31+), myofibroblast-mediated wound contraction (aSMA+), and higher TGF-β1 expression which are hallmarks of improved healing outcomes. Growth factor proteins have been investigated in tissue repair but have demonstrated limited efficacies as monotherapies. In order to address this, we first generated and characterized fusion polypeptide growth factor nanoparticles (GFNPs) that demonstrated high stabilities in wound fluid. Combination treatments of LANS-histamine and GFNPs resulted in higher efficacies of wound closure and tissue repair compared to monotherapies acting alone. An investigation into the kinetics of wound closure indicated that temporally sequential delivery of LANS-histamine followed by GFNPs, coinciding with different stages of tissue repair, demonstrated the highest efficacies of wound closure and tissue repair. Our results indicate that laser sealing and approximation, together with delivery of immunomodulatory mediators, can lead to faster healing and tissue repair, thus reducing wound dehiscence, preventing wounds moving towards chronicity and lowering incidence of surgical site infections, all of which can have significant impact in the clinic.",25,37.0
"Research Interests Homogeneous and Heterogeneous CatalysisCatalyst SynthesisCatalytic Mechanisms and Chemical Reaction EngineeringIntroductionEpoxide ring-opening is a key reaction for synthesizing a range of lab-scale and commercial compounds. A major application is the industrial production of polyols, which are primary raw materials for polyurethane plastics. This process involves the catalytic addition of an epoxide to an alcohol-based initiator, resulting in the formation of a β-alkoxy alcohol, which can further react with additional epoxide molecules to yield the desired polyol. Industrially utilized epoxides such as propylene oxide usually create a mixture of 1°-OH and 2°-OH ring-opening products (figure 1). The latter, however, exhibit poor reactivity for certain polyurethane applications and require substantial reprocessing prior to usage. This tedious step can be avoided by employing regioselective catalysts that predominantly yield 1°-OH products. However, nearly all conventional catalysts are unselective except for B(C6F5)3, which exhibits high selectivity towards 1°-OH products. This work explains its unique behavior in epoxide ring-opening, through the formation of catalytically active adducts with H2O1 and alcohols2 due to its strong Lewis acidity. Further on, we describe the influence of hydrogen bonding additives on the regioselectivity of the B(C6F5)3–H2O adduct and demonstrate a route to enhance its selectivity through the addition of α, β-diols.3MethodologyBatch runs were performed in 20 mL vials on a hot shaker. 1 mL o-Xylene (G.C. I.S.) was added to 0.1 mol%/1 eq. B(C6F5)3, with 5-20 eq. of an additive. The contents were heated, and the reaction started by adding 1 mL/1000 eq. 1,2-epoxyoctane (test epoxide) and 2 mL/4000 eq. of the corresponding alcohol. Progress was monitored using GC-FID and H2O levels were determined via Karl Fisher titrations.Selected ResultsAfter studying this system at different conditions, we observed two unusual trends – reaction rates significantly varied with alcohol type (1°,2°,3°)/H2O levels and reaction selectivities increased with conversion. After rejecting deactivation as a factor, we used DFT predictions to propose the catalytic activity B(C6F5)3–H2O/ROH adducts. Then, combining experimental data with theory, we developed microkinetic models which supported the simultaneous existence of Lewis acid, alcohol-mediated and water-mediated catalytic pathways (figure 2). Additionally, figure 3 also depicts the interesting trend where selectivity increases as a function of conversion in the absence of any additives. Ruling out product consumption as a factor, we proposed that reaction products P1&P2, were interacting with the B(C6F5)3–H2O adduct via hydrogen bonding and enhancing selectivity of the species. After verifying this hypothesis by adding co-catalytic amounts of P1 and P2, we sought to find external additives which could be used to deliberately enhance regioselectivity. Of all our target molecules, certain planar α, β-diols were most effective and presented a hydrogen-bonded complex which directed the incoming nucleophile in a manner that gave high selectivities.Key ConclusionsThis work demonstrates the existence of novel alcohol and water-mediated catalytic pathways for B(C6F5)3. As these have not been well-studied for similar highly Lewis acidic catalysts, this merits further work. Moreover, having demonstrated the influence of hydrogen bonding on reaction selectivity, we were able to enhance selectivity through the judicious addition of α, β-diols which could also extend to other such highly Lewis acidic catalysts.Literature CitedYu, Y., et al., ""Mechanism of regioselective ring-opening reactions of 1, 2-epoxyoctane catalyzed by tris (pentafluorophenyl) borane: A combined experimental, DFT, and microkinetic study,"" ACS Catalysis, 8 (12), pp. 11119-11133 (2018).Bennett, C.K., et al., ""Strong Influence of the Nucleophile on the Rate and Selectivity of 1,2-Epoxyoctane Ring-Opening Catalyzed by Tris(pentafluorophenyl)borane, B(C6F5)3,"" ACS Catalysis, 9 (12), pp. 11589-11602 (2019).Bhagat, M.N., et al., ""Enhancing the regioselectivity of B(C6F5)3-catalyzed epoxide alcoholysis reactions using hydrogen bond acceptors,"" ACS Catalysis 9 (10), pp. 9663-9670 (2019).AcknowledgementsThe authors would like to thank the Dow Chemical Company for its financial support towards this work.",25,38.0
"Research Interests Metal nanocrystals are promising in a broad range of novel applications. They can bear isotropic shapes, such as nanocubes, octahedra, icosahedra, as well as anisotropic shapes such as nanorods and nanobars. Metal nanowires originate from anisotropic growth, and their aspect ratio can reach ∼1000. Fivefold-twinned Cu nanowires (CuNWs) are widely used in electronic, optical, and catalytic applications. Long-chain alkylamine molecules have been employed as structure-directing agents (SDAs) in the shape-controlled synthesis of Cu nanocrystals in aqueous solutions. To understand how tetradecylamine (TDA) might function as an SDA in the growth of CuNWs, we study the adsorption of solution-phase TDA around a small and curved Cu nanoseed using a many-body metal-organic force field with molecular-dynamics (MD) simulations run with the LAMMPS code.Given sufficient TDA, the density of TDA molecules on the Cu nanoseed surfaces is lower than that of the TDA self-assembled monolayer (SAM) on the planar Cu surfaces. Instead, TDA molecules form a distinct “bilayer” around the seed: the surface molecules (inner layer) tend to stand normal to the Cu surfaces while TDA closer to the aqueous phase (outer layer) is prone to be parallel to the surface. The bilayer act as a dense “web” to protect the Cu surfaces against the solution phase. The relatively weak TDA alky-tail interactions around the curved nanoseed allow the exchange of TDA molecules within the bilayer, while the TDA population in each layer remains dynamically stable. By fitting the exchanging probabilities into Poisson forms, we obtained the diffusion coefficient of TDA, which resembles a good agreement with NMR measurement from experimental works. Compare to the TDA SAM on the planar Cu surfaces, the TDA exchanges much more rapidly around the small nanoseed. This exchange could serve as a special mechanism for the growth of nanowire: the solution-phase Cu2+ complexes first attach to the TDA in the out layer, and then reach the Cu surface through the exchange with the inner layer TDA.The significance of corners and edges, which is dominated by the size of the nanoseed, determines if TDA could form SAM or a bilayer around the Cu surfaces. To mind the gap between an infinite planar surface and a small and curved nanoseed, we create stepped Cu surfaces, which combine the features of a planar surface and a curved seed. The extent to which feature can be tuned by the dimensions of stepped surfaces. As expected, we found TDA can form SAM on the stepped surface when the effect of the planar surface overwhelms that of the edges and corners. In contrast, when the planar surface is relatively smaller, the effect of edge and corners dominates. Part of the TDA molecules escape from the SAM to form a second layer.",25,39.0
"Research Interests:  controlled release, drug release, stem cell engineering, nanoparticle formulation, surface modification, polymer chemistryStem cells are emerging as new generation of medicine for treating various diseases and tissue defects, as they can secrete a wide range of therapeutic cytokines and growth factors. However, a major obstacle for past utilization is the limited cell secretion activities due to hostile environment or insufficient activation signal at the transplanted site. To this end, we developed a stimuli-responsive particle system, that can be tethered on stem cell surface to provide proximal stimulation and actively release stimuli to enhance the cellular secretion activity. Specifically, we devised a dual binding poly(D, L-lactic-co-glycolic) (PLGA)-based particle system through RGD and hyaluronic polymer modification. We demonstrated that when the particle-tethered stem cells were exposed to shear stress during injection, less than 10% of the particles detached from cell surface. The improved particle retention led to a better stem cell stimulation as demonstrated with a 1.5-fold increase in cellular secretion activity. Further, we designed a catalytic particle system with an enhanced molecular release profile upon reaction with an external stimuli (H2O2). The catalytic particle system was assembled by encapsulating MnO2 and bioactive molecules of interest in PLGA particles. The MnO2 catalyzed the decomposition of H2O2 into oxygen gas, which increased the internal pressure of particles and accelerated the release of the bioactive molecules. We demonstrated that the catalytic particles encapsulated with epigallocatechin gallate can provide cytoprotective effect against reactive oxygen species and subsequently increase the secretion activity by 2.8-fold due the enhanced molecule release effect. Our approach of functional stimuli-responsive particles therefore provides a promising pathway to engineer stem cell surface and control the release of molecules of interest.",25,40.0
"During my Ph.D., I focused on the flow behavior of particulate systems and specifically on flow in colloidal dispersions, polymer melts and solutions, hydrogels, and solid handling systems.During my first major project, I investigated the mechanism controlling shear thickening in colloidal silica slurries. Shear thickening in colloidal dispersions often seriously limits formulations for coating and spraying operations, as well as flow rates for pumping concentrated dispersions. Besides, controlling shear thickening occurrence is critical in modern cement formulations, oil field extractions, and semiconductor polishing applications as well. The current study seeks ways to modify shear thickening in particulate systems and to investigate the underlying mechanisms controlling the shear thickening phenomena. Commercially available fumed silica suspensions exhibiting irreversible shear thickening at shear rates above 10,000 s-1 were used in this study as the base material. Using a specialized experimental protocol, the impact of spherical silica particles on shear thickening of fumed silica suspensions was investigated. It was shown that spherical particles can increase the critical shear rate when thickening occurs. In addition, rheo-Small-Angle-Light-Scattering (SALS) patterns also confirm the shift in shear thickening of fumed silica suspensions by applying spherical silica particles.In my second major project, I studied the flow of biomass and the problems associated with biomass handling and feeding from both experimental and modeling standpoints. The goals here is to develop physics-based computational methods which will be able to capture interactions between particles and their environment as imposed by the process. To experimentally validate the physics-based models, the impact of feedstock characteristics including particles size and size distribution, moisture content, and the screw feeder operating conditions such as screw speed on the flow characteristics of terrestrial biomass feedstock was investigated. Then, the experimental results were validated by Computational Fluid Dynamics (CFD) simulations using OpenFOAM software package for a simple pipe flow case. We found that at higher screw speeds and moisture contents, the material flows easier. In addition, the viscosity of biomass in the microcompounder falls within the shear thinning region of a Cross model fit. Moreover, the impact of particle size distribution on the flow behavior of compressed biomass is insignificant. For the modeling and simulation efforts, density-dependent Bingham and Cross models were developed and validated for biomass pipe flow under pressure. Rheological values obtained from pipe flow simulations showed that the models developed are able to capture the flow behavior of compressed biomass under pressure.Research and career interests:Experimental and modeling of flow in particulate systems, e.g. colloidal dispersions, solid handling systems, composites and filled polymers, polymer solutions, and hydrogels.",25,41.0
"Research Interests A significant challenge in designing drug and gene delivery agents is understanding how their chemical structure and properties affect interfacial properties at the cell membrane and dictate their transport across the cell membrane. My research interest lies in developing multiscale simulations and computational techniques to understand the molecular mechanisms of how the structure of biomolecules affect: (1) their transport pathways across the cell membrane, and (2) their interaction with other biomolecules. In summary, my research focuses on utilizing the fundamentals of biophysics and chemical engineering along with computational tools to provide faster screening tools (e.g. screening of drugs for properties such as permeability across the lipid bilayer) and better designs for drug and gene delivery agents.Research ExperienceMy research is focused on developing multiscale molecular dynamics (MD) simulations, state-of-the-art enhanced sampling techniques, and Python-coded analysis tools for two subareas.(1) I study the translocation (or “flipping”) of charged peptide loops across the lipid bilayer that is valuable not only to design small cationic peptides for applications in gene/drug delivery but also to understand the large-scale conformational rearrangements that involve multiple charge translocation events in naturally occurring integral membrane proteins. I developed all-atom enhanced sampling simulations (i.e. Temperature Accelerated Molecular Dynamics) protocols to predict the likelihood of loop flipping without predefining a specific translocation pathway. Further, I run the string method with swarms of trajectories to find the minimum free energy path for translocation, which gives insights into engineering materials capable of bypassing the cell membrane.(2) I study the amphiphilic behavior of bacterial signaling molecules that will allow us to understand their behavior in different biological environments that may help to control bacterial group behaviors such as virulence factor secretion, and biofilm formation. I predict the impact of their chemical structure on their aggregate morphology using alchemical free energy calculations using all-atom MD simulations. I then investigate their interactions with other amphiphilic biomolecules, such as lipids using coarse-grained MD simulations. I further collaborate with two experimental research groups at UW-Madison to gain better understanding of the process by validating simulation results with the experimental results and developing accurate prediction models.My research experience has led me to work as an intern at Lawrence Livermore National Laboratory, where I systematically studied the effects of different parameters for calculating the free energy barrier of drug translocation across the lipid bilayer, which led to faster screening of drugs for permeability.",25,42.0
"Research Interests - Catalysis, Process Chemistry, Synthesis and characterization of materials, kinetics, process modelingThe dependence of our civilization on plastics cannot be overstated – in 2018 alone, the worldwide consumption of plastics exceeded 300 million tons (~40 kg per person on earth). The carbon backbone of plastics is composed of monomeric hydrocarbons like C4 and C5 dienes. Diene market is valued at nearly $4 billion, and alongside steel and ammonia, diene production is integral to the growth of economies. These chemicals (butadiene, pentadienes, and isoprene) are currently produced by the cracking of naphtha fraction of crude oil, where ethylene is the major product. With a recent shift towards using shale-gas (primarily composed of light molecules like methane and ethane) as a cheap alternative to crude oil for ethylene production, refineries are decoupling production routes of ethylene and larger fractions including dienes. Consequently, butadiene production in the US has dropped by ca. 30% since 2007. As demands continue to rise and supplies fall, other on-purpose and possibly renewable routes that are not coupled to ethylene production are required.One such chemical pathway utilizes the tandem-ring opening dehydration (hereinafter referred to as dehydra-decyclization) of biomass-derived saturated furanic species to produce these dienes (Scheme 1). For example, four-carbon tetrahydrofuran (THF) reacts on the surface of a solid Brønsted acid catalyst to produce butadiene, while five-carbon methylated tetrahydrofuran (2-MTHF) yields linear pentadienes. Limited prior investigations of this chemistry had not rigorously tackled the three fundamental catalyst design challenges, namely assessing: rates (how fast the reaction occurs per active site of the catalyst), diene selectivities (how much of fed-in carbon ends up in desirable product/s), and lifetime (catalyst stability under reaction conditions). In addition, the origin of side products (propene, formaldehyde, and alkylated aromatics), as well as the identity of key reaction intermediates were unknown. The breadth of accessible dienes would be afforded by our ability to prepare numerous substituted THF precursors, and yet it remained unclear how these substituents tune the chemistry of diene formation. Due to this lack of mechanistic knowledge, the maximum diene selectivities attainable in the chemistry had remained low (< 60%); increasing these selectivities would drive down energy-intensive downstream separation costs, and potentially lead to economic feasibility of the overall process. My dissertation has focused on gaining fundamental insights into the dehydra-decyclization of saturated furans to guide a rational search for active, selective, and stable catalysts for this chemistry.Below, I highlight the results from three independent studies that detail my efforts in i) designing a chemical reactor capable of automated high-throughput catalytic evaluation, followed by ii) detailed kinetic evaluation of THF dehydra-decyclization to expound the catalytic mechanisms and pathways for the chemistry, and finally iii) the design and utilization of a selective and stable class of catalysts exhibiting ~89% diene selectivities at complete furan conversion.1)  Design and implementation of a chemical reactor for automated kinetic evaluation : Detailed mechanistic interrogations on a chemistry are preceded by high-throughput screening of a large number of catalysts to find promising candidates and to establish rate, selectivity, and stability trends to aid this search. Typical laboratory-scale packed bed reactors (PBRs) require constant human monitoring owing to multiple independent process control elements, and separate and non-coupled reaction and separation/quantification components, making them unsuitable for high-throughput studies. Motivated by the ability to perform high fidelity kinetic studies with minimal human oversight, I constructed and implemented a method to integrate a typical PBR within a Gas Chromatograph (GC), enabling the control of reaction parameters directly from the GC circuitry (Scheme 2). After carrying out reactor validation tests (e.g. residence time distributions, isothermal operation, absence of heat and mass-transfer limitations), it was found that typical reaction parameters can be obtained with reasonable accuracy with nearly no manual intervention on this low-cost setup. This automated reactor setup has since helped in the successful screening of promising catalysts for THF dehydra-decyclization (as discussed in 3)).2) Highlighting the mechanisms and pathways for diene formation : We studied THF dehydra-decyclization on an aluminosilicate zeolite H-ZSM-5 by a combination of first-principle calculations, microkinetic modeling, and experimental kinetic measurements. As part of this collaborative effort, my specific contributions to this work were three-fold; i) Experimental kinetic measurements for the chemistry: Assessing THF conversion rates enabled the identification of two key deleterious pathways compromising diene selectivities, namely the fragmentation of THF to propene, and C-C condensation pathways to alkylated aromatics. Furthermore, nearly identical activation energies of diene and propene formation pathways guided the search for a common intermediate formed before their respective rate-limiting steps (RDS); ii) Identification of key reaction intermediates for diene production: Co-processing potential intermediates with THF and observing changes in the evolution of butadiene and propene enabled the unambiguous identification of the key intermediates for both desirable and undesirable pathways in the chemistry. Furthermore, a > 100x higher rate of intermediate conversion vs. THF allowed us to conclude that the dehydra-decyclization RDS preceded the dehydration of these intermediates; iii) Experimental identification of the RDS: C-O bond cleavage leading to the ring-opening of THF was hypothesized to be the kinetic bottleneck as it exhibited a high activation barrier of ~ 49 kcal/mol. By stabilizing the carbenium ion transition state formed during this step by methyl-substitutions on the THF ring, I was able to increase the diene formation rates by > 50x (Figure 1a), experimentally demonstrating the rate-limiting nature of furan ring-opening step for the first time.3) Enhancing diene selectivities by weakly acidic catalysts : Explicating the mechanisms and pathways of THF dehydra-decyclization cultivated a quantitative understanding about the origin of selectivity; dienes resulted from C-O scission, while the primary side product (propene) originated from C-C fragmentation of THF ring, meaning that the diene selective catalysts should selectively activate C-O over C-C bonds. I hypothesized one possible ‘knob’ to tune C-O/C-C bond scission rates to be the Brønsted acidic site strength, as the energetics of proton transfer from the weak acid are likely more favorable towards C-O than C-C, owing to its significantly higher proton affinity. Catalytic evaluation of weakly acidic borosilicate zeolites in different surrounding framework environments consistently exhibited ca. 3-30x higher C-O/C-C scission rates than strongly acidic aluminosilicate zeolites (Figure 1b), evincing the inability of weak acids to cleave C-C bonds, and leading to ~10-30% increase in diene selectivities. In addition, borosilicate zeolites converted ~ 3-6x higher amounts of carbon from the feed to dienes during their lifetime than aluminosilicate zeolites (Figure 1c). This remarkable stability was conferred by the inability of these sites to bind and catalyze C-C condensation pathways to alkylated aromatics, which coke up the catalyst and limit their lifetime. The suppression of competing pathways lead to an unprecedented ∼ 89% combined linear C5 dienes’ yield from 2-MTHF, a 30% improvement from the previous best performing catalysts.In short, my research has provided key mechanistic insights into a sustainable catalytic route to produce commercially high-volume dienes from biomass-derived feed, which in turn, has enabled the discovery of selective and stable catalysts for this chemistry. As our society gradually moves towards renewable fuels and chemicals production, I believe that the results from this dissertation will be helpful in potential integration of this process chemistry with other hydrocarbon production routes within a biorefinery.",25,43.0
"Research Interests The knowledge of mass transfer is a crucial parameter for any separation process. A considerable amount of research has been conducted on the absorption of gas into a liquid, while limited literature is found on the rate of desorption of gas (i.e., gas evolution) from the liquid. The rate of gas evolution is assumed to be instantaneous or within the retention time in the vessel. The rate of absorption and desorption are often assumed to be identical under the same pressure, temperature, and hydrodynamic conditions (assuming no bubbles are formed). The symmetry between absorption and desorption of gas may have been investigated using low viscous liquids. However, in the oil and gas industry, high viscous liquids are also encountered. The incomplete separation of gas can lead to plant shut down, which can cause huge revenue losses. My research interest is aimed towards better understanding the gas-liquid separation at elevated pressures and deconvoluting the factors that influence the rate of gas evolution like viscosity, presence of water in the form of emulsions, presence of solid particulates, and supersaturation ratio. The symmetry between rates of absorption and desorption would be understood as well, along with the factors mentioned.Postdoctoral Project: Fluid Phase Kinetics – Joint Industrial ProjectUnder the supervision of Clint P. Aichele, School of Chemical Engineering, Oklahoma State University.Ph.D. Dissertation: “Bio-Based Active Barrier Material and Packaging Development”Under the supervision of Maria R. Coleman and Saleh A. Jabarin, Department of Chemical Engineering, University of Toledo/ Polymer Institute, University of ToledoResearch Experience: Throughout my academic career, I have worked on various application-oriented research. The research areas that I have worked on are the removal of dyes (adsorption), reduction of oxygen permeability through polymers, and factors affecting the rate of gas evolution from supersaturated liquids at elevated pressures. As a result of working on these projects, I have acquired experience in many research areas: water treatment, polymers, nanomaterials, emulsions, surface tensions, and gas-liquid separations. I have become proficient as an experimentalist with an ability to develop a lab that is geared towards addressing these areas of research.Teaching Interests: I have been a Teaching Assistant (TA) for various courses, including Process Separation, Fluid Dynamic, Reaction Kinetics, and Unit Operation Lab. For the above course, I have been well appreciated and reviewed by the students. As an instructor, my teaching philosophy would be to embrace an active classroom environment, which encourages question/discussion and solving practical problems to better understand chemical engineering concepts.Teaching Experience:Throughout my doctoral program, I have undertaken various TA assignments at the University of Toledo. I have assisted in handling the Unit Operations Lab for 4 semesters and given review lectures for Process Separation and Reaction Kinetics. Having received the Best Teaching Assistant award twice (University of Toledo) reflects my ability as an instructor. I have also mentored graduate and undergraduate students in their research.Publication: Michael Angelo Miranda, Sayeed A Mohammad, Hariprasad J. Subramani, and Clint P Aichele “Kinetics of Gas Evolution from Super-Saturated Oils at Elevated Pressures and Temperatures” Energy & Fuels (2020)Michael Angelo Miranda, Ashwin Yegya Raman, David M. Lavenson, Hariprasad J. Subramani, Sayeed A. Mohammad, and Clint P. Aichele, “Gas Evolution Rates in Supersaturated Water-in-Oil Emulsions at Elevated Pressures” Energy & Fuels (2019).Alden B Daniel, Sayeed A Mohammad, Michael A Miranda, and Clint P Aichele “Absorption and Desorption Mass Transfer Rates as a Function Pressure and Mixing in a Simple Hydrocarbon System” Journal: Chemical Engineering Research and Design (2019).Michael Angelo Miranda, Maria Coleman, Saleh. A. Jabarin “Modification of Polyethylene terephthalate (PET) using linoleic acid for oxygen barrier improvement: Impact of processing methods” Journal of Applied Polymer Science (2017).Miranda, M.A., M. Coleman, and S. Jabarin. Impact of Processing Method and Loading of Active scavenger (Linoleic Acid) on Properties of Polyethylene Terephthalate. in SPE ANTEC™ Indianapolis 2016.Michael Angelo Miranda, P. Dhandapani, M. Helen Kalavathy, and Lima Rose Miranda, “Chemically activated Ipomoea carnea as an adsorbent for the copper sorption from synthetic solutions” Adsorption (2010) 16: 75–84.",25,44.0
"Research Interests Rheology is the study of flow and deformation of the matter. It is a useful tool to understand the structure-property relationships of materials at various conditions. Fundamental theories and empirical equations are established to connect the rheological properties to the many material properties, which guide formulation screening and process optimization. My research interest lies in the material characterization together with model building and validation to address the challenges in material design and manufacturing.Research BackgroundIn my PhD study, I use my expertise in rheology to characterize various materials in many applications aiming to screen formulations and optimize processes. My thesis work focuses on rheological characterization of cellulose nanomaterials. [1] To expand the nanomaterials production capacity to industrially relevant scale and to ensure consistent production of the cellulose nanomaterial, one of the most urgent issues to be addressed is the lack of standardized, rapid and reliable characterization method for quality control during the manufacturing process. My research shows the potential of rheology as a quality control tool for these nanomaterials. Preparation and test protocols are first established to obtain reliable viscosity data at manufacturing relevant conditions. Next, a rheological model is developed, which can accurately capture the viscosity across shear rates and concentrations. The model can be used to estimate the concentration of an uncharacterized sample which is much faster than the current method to determine the concentration by drying in oven. Moreover, a flow index is developed to condense large sets of data into one single value, working as a “fingerprint” of the material. This flow index can be used to identify materials at different processing conditions, including different morphologies or surface charges. Finally, a processing method is developed to concentrate cellulose nanomaterials and obtain high loadings of well-dispersed nanomaterials in polymer composite gels. For collaborative projects, I use rheology to screen formulations for solution-based 3D printing [2], composites for fiber spinning, hydrogel fabrication and food formulations for swallowing difficulties study.[1] Liao, J., Pham, K. A., & Breedveld, V. (2020). Rheological characterization and modeling of cellulose nanocrystal and TEMPO-oxidized cellulose nanofibril suspensions. Cellulose, 1-17.[2] Zhang, F., Ma, Y., Liao, J., Breedveld, V., & Lively, R. P. (2018). Solution‐Based 3D Printing of Polymers of Intrinsic Microporosity. Macromolecular rapid communications, 39(13), 1800274.",25,45.0
"Data-driven modeling has proven a powerful tool for drawing useful correlations from large datasets, yet the ability to generate physical insights from purely data-driven models remains limited due to their black-box nature. Hybrid models (HMs) have the potential for increased physical interpretability by leveraging mechanistic models for known and hypothesized relationships with data-driven models for unknown relationships in a single framework. HM frameworks often assume mechanistic information is fully known, including parameter values, prior to model regression.[1] However, for many systems of interest, constructing an HM requires estimating the parameters of both data-driven and hypothetical mechanistic submodels on the same limited dataset.[2]This submission considers novel methods for estimating parameters of dynamic models containing unknown mechanistic and data-driven parameters. Specifically considered are systems that can be formulated as a series of coupled ordinary differential equations. These methods are compared with traditional approaches to parameter estimation using forward sensitivity analysis. The comparison is further extended to the case where experimental data is low-quality and sparse. Results indicate that methods which merge data-driven models with numerical methods provide better estimates of time-evolved data and their derivatives than purely data-driven approaches. Conclusions for this presentation will identify parameter estimation approaches that accelerate the validation of interpretable models for systems when both data and mechanistic knowledge is limited.ReferencesOliveira, R., Combining first principles modelling and artificial neural networks: a general framework. Computers & Chemical Engineering, 2004. 28(5): p. 755-766.Yang, A., E. Martin, and J. Morris, Identification of semi-parametric hybrid process models. Computers & Chemical Engineering, 2011. 35(1): p. 63-70.Research Interests My research has focused on investigating methods for merging data-driven tools and mechanistic knowledge for modeling dynamic process data. The algorithms developed enable modelers to accelerate the systematic validation of hypothesized chemical-physical relationships without system-level information. I have further investigated the application of modern tools in automatic differentiation and numerical methods to support automated estimation of complex differential equations. My research interests lie primarily in developing novel solutions for modeling, control and optimization of manufacturing systems and supply chains.",25,46.0
"Research Interests In the past five years, monoclonal antibodies (mAbs) market has been increasing rapidly, and is predicted to reach $130-200 billion in year 20221. There is a need to improve the overall process productivity to satisfy the increasing demands for mAb market. Two approaches to improve the process productivity include – 1) selection of high-performance process equipment, 2) application of optimum operating conditions. Continuous processing shows a significant benefit in increasing productivity, reducing the footprint, and cost-effectiveness over the conventional operations2. However, quantitative evaluation of the overall process performance is critical to decide the preferred mode of operation (batch or continuous) to satisfy the increasing market demands for mAb production. In addition, it is essential to procure detailed process understanding of unit operations to enhance product quality and process productivity.In this work, we explore different simulation methods, including flowsheet modeling and mathematical modeling, to accurately predict mAb production. We implement flowsheet modeling to design and construct a fully integrated framework for continuous mAb production and compare its performance with the batch process using techno-economic analysis3. Scenario studies are simulated to evaluate process cost-effectiveness under varied production scales and upstream/downstream parameters. As bioreactor takes the highest percentage of the operating cost, we employ kinetic modeling method to capture the nonlinear bioprocess dynamics between operating conditions and output variables, aiming to improve the process productivity while maintaining product quality for CQAs like glycan concentration. Proposed kinetic modeling approach allows to evaluate important process variables like viable cell density, glucose, lactate, ammonia concentration, protein titers, and glycosylation during the batch culture for different temperature and pH values. Following this, we develop a surrogate model to investigate the optimum operating conditions for bioreactor. This is further integrated within the flowsheet model to capture corresponding effect on the mass balance, process scheduling, and critical quality attributes of the overall process. Thus, in conclusion, we propose applications of system engineering tools like process modeling and optimization for biopharmaceutical manufacturing, focusing on comparison of operation modes – batch and continuous, bioreactor modeling and mAb production, paving the way for cost-effective manufacturing and high quality biologics production.Reference Grilo, A. L. & Mantalaris, A. The Increasingly Human and Profitable Monoclonal Antibody Market. Trends Biotechnol. 37, 9–16 (2019).Yang, O., Qadan, M. & Ierapetritou, M. Economic Analysis of Batch and Continuous Biopharmaceutical Antibody Production: a Review. J. Pharm. Innov. 15, 182–200 (2020).Yang, O., Prabhu, S. & Ierapetritou, M. Comparison between Batch and Continuous Monoclonal Antibody Production and Economic Analysis. Ind. Eng. Chem. Res. 58, 5851–5863 (2019).",25,47.0
"Research Interests: 1. Design of rules for synthesis of stable single-site catalysts from experiment and first principles theoryDeveloping a computationally derived and experimentally validated feature-based predictive model for establishing the stability of precious group metal (PGM) precursors under solid-liquid synthesis conditions to guide the synthesis of stable PGM single-site catalystsInvestigating active sites for low temperature CO oxidation over ceria supported single atom catalyst with validation through experiments and microkinetic analyses2. Design of oxide@metal interface for hydrodeoxygenation chemistry of biomass derivativesComputational screening for predicting an optimal oxide@metal catalyst based on the fundamental properties of the oxide and the metal towards the selective C-O cleavage of oxygenated biomass derivatives, studied through Density Functional Theory (DFT)Investigating the electronic perturbations of the oxide through transition metal dopants and the catalytic implications for interfacial reaction chemistry3. Tuning conductivity of high molecular weight and crystalline PEO6 based polymer electrolytes using X-ray scattering experiments and DFT modelsPoster abstractUnderstanding the synthesis of single atom catalysts and the underlying metal-support interaction is crucial in improving the anti-sintering capability during chemical reactions. Strong electrostatic adsorption (SEA), which relies on an electrostatic driving force for the deposition of appropriately charged metal precursors on an oppositely charged metal oxide surface, demonstrates considerably higher weight loading threshold than for incipient wetness. Our study combines density functional theory (DFT) calculations and solution phase isothermal titration calorimetry (ITC) adsorption experiments to examine charged and solvated precursor adsorption onto ceria nano-cubes. DFT calculations bridge from an isolated gas phase metal atom and a single crystal plane of oxide surface to a ligated metal atom and bulk oxide surface (with a mixture of different crystal planes) in solution phase. We demonstrate agreement between DFT and ITC adsorption energy trends, towards developing a predictive framework for the stability of precious group metal (PGM) precursors under solid-liquid synthesis conditions. Our SEA adsorption model considers mainly electrostatics, with charge separated over the ligated complexes and the support surface. We use DFT to model the adsorption including partial deligation of the precursor, examining how the ligand chemical potential can alter adsorption thermodynamics.We also examined CO oxidation kinetics over the single atom catalysts on ceria. DFT calculations in conjugation with experiments examine the catalytic performance and identify the synergy between individual metal atoms and the support towards CO oxidation. The emphasis is laid on specific metal-support interaction via structural relaxation or electronic structure induced by the presence of the adatom.",25,48.0
"Research Interests: From shape memory polymers to heterogeneous catalysts, my research experiences in a wide array of areas highlight my passion for materials research. Through a combination of design, synthesis, and characterization, I elucidate structure-function relationships to provide fundamental insights for rational materials design. In particular, my PhD research focuses on studying metal oxides and sulfides as catalysts for light alkane dehydrogenation, which is an important route to produce olefins—a critical feedstock for many materials and chemicals. Using a combination of catalytic testing and spectroscopic tools in my study on a series of supported molybdenum oxides, I was able to draw correlations between the catalyst surface species, reducibility, and activity. Most notably, I found that while previous studies focused primarily on bulk and high loading molybdenum oxide materials, it is sub-monolayer molybdenum oxides that warrant further study as light alkane dehydrogenation catalysts with its superior catalytic stability under the harsh reaction conditions.Throughout my PhD, I have developed extensive expertise in many areas, from reactor engineering to materials synthesis and characterization, which readily translate to applications beyond heterogeneous catalysis. Looking forward, it is my goal to leverage the skills I have honed and continue engineering new materials to drive innovation forward.",25,49.0
"Research Interests I am a Ph.D. candidate and NSF Graduate Research Fellow in the Department of Biological Engineering at MIT. In Katharina Ribbeck's lab, I study how unique glycopolymers in mucus can be leveraged as antivirulence therapeutics to fight infections while circumventing antibiotic resistance. I'm interested in understanding the mechanisms of environment- and community-dependent bacterial interactions to promote human health and sustainability. I have experience in glycobiology, polymers, microfluidics, microbiology, and synthetic biology. Before coming to MIT, I earned my B.S. in Chemical Engineering at Caltech where I engineered proteins from soil bacteria with Frances Arnold. I then spent a year in Switzerland as a Fulbright Fellow, where I studied cell-free synthetic biology with Sebastian Maerkl at EPFL. I have 4+ years of experience mentoring and teaching inside and outside the lab. I'm a trained conflict management coach through MIT REFS (since 2016). I have mentored four undergraduate students at the bench. I have been a teaching assistant for 8 courses in my career, earning me department awards for teaching excellence. ",25,50.0
"Research Interests A common theme in my current research is the rational design of emerging technologies based on a mechanistic understanding of their deficiencies. This approach has been specifically applied to the development of immobilized enzyme sensor for the detection of explosive chemicals. Enzyme sensor technology has incredible potential, but limited success due to the loss of enzyme function upon immobilization. Significant advances in understanding of the mechanism behind the reduced activity of immobilized enzymes has been possible through several complementary single-molecule fluorescence microscopy techniques in combination with sophisticated trajectory analysis using stochastic (non)linear, (non-)Markovian state-space models. Using these methods, we have been able to quantitatively identify heterogeneity in functionality of immobilized enzymes and distinguish several important modes of failure, including inaccessible active sites, structural instability, inhibited conformational mobility, and substrate/product transport barriers, which are typically convoluted by less sophisticated measurements and analyses. Using this nuanced understanding of immobilized enzyme function, we developed several immobilization materials and methods showing enhanced performance relative to enzymes in solution.More recently, my research has been focused on transport in ultra-thin polymer films commonly used in separations membranes, biosensors and drug delivery applications. These technologies are reliant on control over the transport of specific chemical species, but the current methods for assessing the transport properties rely on macroscopic ensemble characterization, yielding limited mechanistic insight and ambiguity in the steps for improvement. Using unique single molecule tracking methods and analysis based on machine learning, I have developed a method for tracking nanoscale single-molecule diffusion in 3D. This approach will provide the nuanced understanding of polymer and small molecule transport in thin polymer films necessary for the development of scalable predictive models and rational design of polymer thin film technologies.",25,51.0
"Research Interests My research is focused on design and development of micro-structure platforms for (i) a superior end to end continuous process of manufacturing crystalline materials and (ii) investigation and analysis of dendrite crystallization and growth in the energy storage and conversion systems.Research Area1: Continuous Manufacturing of Pharmaceuticals with Microstructure platformsDescription: Manufacturing of pharmaceuticals requires for multiple batch operations, long operational hours and higher costs for supply chains. The rising costs has pressurized pharmaceutical industries to employ alternative approaches and increase their efficiencies to meet the growing demands. One of the most fundamental alternative approaches is continuous flow synthesis and formulation of active pharmaceutical ingredients because of its promising lower cost of production while being more reliable and safer. As a part of pharmacy on demand (PoD) development a compact end to end manufacturing system will significantly shorten the lead times. The emergence of the continuous manufacturing of pharmaceuticals started with small molecules, yet it is still challenging to extend the continuous approach for larger molecules such as monoclonal antibodies. Moreover, overcoming these challenges becomes more substantial for some of the cases such as vaccines, cell based or gene therapies. The significance of an efficient, robust and inexpensive process for manufacturing of vaccines has become more significant specially after the most recent global pandemic of COVID-19. It is an inevitable fact that after the antibody discovery and vaccine development, the extremely high demand for these products complexes their supply chain over the world. Therefore, shifting to PoD synthesis using microstructure platforms will mitigate the expenses for production and distribution and decrease the time span of the distribution of these products around the world, especially in the underdeveloped and developing areas.The proposed research is on the development of compact and reconfigurable platform for:Increasing the productivity and efficiency of manufacturing of APIs;Reducing the cost of R&D and further APIs;Faster distribution of the essential drug products and shorter the supply chain distribution time span.In the most recent studies, the continuous manufacturing of the APIs has been perused by designing compact modules for each section. These end-to-end manufacturing process use lab scale setups and are improved over the past few years with respect to the yield of productions as well as purity of the products. Hence, a micro-structure setup which has never been studied for the continuous manufacturing. The development plan of the continuous manufacturing process with microstructure platforms for a PoD system requires deep understanding of essential synthesis, filtration, and formulation steps as well as appropriate crystallization technique and operational condition. In order to obtain the optimal design for the continuous manufacturing process, Multi-physics simulations will be employed for the fabrication and assembly of the continuous manufacturing setup.Research Area 2: Dendrite Growth and Crystallization in Energy Storage and Conversion Systems Description: Solid state electrocatalysis are crucial to develop electrochemical energy storage and conversion systems to sustain our future energy needs that currently depend on depleting fossil fuels. In this energy transition, lithium (Li) metal is known as a promising candidate for rechargeable battery technologies owing to its high theoretical specific capacity. Nevertheless, its practical application is limited due to high reactivity of Li metal in contact with electrolyte as well as with air components such as humidity, carbon dioxide and etc. This will lead to formation of dendrites on the surface of the Li anode upon striping/plating over the cycling process that lowers the columbic efficiency, cycling performance and more importantly, causes short circuit by punctuating the porous separator. Therefore, an overarching challenge for this technology is to preserve the Li metal anode from possible parasitic reactions, making it a safe and higher performance, rechargeable battery system. To this end, extensive studies have been carried out on the electrolyte selection, electrolyte additives, electrochemical, chemical and physical artificial solid electrolyte interphase layers, super-concentrated electrolytes, solid electrolytes, membrane modification, as well as effect of operating parameters to stabilize the surface of Li anode by suppressing the dendrite growth. However, the so-called techniques either possess poor mechanical strength, high costs, low ionic conductivities, complication in fabrication process or almost no effect on the dendrite growth and nucleation, making this technology infeasible. Therefore, deeper scientific insights are required to precisely understand the Li plating/stripping behavior and avoid dendrite formation. My research proposal is focused onDevelopment microstructure devices with ability to incorporate both closed and open battery architectures for real time analysis of dendritic growth and crystallization of Li metal;Monitoring the dendrite crystallization can be as in-situ bright-field microscopy, confocal Raman microscopy, and X-ray diffraction;Screening different process parameters such as current density, the type of the electrolyte.The information obtained by such analyses will be used to develop strategies, i.e. artificial solid electrolyte interphase layers, that can suppress the dendrite formation for higher performance and a safer battery technology.",25,52.0
"I am a Ph.D. student working with Prof. Karthish Manthiram in the Chemical Engineering department at MIT. I will be completing my Ph.D. in 2021. I am passionate about pushing the frontiers of sustainable chemistry swiftly and practically, so that we may have the best chance of maintaining a human-habitable planet in the coming decades; it is my hope that through my career I may contribute to this grand endeavor in some way. My doctoral research has focused on tying in chemical engineering fundamentals to practical problems in electrochemical synthesis. In the next steps of my career, I would like to move further in the direction of developing electrosynthetic reactor systems on a device level. Research Interests Electrochemistry is one promising route toward renewable energy storage and sustainable chemical synthesis. For example, using renewably-sourced electricity to drive water splitting is a lower-greenhouse-gas-emission alternative to steam methane reforming for the production of hydrogen; in addition, greenhouse gases such as CO2 can themselves be transformed back into fuels and useful products using electrochemical potential. However, such processes require both fundamental study and device development in order to be economically feasible, whether or not they are accompanied by policy solutions such as carbon pricing. In modern electrocatalysis, reactions span a wide range of developmental stages. Processes such as chlorine evolution and aluminum smelting have long been staples of industry, and water splitting for the generation of hydrogen gas is nearing large-scale implementation; whereas the electrochemical reduction of carbon dioxide, while widely researched, is only beginning to be studied at high rates and moderate scales. Less developed still are reactions such as electrochemical nitrogen reduction or the direct oxidation of small organic molecules for synthetic purposes. Additionally, with some exceptions, when these reactions are studied, they are usually done so independently from their counter reactions – the reactions at the counter electrode required in order to complete the circuit.With these points in mind, I am interested in two major approaches toward electrochemical research: (1) leveraging the state-of-the-art catalysts for small molecule transformations to study the impacts of device design upon the kinetics and transport of reactions, including the hydrogen evolution reaction, CO2 reduction reactions, N2 reduction reaction, and organic oxidative functionalization reactions using water as the O-atom source, under industrially-relevant conditions – for example, using realistic feedstocks – and in this way, connecting device development with fundamental understanding. (2) Examining the impacts of tuning the counter reaction on both the efficacy and the technoeconomics of the working reaction. For example: could we leverage clever flow fields to separate products made at the cathode and anode without the need for a physical separator? How do we understand the tradeoffs between the energy required to pump electrolyte, or to overcome ion diffusion resistance in physical separators, versus to perform downstream separations? Might it be possible to alter the economic prospects of reactions such as hydrogen evolution or CO2 reduction by altering the counter reaction to perform a more lucrative partial oxidation reaction as opposed to oxygen evolution? If so, are there oxidative substrates that could be employed in such a way that electrolyte conditions enable synergistic interactions between oxidative and reductive substrates? Ph.D. ResearchTwo major projects I have pursued during my doctoral research include: (1) uncovering the role of mass transport in protecting the CO2 reduction reaction from parasitic side reactions with gas-phase contaminants, and (2) understanding solvent non-idealities in blended aqueous—nonaqueous electrolytes during electrochemical oxidative O-atom transfer reactions involving water. During this work I have extensively employed skills in electrochemical kinetic analysis, bulk electrolysis, cyclic and square-wave voltammetry, gas flow setup, gas chromatography, headspace sampling for solution thermodynamic measurements, and quantitative nuclear magnetic resonance (NMR) spectroscopy. I have additional experience in solid-state and colloidal approaches to metal oxide and nanoparticle synthesis; X-ray absorption spectroscopy (XAS); X-ray photoelectron spectroscopy (XPS); powder X-ray diffraction (XRD); UV-visible spectrophotometry; electron microscopy (SEM/TEM); elemental analysis via inductively coupled plasma (ICP); hydraulic pressing; organic synthesis; practical glassblowing; chemisorption analysis; and ultra-high vacuum (UHV) setups. Selected PublicationsWilliams, K.; Corbin, N.; Zeng, J.; Lazouski, N.; Yang, D. T.; Manthiram, K. Protecting Effect of Mass Transport during Electrochemical Reduction of Oxygenated Carbon Dioxide Feedstocks. Sustain. Energy Fuels 2019, 3 (5), 1225–1232.Lazouski, N.; Chung, M.; Williams, K; Gala, M. L.; Manthiram, K. Non-aqueous gas diffusion electrodes for rapid ammonia synthesis from nitrogen and water-splitting-derived hydrogen. Nature Catalysis 2020, 3 (5), 463-469.Zeng, J.; Corbin, N.; Williams, K.; Manthiram, K. Kinetic Analysis on the Role of Bicarbonate in Carbon Dioxide Electroreduction at Immobilized Cobalt Phthalocyanine. ACS Catalysis 2020, 10 (7), 4326-4336.Lazouski, N.; Schiffer, Z. J.; Williams, K.; Manthiram, K. Understanding continuous lithium-mediated electrochemical nitrogen reduction. Joule 2019, 3 (4), 1127-1139.Yang, D. T.; Zhu, M.; Schiffer, Z. J.; Williams, K.; Song, X.; Liu, X.; Manthiram, K. Direct Electrochemical Carboxylation of Benzylic C-N Bonds with Carbon Dioxide. ACS Catalysis 2019, 9 (5), 4699-4705.Corbin, N.; Zeng, J.; Williams, K.; Manthiram, K. Heterogeneous molecular catalysts for electrocatalytic CO2 reduction. Nano Research 2019, 1-33.",25,53.0
"Research Interests:  I am a 5th-year Ph.D. candidate in the Chemical Engineering department at Massachusetts Institute of Technology. After defending in the next year, I hope to pursue a career in industry focused on modeling and optimization for process operation and design. My Ph.D. work as a member of the Process Systems Engineering Laboratory has involved both developing new numerical approaches and applying them to real systems. In particular, I have been collaborating with OCP’s phosphate-processing facility in Morocco to ensure that these tools can be effectively used to direct innovation in an industrial setting. I have enjoyed the opportunity to use modeling and optimization to develop specific methods to make concrete improvements, and I hope to also have these qualities in my future career. I am particularly interested in bringing my expertise in method development to create new tools that can address traditionally challenging systems that are too complex for current approaches to handle.Ph.D. Work: The focus of my Ph.D. research has been using recent advances in nonsmooth equation solving to develop new methods for modeling complex chemical systems. Many processes in chemical engineering, including phase changes and batch or semi-batch manufacturing, transition between different regions, making them naturally nonsmooth. Traditionally, these processes are modeled using disjunctive or mixed-integer programs that are often nonlinear and scale poorly with system size. Instead, I have proposed simulating these systems using compact, explicitly-nonsmooth equations, which, unlike current approaches, requires only equation-solving methods, scales well with system size, and can solve for any unknown process variable. In particular, I have been collaborating with OCP to apply this approach to improve process integration and reduce resource use at their Jorf Lasfar platform. Throughout this project, I have developed and implemented process models for the major units at Jorf Lasfar using a combination of fundamental relations and empirical plant data. I have also performed analyses to identify areas with the most potential for improvement in resource use, utilized new research developments to create numerical methods tailored to simplifying this problem, and applied these methods to propose specific process designs for the sustainable expansion of Jorf Lasfar. I am currently working on developing nonsmooth optimization methods to minimize the costs of these designs by adapting global optimization algorithms. In the future, I hope to be able to continue applying these skills for process design and method development to help tackle key problems across chemical processes.",25,54.0
"Research Interests My success as a research associate at Purdue University is the result of my skills and qualifications as a proven problem-solver and communicator who is interested in identifying interesting questions and collaborating with two members of national academy of engineering (Sangtae Kim and Doraiswami Ramkrishna) to find tangible solutions.I believe that the pharmaceutical development and manufacturing is an excellent match for me, and the following offers a few highlights of my skills and qualifications:Problem solver: As a research associate, on a day to day basis I engage in interdisciplinary research problem using rigorous scientific methodology to analyze data, draw conclusions and communicate results. While I have a fundamental knowledge of separation, filtration, and crystallization for the pharmaceutical industry, my areas of research include transport phenomenon, reaction engineering and processing engineering. These skills have been developed over the course of my 10-year scientific career and have led me to publish papers in top ranked journals, including Nature Scientific Reports and Journal of Fluid Mechanics. These skills will provide the essential basis of my chemical engineering career.Flexibility and people skills: The range of professional environments I have worked in, including research institutions, software industry, and national research center has made me adept and comfortable interacting with anyone at any level in an organization. What I find more engaging is to understand each individual’s perspective and their stakes, access their abilities and goals, and develop strategies to meet our common goal.Communicator/Leader: I am dynamic and motivated as a communicator when I have the ability to collaborate with chemist/experimentalist and to present my scientific results and ideas. In addition to my verbal presentation skills, I have proven myself an effective writer, as evidently by multiple published papers and book chapter. Lastly, I have supervised fellow workers in both scientific and non-scientific settings.Selected publications(out of 13 total, google scholar: https://scholar.google.com/citations?user=uEmQO5cAAAAJ&hl=en)First-author contributions are underlinedWang, S., Ramkrishna, D., Narsimhan, V. “Exact sampling of polymer conformations in an external field using Brownian bridges” Journal of Chemical Physics (accepted).Wang, S., Martin, C.P. and Kim, S., 2019. Improper integrals as a puzzle for creeping flow around an ellipsoid. Physics of Fluids, 31(2), p.021101. (Invited papers on transport phenomena in celebration of Prof. Robert Byron Bird’s 95th birthday)Wang, S. and Ardekani, A.M., 2015. Biogenic mixing induced by intermediate Reynolds number swimming in stratified fluids. Scientific reports, 5, p.17448. (Highlighted on Physics.org, Futurity, Geology Page, Purdue College of Engineering News Page, Purdue Research Computing Cluster)Wang, S. and Ardekani, A.M., 2012. Unsteady swimming of small organisms. Journal of Fluid Mechanics, 702, pp.286-297.AuthorShiyan WangPurdue University",25,55.0
"AAA+ proteases are enzymes responsible for removing unwanted proteins and activating cellular processes through the degradation of tagged proteins. Escherichia coli ClpXP is a model AAA+ protease that uses the energy of ATP binding and hydrolysis to fuel protein degradation. The hexameric ClpX ring first unfolds a specific protein before translocating the polypeptide into the ClpP degradation chamber. Various biochemical and structural studies have been used to characterize ClpXP, yet the conformational transitions and nucleotide transactions of ClpXP subunits are still unknown. Here, we engineered two novel, short-range single-molecule fluorescence quenching assays to monitor conformational switching and nucleotide-binding in a single subunit of ClpXP. We synthesized an ATP analog labeled with a dark quencher that we call ATPQ for our single-molecule nucleotide-binding assay. ATPQ supports ClpXP motor activity and permits the observation of binding events at concentrations up to 100 μM. Our conformational switching assay and DNA-based distance calibrations allow for quantitative measurements of conformational states and dynamics of individual ClpX subunits. Our results show that ClpXP subunits adopt both static and dynamic conformational states at varying nucleotide and substrate conditions. The single-molecule techniques we developed here facilitate real-time observation of molecular events in working AAA+ motors and can be applied to a wide range of molecular motors.Research Interests Biophysics, Biochemistry, Molecular Biology, Optical Trapping, Protein Engineering, Optics, Photonics, Fluorescence, Single-Molecule Biophysics, AAA+ Motors",25,56.0
"Abstract: Paving the way for cybernetic modeling of biological processes in mammalian systemsOur bodies are well equipped to address and respond to external injury. The body has a two-line defense system against invading pathogens: the skin which acts as a physical barrier and the immune system which triggers an inflammatory response to fight off pathogens. Significant variation in the inflammatory response can lead to complications in certain disease states (e.g., cytokine storm in Covid-19 disease). Macrophages are a versatile immune cell which play a crucial role in inflammation. These cells have the ability to identify a foreign stimulus and trigger a cascade of cellular signaling events. Developing a working knowledge of these cellular events and a description of signaling pathways using mathematical models is imperative for studying immune-modulating drugs and their impact on disease outcome.Regulation of metabolism in mammalian cells is achieved through a complex interplay between cellular signaling, metabolic reactions, and transcriptional changes. These complex interactions can be modeled through implicit accounting of regulation using a cybernetic framework, as seen in the modeling of prostaglandin metabolism in inflamed macrophage cells [L. Aboulmouna et al., Processes. 2018]. The premise of the cybernetic framework is that the regulatory processes affecting metabolism can be mathematically formulated through variables that constrain the network to achieve a specified “goal”.Cybernetic models, developed by Ramkrishna’s group, assume that the kinetic parameters of the mechanistic fluxes are not constants and can vary with time. The kinetic parameters are represented as the product of unregulated rate constants and cybernetic control variables for induction of enzyme synthesis (u) and modulation of enzyme activity (v). The cybernetic control variables can be intuitively formulated based on the goal of the system. This model framework describes gene regulation by attributing metabolic preferences to a suitable survival goal of the organism. Previous cybernetic models use regulatory goals like maximizing growth rate [1] or carbon uptake rate [2] to describe various biological phenomena in bacterial and yeast systems, such as diauxic growth. The cybernetic goal for mammalian cells may not be based solely on survival or growth but on specific context dependent cellular responses.This work adapts the cybernetic framework to model the eicosanoid pathway— production of prostaglandins (COX-branch) and leukotrienes (LOX-branch) from arachidonic acid— in bone-marrow derived macrophage (BMDM) cells using the in vitro data across four different experimental scenarios. Several models of the eicosanoid pathway precede this work [3, 4], but none consider the regulatory phenomena [5]. Using a defined cybernetic goal to maximize system inflammation, we developed a weighted formulation of cybernetic control variables by correlating the lipidomic and transcriptomic data. The model effectively describes the experimental data and provides insights into the complex regulatory patterns present in the macrophage.Research Interests:  From the cell to whole animal model—Understanding dysregulation of systems at multiple levelsMy previous work has blended mathematical modeling with experimental techniques to develop an understanding of complex biological phenomena. A coupled approach to addressing biological phenomena and understanding how crucial process become dysregulated in certain disease states is required when developing therapeutics. Here, I highlight previous research initiatives.Novel isotope tracer method for quantifying glucose metabolism in type-2 diabetes Accurately assessing in vivo rates of glucose turnover, glycogen breakdown, gluconeogenesis, and futile cycling in the liver are crucial to understanding the control of whole-body glucose metabolism and how it becomes dysregulated in conditions such as diabetes and obesity. Currently, these rates are estimated using methods that do not account for the full complexity of intracellular biochemical reaction networks, and, therefore, often lead to inconsistent results. In prior work, we conducted metabolic flux analysis (MFA) studies using a unique formulation of 13C- and 2H-labeled glucose tracers and Gas Chromatography-Mass Spectrometry (GC-MS) profiling in order to understand the regulation of liver glucose fluxes in both fasted and fed Sprague-Dolly rats. By analyzing GC-MS labeling data of plasma glucose and lactate using our custom MFA software tools, we developed strategies to quantify whole-body glucose metabolism using an integrated experimental and computational approach. Single-cell-based investigation of HL60 differentiation using a microwell arrayThe average cell population response to a range of stimuli inaccurately reflects individual cell response due to heterogeneity in previously assumed homogeneous cell populations. It is hypothesized that this contributes to metastatic potential in tumors, with the most invasive cells being molecularly and behaviorally distinct from the bulk. In prior work, we designed and optimized a microwell platform using standard photolithography techniques for use in high throughput single cell studies to address heterogeneity in tumor populations. The acute myeloid leukemia cell line HL60 was used as a model uncommitted precursor cell line, and we began investigating population variance by inducing differentiation along the granulocytic lineage with all-trans retinoic acid.Future WorkI am especially interested in continuing to advance my understanding of the human system and the role inflammation plays in disrupting pathogen invasion in our bodies. As I look to pursue future research initiatives, I am excited at the opportunities available to further understand our collective understanding of what a healthy body looks like at the molecular level leading to the organ level and, ultimately, the system as a whole in order to optimize individuals’ health and well-being.",25,57.0
"Research Interests Past Research: As a junior student in my undergraduate program, I began my first research study on modeling and comparison of water desalination by the humidification-dehumidification and adsorption methods. My passion and interest in discovering new things, along with my hardworking, resulted in publishing a conference paper from my bachelor’s final project. My master project was focused on the heat transfer of SiO2/water nanofluid in jacket side of agitated vessels through experimental and numerical techniques. My research findings was published as two conference papers and one peer-reviewed article. I subsequently expanded my academic knowledge through collaboration with other scholars in conducting both experimental and numerical studies on heat exchangers and renewable energies. To this end, a series of studies were conducted on the hydrodynamic performance of a new wave energy converter called Searaser and evaluated its efficiency for use in the Caspian Sea.Current Research: I receieved a PhD addimision from the Department of Mechanical Engineering at Iowa State University. My Ph.D. project included an innovative method of lignocellulosic biomass deconstruction with an emphasis on lignin. I started to work on lignin depolymerization by oxygen to produce valuable phenolic monomers. Lignin oxidation is a promising method for lignin valorization. It can produce a range of functionalized chemicals of significant economic value such as aromatic acids, aromatic aldehydes, and aliphatic carboxylic acids. I designed a small batch reactor system to perform lignin oxidation tests. This system enabled us to demonstrate the non-catalytic depolymerization of native lignin to oxygenated phenolic monomers. Oxidation tests were performed in perfluorodecalin. Perfluorodecalin is a perfluorocarbon (PFC), characterized by their chemical stability and exceptionally high solubility for O2. High yiled of valuable phenolic monomers was achieved from lignin oxidation using perfluorodecalin. Moreover, the role of molecular oxygen was elucidated as both an oxidant to achieve oxidative depolymerization and a radical scavenger to mitigate the condensation of phenolic monomers. The lignin oxidation process was scaled up from 5 ml to 250 ml without losing yield. For scaling up the process, a stirred reactor was used. Since the reaction time is a key factor in oxidation tests, the reactor was modified.Our new batch system is a unique system with fast heating and cooling capability. My research findings on this topic have been presented in several well-known conferences, and a paper is currently under review in the Journal of Energy and Environmental Science .As the second phase of this project, I am currently working on oxidative depolymerization of lignin in a biphasic system. In this study, technical and native lignin sources are depolymerized in the presence of oxygen inside a biphasic system composed of solvents with high lignin solubility and perfluorodecalin of high oxygen solubility. We hypothesis that organic species formed by oxidative cleavage of lignin in perfluorodecalin can be extracted and stabilized into the organic phase with much lower oxygen concentration to improve the phenolic monomer yield. I am also working on lignin depolymerization and esterification using carboxylic acids to produce phenyl ester. The net carbon emissions from biodiesel could be decreased if methanol is replaced by a biobased reactant in the production of methyl esters. We hypothesize that lignin can be used in the production of phenyl esters. To this end, a series of tests on native and technical lignin have been performed. We would like to test both one-pot and two-stage esterification of lignin.Future Research: Lignin-first biorefinery: To retain the intrinsic value of virgin lignin, biorefineries will have to employ either mild depolymerization processes, such as ammonia-based fractionation and low-temperature organosolv techniques employ processes that continuously stabilize phenolic products as they are released from biomass. This second approach, sometimes referred to as the lignin-first strategy for deconstructing lignocellulose, is receiving increasing attention with reductive catalytic fractionation (RCF) and formaldehyde-assisted fractionation, as prominent examples. I intend to use my knowledge in lignin chemistry and depolymerization to introduce new methods of delignification to biorefineries to produce a more reactive, less condensed lignin.Lignin depolymerization to chemicals, fuels, and material: There are two different categories of products from lignin. The first category, which currently is the main application of lignin other than heat and power production, utilizes the polymeric structure of lignin to produce carbon nanofibers, polymer composites, adsorbents, and glues. Recently, the utilization of lignin to produce bio-based chemicals and fuels has received increasing attention. In this method, lignin is first depolymerized and then upgraded to a wide array of valuable products. There is still a long way to go to convert lignin to high-quality material and produce high yield of chemicals and fuels economically. Therefore, I intend to work on this area to improve the economic viability of biorefineries.Fast pyrolysis and solvent liquefaction of biomass: Fast pyrolysis of biomass is a simple and robust thermochemical technology for converting biomass into bio-oil, syngas, and biochar. Solvent liquefaction has been receiving increasing attention and has the potential of becoming a robust and economical method of processing biomass . I would like to focus on both of these methods in my future research plan. Both of these methods seem promising ways to produce bio-oil from lignocellulosic biomass.AbstractGlobal energy consumption has significantly increased in the last 40 years and approximately 80% of this demand is supported by fossil fuels. Environmental and human health concerns regarding fossil fuels consumption has encouraged researchers to find sustainable alternatives for the current petrochemical industry. Within this context, biodiesel production has received increasing attention in recent years. Biodiesel is a form of diesel fuel which is composed of esters formed by chemically reacting alcohols and long chain carboxylic acids. Lignin as the only component of biomass containing aromatic structure is a great precursor for producing chemicals and fuels. Interestingly, phenols can be esterified by carboxylic acids to produce esters. We hypothesize that lignin can be used as a potential precursor for biodiesel production. Phenolic products obtained from lignin depolymerization could be esterified with a carboxylic acid to produce phenyl esters. In order to test our hypothesis, technical and native lignin sources were applied as phenolic source and carboxylic acids like acetic acid and hexanoic acid were used as acetylating agents. Both in-situ and ex-situ esterification of lignin were studied. In the in-situ esterification process, lignin was depolymerized and esterified in one pot, while in the ex-situ esterification process, lignin was first depolymerized, and then the obtained lignin-oil was esterified. Lignin-oil was further characterized by GC-FID, FTIR and 2D HSQC NMR spectroscopy.",25,58.0
"Research Interests Modular chemical systems for advanced manufacturing have attracted increased interest in recent years due to their ability to employ process intensification, as well as their potential for increased flexibility, and productivity (e.g., see [1,2] and citations therein). This interest is particularly prominent in the chemicals, pharmaceuticals, biotechnology, and energy industries (e.g., [3-5]). This Ph.D. thesis addresses modular chemical systems from a process optimization and control perspective that enables automation. Modular chemical systems are advanced manufacturing systems with characteristics such as (1) dynamics described by combinations of algebraic, ordinary differential, partial differential, and integral equations, (2) high to infinite state dimension, (3) time delays, (4) actuator, state, and output constraints, (5) stochastic noise and disturbances, (6) parametric uncertainty. Most methodologies are developed to handle dynamical systems with a subset of these characteristics and a focus on the specifics of the process.However, to promote efficiency in industrial practice, autonomous design of optimization and control for modular chemical systems, connected in a plug-and-play manner, should be pursued. Such control and optimization systems are mainly conducted through a computer with little to no supervision. In the work conducted in the context of this Ph.D. thesis, we describe steps taken towards such automation, through modeling, control algorithms and optimization approaches tailored to address the characteristics and challenges of such systems. Such steps include (1) automating the construction of dynamic first-principles plantwide models obtained by plug-and-play interconnection of modules that are modeled in advance; (2) automating the hierarchical plant-wide control; (3) developing methodologies to expand the range of applicability of linear input-output models in linear model predictive control, to address the high state dimension of modular systems with low on-line computational cost [6]; (4) addressing the optimization of transient periods (i.e. startup) to minimize waste and increase efficient operation times, through hybrid dynamic optimization and linear model predictive control schemes [7]; (5) creating feedback controllers with guaranteed properties for dynamic neural network models, as an alternative to first-principles model control [8]. This Ph.D. work builds upon recent publications that propose ways to modify linear model predictive control formulations to handle significant process nonlinearities [6], with applications that extend to process startup [7]. It also demonstrates progress in designing controllers with guaranteed properties for dynamic artificial neural networks [8], which can be used in the place of first-principle models, providing an alternative to nonlinear model predictive control, which usually is a nonconvex problem that requires the solution of an NLP online, with unknown feasibility and performance properties.Interests I developed during my thesis work are related to systems modeling, simulation, and control. My current research interests include, but are not limited to, first principles modeling, and simulation, empirical modeling of dynamical systems using data, surrogate modeling, dynamic optimization, model predictive control of high dimensional systems, and autonomous design of advanced control systems. I am also interested in expanding the above methodologies to address uncertainties in systems, such as parametric uncertainties and model-plant mismatch. My research interests find applications in numerous fields, such as in the (bio)pharmaceutical industry which is rapidly moving towards continuous processing, in the chemical industry where modeling and control are always relevant, as well as in the energy industry, with the evolving subfield of battery modeling and control.References-C. Bédard, A. Adamo, K. C. Aroh, M. G. Russell, A. A. Bedermann, J. Torosian, B. Yue, K. F. Jensen, and T. F. Jamison. Reconfigurable system for automated optimization of diverse chemical reactions. Science 361(6408):1220-1225, 2018.A. Paulson, E. Harinath, L. C. Foguth, and R. D. Braatz. Control and systems theory for advanced manufacturing. In Emerging Applications of Control and System Theory, edited by Roberto Tempo, Stephen Yurkovich, and Pradeep Misra, Lecture Notes in Control and Information Sciences, Springer Verlag, Chapter 5, 63-80, 2018.Lakerveld, P. L. Heider, K. D. Jensen, R. D. Braatz, K. F. Jensen, A. S. Myerson, and B. L. Trout. End-to-end continuous manufacturing: Integration of unit operations. In Continuous Manufacturing of Pharmaceuticals, edited by P. Kleinebudde, J. Khinnast, and J. Rantanen, Wiley, New York, Chapter 13, pages 447-483, 2017.T. Myerson, M. Krumme, M. Nasr, H. Thomas, and R. D. Braatz. Control systems engineering in continuous pharmaceutical processing. Journal of Pharmaceutical Sciences, 104(3):832-839, 2015.S. Hong, K. A. Severson, M. Jiang, A. E. Lu, J. C. Love, and R. D. Braatz. Challenges and opportunities in biopharmaceutical manufacturing control. Computers & Chemical Engineering, 110:106-114, 2018.Nikolakopoulou, M. von Andrian, and R. D. Braatz. Plantwide control of a compact modular reconfigurable system for continuous-flow pharmaceutical manufacturing. 2019 American Control Conference (ACC), Philadelphia, PA, USA, pp. 2158-2163.Nikolakopoulou, M. von Andrian, and R. D. Braatz. Fast model predictive control of startup of a compact modular reconfigurable system for continuous-flow pharmaceutical manufacturing. 2020 American Control Conference (ACC), Denver, CO, USA, in press.Nikolakopoulou, M. S. Hong, and R. D. Braatz. Output feedback control and dynamic artificial neural networks using linear matrix inequalities. Submitted.",25,59.0
"Research Interests:  Design chemical reactors1. INTRODUCTION.Design a chemical reactor is a complex process since it must allow flexible operation in the ranges of temperature and pressure required by the chemical process that is to be developed. Nature of the substances involved, the phases present, the kinetics and thermodynamics of the process are considered. Especially when it comes to photocatalytic reactors since radiation plays a leading role in the reaction process these are aspects of high relevance for the selection of materials, type of operation, fluid dynamics and geometry1,2.Commercially, we can find reactors built in different materials such as steels, sapphire, borosilicate, PVC, some of these designed to withstand high pressures, others allow lighting inside the reactor, and another group operates in heterogeneous systems with suspended catalysts. However, having these three characteristics in a single device requires a rigorous mechanical analysis and some modifications in the commercial designs of high-pressure reactors3. Due to the expansion that heterogeneous photocatalysis has had in recent decades due to the variety of chemical processes that can be performed. Such as selective oxidation reactions using atmospheric oxygen4,5, where there is a great interest in replacing organic solvents with supercritical fluids, especially CO2. These high pressure photoreactors are required4. Since the reactors for heterogeneous photocatalysis using scCO2 as a solvent are few and often use immobilized acid catalysts. In this work, a photoreactor was designed to allow processes of transfer of oxygen atoms to olefins using scCO2 as a solvent and a dioxo-molybdenum complex supported on TiO2 as a catalyst, activated by UV-Vis radiation6.2. METHODOLOGY.In Fig. 1, the primary considerations for the selection of materials, geometry and couplings for the design of the batch photoreactor are shown. The nature of the substances, the reactions and the operating conditions to which this equipment is subjected. The design rules for high pressure vessels presented in ASME VIII code were used to establish the mechanical requirements for the construction of the high-pressure reactor. These data were simulated and adjusted using the finite element method using the CAD-type software, SolidWork, the main equations used are shown in Table 1.3. RESULTS.Fig. 2 shows the scheme of the photoreactor designed to perform supercritical CO2 processes that require lighting inside it. It was decided to make a container with a rectangular external geometry with a cylindrical cavity where the different processes would be carried out, the whole body is designed to be constructed in 316SS and the volume is 300 mL.",25,60.0
"Research Interests:  Photopolymerization, fiber spinning, polymer processing, 3D printingMicron-sized nonwovens serve a diverse range of applications such as medical textiles, geotextiles, filtration media, etc. Current methods for producing synthetic nonwovens rely on transforming pre-formed thermoplastic polymers by heating or adding organic solvents to facilitate drawing, which respectively results in their high energy demands and concerns about volatile solvent emissions. Furthermore, conventional synthetic fiber manufacturing is limited to thermoplastics since cross-linked thermosets do not flow like liquids. However, cross-linked (thermoset) fibers are attractive targets due to their superior thermal and chemical resistances. Herein, we describe a “cure blowing” process that addresses these issues by producing cross-linked fibers at room temperature with little or no volatile solvent. In this process, a photocurable liquid mixture of thiol and acrylate monomers was extruded through a spinning die resembling those used for commercial melt blowing and drawn into liquid filaments by high-velocity, ambient temperature air jets. The liquid filaments were cross-linked in flight into solid fibers by in-situ photopolymerization. Studies of the effects of process parameters on the fiber diameter and morphology revealed two inherent process limits: (1) a kinetic limit dictated by rate of photopolymerization relative to fiber fight time, and (2) a stability limit dictated by the surface tension and viscoelasticity of the monomer mixture. These limits manifest as fibers that are fused at their points of contact or fibers that possess surface undulations and non-uniform diameters, respectively. Several strategies to suppress or delay these limits were also identified and demonstrated experimentally to expand the cure blowing process window. Furthermore, we observed that the various parameters involved in cure blowing govern the final fiber characteristics in a convoluted and inter-dependent manner. A comprehensive analysis in terms of a few characteristic timescales corresponding to the different competing features of the process was also performed. This analysis led to the development of an universal operating diagram for cure blowing. This diagram can be used as a predictive process design tool where all the parameters of a new reactive system can be tuned a priori of fiber production. This will essentially eliminate the need to determine the appropriate process conditions through many trial-and-error experiments. These studies suggest the industrial relevance of this new cure blowing technique for producing cross-linked synthetic nonwovens for a wide range of applications.",25,61.0
"Magnetic characterization is of much importance in the fields of cell labeling, cell purification, cell physiology, endocytosis, biochemical micro assays, endosome research, cell separation, drug targeting, and in vivo diagnostics. The HyperfluxTM velocimeter is utilized to measure magnetophoretic mobility, size and other morphological parameters of magnetic particles and labeled cells by particle tracking velocimetry. Magnetic cytometry by velocimetry records the motion of labeled cells in an isodynamic magnetic field thereby estimating the key parameter, magnetophoretic mobility of labeled cells. The calibration capability of the instrument has been extended in order to estimate the actual particle size and thereby estimating the intrinsic magnetic properties of several commercial beads. The optical density method and chain velocity method has been explored to estimate the magnetophoretic mobility of a single bead of nanoparticle chains. The rapid estimation of magnetophoretic mobility by the instrument and collection of multiple thousand data points facilitates cellular uptake quantification and kinetic studies in less time than any other existing technique. The receptor-independent uptake by cultured CHO (Chinese Hamster Ovary) cells of 100 nm iron oxide nanoparticles with different surface coatings, namely starch, amino groups, and polyethylene glycol (PEG 2K Daltons), was studied to reveal the role of nanoparticle endocytosis mechanisms. Caveolae-mediated and clathrin-coated endocytosis are revealed by using specific mechanism-based inhibitors. The cellular survival rate, toxicity, and uptake of nanoparticles during endocytosis by different mechanisms with the different surface coatings have been investigated. This research facilitates the rapid estimation of intrinsic magnetic properties, the optimization of MNP coatings, a better understanding of cell labeling, and different cellular endocytosis mechanisms.Research Interests:  magnetic cytometry, biomedical research, cell biology, semiconductor development, process intensification",25,62.0
"Theoretical and computational understanding of crystallization processes can significantly replace the trial-and-error methods in place for crystallization at different pharmaceutical, chemical and fertilizer industries. However, the self-assembly of molecules during the process of crystallization spans multiple timescales which makes full computational modeling of crystallization process difficult. Recently, with the help of molecular simulations, it was shown that dynamics of solvation shell determines the crystal structure and growth rates. However, more appropriate computational approach is required for calculation of rate constants for nucleation and growth of crystals. In this study, stochastic simulations of glutamic acid molecules were performed. The molecular features of glutamic acid were approximated using the already established models to retain its molecular feature while reducing the computational time. Such approximation allows simulating higher number of glutamic acid molecules in a simulation box for longer times. Findings of this study yield in rate constants for glutamic acid crystallization which reasonably agree with experimental observations. The approach also shows potential to be applied to other crystallization systems.",26,0.0
"The mineralization of sparingly soluble compounds in process equipment may result in corrosion, equipment damage, and cause reduction or loss of finished product for oil and gas production, wastewater treatment, and manufacturing processes. Conventional treatments rely on the use of caustic solutions that pose a negative environmental impact. Designing effective biodegradable chemical treatments to reduce or eliminate scale formation requires an understanding of the molecular-scale interactions of chemical additives in supersaturated environments at the crystal interface. Here we implement a cooperative approach to design and test environmentally friendly chemical treatments for scale formation using a combination of surface science techniques and microfluidic technology. Crystallization can be suppressed in highly supersaturated solutions using dilute quantities of modifiers. These studies focus on barium sulfate (barite), a highly insoluble and common scaling mineral. In this presentation we will describe the effect of organic acids on barite crystallization in bulk assays under flow and in quiescent conditions. Using in situ scanning probe microscopy, we explore the interfacial interactions between the modifiers and different surfaces of barite over a range of supersaturated conditions to elucidate the role of these inhibitors during crystal growth. Our findings identify unique crystal-modifier interactions that suppress barite crystallization via an irreversible inhibition mechanism. Collectively, these studies provide new fundamental understanding of crystal growth modifier action towards the design of improved scale treatments.",26,1.0
"A method for the solidification of metallic alloys involving spiral self‐organization is presented as a new strategy for producing large‐area chiral patterns with emergent structural and optical properties, with attention to the underlying mechanism and dynamics. This study reports the discovery of a new growth mode for metastable, two‐phase spiral patterns from a liquid metal. Crystallization proceeds via a non‐classical, two‐step pathway consisting of the initial formation of a polytetrahedral seed crystal, followed by ordering of two solid phases that nucleate heterogeneously on the seed and grow in a strongly coupled fashion. Crystallographic defects within the seed provide a template for spiral self‐organization. These observations demonstrate the ubiquity of defect‐mediated growth in multi‐phase materials and establish a pathway toward bottom‐up synthesis of chiral materials with an inter‐phase spacing comparable to the wavelength of infrared light. Given that liquids often possess polytetrahedral short‐range order, our results are applicable to many systems undergoing multi‐step crystallization.",26,2.0
"Crystallization is commonly used in industrial processes to convert solute molecules dissolved in solvent to a structured solid state. Pharmaceutical companies often crystallize APIs in the form of organic molecules to selectively formulate specific crystal habits for optimal bioperformance. Crystal engineering is also of importance for developing catalysts with tailored surfaces to maximize active sites. Furthermore, tuning crystallization is desirable for varying electrical and optical properties in the field of electronic materials and for altering the impact sensitivity of energetic materials. Given the ubiquity of crystal growth in industrial processes, there is substantial demand for predictive and mechanistic modelling of crystallization. Crystallization of organic molecules is well understood for ideal systems (i.e., Kossel crystals with a single centrosymmetric growth unit). There is interest in studying crystal systems in which non-idealities are introduced, as these are more representative of realistic conditions. One such non-ideality involves the presence of impurities or ‘imposter molecules.’ There are numerous notable examples of crystal growth systems in which the presence of impurities significantly influences crystallization processes. Examples include “antifreeze” proteins and antimalarial medications. While these systems are prevalent and of importance in many fields, there is currently no comprehensive model that captures the full effect of impurities on crystal growth processes.The goal of this research is to investigate the effect of impurities on the crystal growth process and to develop theoretical models for the mechanisms by which impurities influence crystal growth and hence affect crystal morphology and size. Impurities affect growth kinetics at the scale of kink attachment and detachment events, which are too fine to examine experimentally in real time. Thus, we use simulations to study the proposed mechanisms for growth inhibition. We employ Kinetic Monte Carlo (KMC) to simulate the time evolution of centrosymmetric organic crystal growth. Various mechanisms have been proposed to explain the growth-inhibiting effect of impurities, such as Cabrera and Vermilyea’s step-pinning model and Doherty and Sizemore’s spiral-pinning models. Model equations have been developed in accordance with these mechanisms to quantify the effect of inhibition on the crystal face. We use KMC simulations to examine the limits of these equations under different growth regimes and to elucidate the form of a more general model. The simulations serve to quantify the degree of growth inhibition of a simple crystal growth system populated with impurities. The results suggest a monotonic decrease of the step velocity with increasing impurity concentration, though the form of this dependence is based on the nature of the impurity distribution. Furthermore, the simulations suggest a critical impurity coverage at which crystal growth is halted completely. ",26,3.0
"Metal organic frameworks (MOFs) are crystalline materials which are well-known due to their applications in separations, catalysis, and chemical capture, among others, brought forth by their large surface areas and high porosity. These materials are structurally versatile as they are composed of metal cluster and organic ligand pairs, which can be strategically switched to make MOFs with certain functionalities. A subset of MOFs with a “rod-packing” motif (“rod-MOFs) are promising as breakthrough materials for size-selective, passive chemical separation processes due to their fine unidirectional channels. The main challenge for these applications, however, rests on the optimization of the crystallization process of these MOFs to make them larger than their conventional micron scales and with negligible intergrowths.Crystallization through the hydrothermal method is a common process used to synthesize certain classes of Metal organic frameworks (MOFs), including MIL-53, which falls under the rod-MOF classification. Though the process is ubiquitous to the formation of a variety of crystals, control of the dynamics of the growth process itself remains a challenge as the closed nature of the batch system typically used for these reactions is reminiscent of a “black box”. A wealth of literature exists on the exploration of crystal growth dynamics through Monte Carlo (MC) and Molecular Dynamics (MD) simulations, as well as powerful models which incorporate both. However, these models have seldomly been applied to investigate the crystal growth kinetics of MOFs, and more specifically, the MOF crystallization phenomena for special cases where the inputs to the system are changing periodically.Herein we apply our understanding of the physics of crystallization, statistics and well established kinetic Monte Carlo algorithms (kMC) based on the solid-on-solid (SOS) model and Metropolis MC to develop a deeper insight on MOF crystallization phenomena under hydrothermal conditions.",26,4.0
"The combination of amino acids into peptides and proteins, through peptide bonds, are the building blocks of life on earth. The last decade has seen a significant increase in the use of these materials in the treatment of chronic and metabolic diseases (e.g. cancer, obesity and diabetes).1-2 However, the general peptides product process still has severe problems, like low solubility, proteolytic degradation and physiochemical instability.3 Peptide crystallisation, as a good alternative, can help us get the peptide structure and get ideal physicochemical products. Water as a most popular solvent used in peptide and protein crystallisation will not only support the bioactivity of these materials, but also can stabilise the more complex molecular conformations that arise from long chain biomolecules, through satisfying the multiple hydrogen bonding sites of these materials.4-5 Solving how the water molecule effect the peptides crystal structure can help us to uncover the details of the peptide conformation and waters role in their crystallisation, giving insight into their roles in the crystallisation of highly hydrated protein structures.Glycine is an important and well-studied amino acid, being abundant in proteins and utilized as a pharmaceutical excipient. Its simple hydrogen side chain makes this an ideal case study candidate for biopharmaceutical research.6 Here we investigate the crystallisation of pure and hydrated forms of mono-, di- and tri-glycine. However, only mono- and tri-glycine are known to crystallise in hydrated forms. The crystallisation of mono, di- and tri-glycine from aqueous solutions was compared in this research. The single crystal structure of triglycine dihydrate (TGDH) form was solved for the first time, which can help us to analysis the interaction between water and peptide molecules. For Glycine homopeptides, crystal forms of up to five glycine residues in a peptide chain are known, and the antiparallel β sheet structure was found is the most stable crystalline form for triglycine anhydrate. However, from the crystal structure of triglycine dihydrate, the backbone of it trends to bend in water to find a more stable form. The activation free energy (△Gc) of nucleation was calculated based on induction time measurement at different supersaturation and different temperature. Compare with mono- and di-glycine, triglycine has the greatest decrease in △Gc at 283.15K when the form is dihydrate. Molecular modelling was also used to investigate the delicate balance of conformation and packing forces, as a function of chain length. In summary, we solved the first TGDH crystal structure and explored the water role in the hydration conformation of peptides. The modeling work help us to find several competing effects which are driving TGDH. It also gives protein crystallisation a new insight about the interaction between water and protein, this discovery has an important implication for pharmaceutical engineering and biology research.Reference Cicero, A. F. G.; Fogacci, F.; Colletti, A., Potential role of bioactive peptides in prevention and treatment of chronic diseases: a narrative review. Br J Pharmacol 2017, 174 (11), 1378-1394. Marqus, S.; Pirogova, E.; Piva, T. J., Evaluation of the use of therapeutic peptides for cancer treatment. J Biomed Sci 2017, 24 (1), 21. Otvos, L., Jr.; Wade, J. D., Current challenges in peptide-based drug discovery. Front Chem 2014, 2, 62. Scoppola, E.; Sodo, A.; McLain, S. E.; Ricci, M. A.; Bruni, F., Water-peptide site-specific interactions: a structural study on the hydration of glutathione. Biophys J 2014, 106 (8), 1701-9. Wyttenbach, T.; Liu, D.; Bowers, M. T., Hydration of small peptides. International Journal of Mass Spectrometry 2005, 240 (3), 221-232. Koleva, B. B.; Kolev, T. M.; Spiteller, M., Structural and spectroscopic analysis of hydrogensquarates of glycine-containing tripeptides. Biopolymers 2006, 83 (5), 498-507.",26,5.0
"Historical perspective of data in pharmaceutical industry has always been the by-product of scientific experiments and business processes required to maintain for regulatory reasons. With the rapid advancements in the field of machine learning (ML) and artificial intelligence (AI), there is a growing view of data as the new oil that could be mined to make quality decisions early and accelerate programs to make life-saving drug reach patients much faster.To get to ML/AI, we need to build a robust data and information infrastructure which by design allows for these end use cases effortlessly. In this regard, a widely accepted philosophy is FAIRization of data which means data should be Findable, Accessible, Interoperable and Reusable. By making data FAIR, one can pass two litmus tests: a) data can be consumed without the presence of data owner and b) data is machine actionable.In this talk, we will present vision and strategies that could help in making our laboratory and plant data FAIR and guide efficient process development and robust manufacturing across pipeline projects in pharmaceutical R&D space. We will talk about challenges, pitfalls and learning in this journey. We will also present the case-study of our in-house built platform that makes research data FAIR in the backend while making scientist’s data journey simple in the frontend.Disclosures: All authors are AbbVie employees and may own AbbVie stock. AbbVie sponsored and funded the study; contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final abstract.",27,0.0
"Over the past two and a half years, Bristol Myers Squibb (BMS) has shifted from internal scale up and production of clinical small molecule Drug Substance, to external supply from contract manufacturing organizations (CMO`s). This paradigm shift has created a need for BMS to develop new strategies to measure, understand, and manage the performance of our technology transfer & scale up. Our team has developed a culture of performance data collection, gathered across our network of vendors for each project, to better understand performance metrics, such as yield variability, quality variability, key cost drivers, and supply delays. The aggregated, contextualized data enables the creation of fit-for-purpose analysis and reports to support future site selection decisions, vendor management, and internal continuous improvement efforts. Predictive models for business drivers such as project cost and vendor capacity can also be created from the aggregated data. The predictive models can then be used to drive tactical and strategic decisions across the vendor network and project portfolio. These data-driven products are used for multiple business purposes, including high-level performance metrics for API delivery, targeted feedback to vendors, budget estimation, and to support individual strategic decisions around network capacity and capabilities. This presentation will show examples of the workflow used to capture this data, build data-analysis products, and drive decisions and continuous improvement. In particular, we will describe how we measure vendor performance against various business metrics and provide data-driven feedback to partners. We will also cover the use of probabilistic models and Monte-Carlo simulation to understand future expected demand on our supplier network.",27,1.0
"Biopharmaceutical processes are prone to many challenges including highly nonlinear dynamics, batch-to-batch variability and high sensitivity to changes in operating parameters. Such complexities can introduce heterogeneity in biopharmaceutical products. At the same time, consistent product quality remains a top priority for this industry to ensure patient safety. Antibody-producing platforms consist of two main operations: upstream (USP) and downstream (DSP) production. In USP cells are inoculated into a series of progressively larger bioreactors for the growth of mammalian cells, typically Chinese hamster ovary (CHO) cells in the case of monoclonal antibodies (mAbs). These are then transferred to the production bioreactor for protein expression. At the end of this step DSP begins with a series of filtration and purification stages to remove impurities and other unwanted components from the final drug formulation.The performance of such processes is assessed both for productivity and product quality. The latter is a broad concept that includes various aspects, amongst which product functionality. In mAb production, glycosylation percentage is one of the key indicators to measure cell culture quality performance. Glycosylation is a post-translational modification of mAbs that affects their stability and efficacy as therapeutics. As such, it is considered a Critical Quality Attribute (CQAs) under the Quality by Design (QbD) framework and needs to be controlled within a pre-specified range [1]. There have been various studies on the improvement of mAb glycosylation intervening both at process- [2], [3] and genetic- level [4], [5]. However, the complexity of the system and its underlying interactions make process optimisation difficult, with optimality unlikely to be achieved solely by experimental studies. This is because strategies to improve mAb glycosylation can enhance mAb glycosylation but, at the same time, reduce cell growth and, thus, mAb concentration at harvest.In this work we use an Ordinary Differential Equation (ODE) model as digital-twin for an IgG-producing CHO cell culture system [6]. The model [7] comprises three main components; namely an unstructured model to describe cell growth, a reduced kinetics model for the intracellular nucleotide sugar donor (NSD) synthesis [8]and a mAb glycosylation model. The mAb glycosylation model as considered in this work, comprises four continuous stirred tank reactors (CSTRs) using the reaction network presented in del Val et al.[9]. We formulate and solve an exhaustive dynamic optimisation experiment to identify feeding regimes for galactose and uridine under which the amount of galactosylated antibody is maximised. The performance of every optimisation formulation is assessed based on: (a) achieved galactosylation percentage, (b) productivity, (c) culture viability and (d) experimental applicability. Towards an effort to enhance USP automation, measured process disturbances are introduced in the problem formulation. In this respect, the most successful scenarios are used as a basis for the design of a real time dynamic optimisation strategy as a means to achieve the optimal operational policy [10]. The system is re-optimised online and based on the effect of the disturbances to the process, adapts and calculates the optimal input trajectories. In this work these are validated in closed-loop against the process model.References[1] A. Eon-Duval, H. Broly, and R. Gleixner, “Quality attributes of recombinant therapeutic proteins: An assessment of impact on safety and efficacy as part of a quality by design development approach,” Biotechnol. Prog., vol. 28, no. 3, pp. 608–622, May 2012.[2] R. K. Grainger and D. C. James, “CHO cell line specific prediction and control of recombinant monoclonal antibody N -glycosylation,” Biotechnol. Bioeng., vol. 110, no. 11, pp. 2970–2983, Nov. 2013.[3] M. J. Gramer et al., “Modulation of antibody galactosylation through feeding of uridine, manganese chloride, and galactose,” Biotechnol. Bioeng., vol. 108, no. 7, pp. 1591–1602, Jul. 2011.[4] C. Chung, Q. Wang, S. Yang, B. Yin, H. Zhang, and M. Betenbaugh, “Integrated Genome and Protein Editing Swaps α -2,6 Sialylation for α -2,3 Sialic Acid on Recombinant Antibodies from CHO,” Biotechnol. J., vol. 12, no. 2, p. 1600502, Feb. 2017.[5] N. Yamane-Ohnuki et al., “Establishment ofFUT8 knockout Chinese hamster ovary cells: An ideal host cell line for producing completely defucosylated antibodies with enhanced antibody-dependent cellular cytotoxicity,” Biotechnol. Bioeng., vol. 87, no. 5, pp. 614–622, Sep. 2004.[6] P. Kotidis et al., “Constrained global sensitivity analysis for bioprocess design space identification,” Comput. Chem. Eng., vol. 125, pp. 558–568, Jun. 2019.[7] P. Kotidis et al., “Model‐based optimization of antibody galactosylation in CHO cell culture,” Biotechnol. Bioeng., vol. 116, no. 7, pp. 1612–1626, Jul. 2019.[8] S. N. Sou, P. M. Jedrzejewski, K. Lee, C. Sellick, K. M. Polizzi, and C. Kontoravdi, “Model-based investigation of intracellular processes determining antibody Fc-glycosylation under mild hypothermia,” Biotechnol. Bioeng., vol. 114, no. 7, pp. 1570–1582, Jul. 2017.[9] I. Jimenez del Val, J. M. Nagy, and C. Kontoravdi, “A dynamic mathematical model for monoclonal antibody N-linked glycosylation and nucleotide sugar donor transport within a maturing Golgi apparatus,” Biotechnol. Prog., vol. 27, no. 6, pp. 1730–1743, Nov. 2011.[10] J. V. Kadam, M. Schlegel, B. Srinivasan, D. Bonvin, and W. Marquardt, “Dynamic optimization in the presence of uncertainty: From off-line nominal solution to measurement-based implementation,” J. Process Control, vol. 17, no. 5 SPEC. ISS., pp. 389–398, Jun. 2007. ",27,2.0
"Condition-based maintenance in pharmaceutical processes requires the continuous monitoring of process parameters and asset performance features to enable predictions of when corrective operational or maintenance action would prove beneficial. Although this data does reflect the current-condition of the process and the equipment/facility, it is never sufficient to provide the complete picture, particularly, of the future state.1 This missing information, coupled with the variability in sensor data, contributes to uncertainty that needs to be explicitly quantified and incorporated into predictive models. Furthermore, identifying parameters and extracting features that best represent the state of the process require the use of training data,2 which may or may not completely represent the state of the process under normal operating conditions or under recurring abnormal conditions.Even with the abundance of data brought about by the digitalization of processes, uncertainty is still prevalent. Aside from the inability to directly measure the condition of a process, the variability of operating parameters, e.g. raw materials, environment, personnel tendencies, can directly affect a process and hence affect the accuracy of the process model. The explicit expression of uncertainty, of predictions and beliefs deduced from a model, may be reduced to calculation of the probability of that prediction/belief. With condition monitoring, this means using the data to estimate the probability of the condition of the system, and then updating these estimates to reflect new data in real time. Bayes’ theorem fits well into this paradigm, since it provides a mechanism to correctly update probabilities (as a measure of uncertainty) in light of new data, hopefully reducing uncertainty in predictions.3In this work, we demonstrate the use of Bayesian methods and the advantages of implementing condition monitoring under a probabilistic framework. We illustrate this approach using cases drawn from the operation of the Continuous Pharmaceutical Tableting Pilot Plant at Purdue University, which is a state-of-the-art high-bay facility that can be configured to produce tablets via direct compaction, dry granulation, or wet granulation. Moreover, the leading software for real-time manufacturing management would be used for these cases, particularly PI System (OSIsoft) and SmartFactory Rx (Applied Materials). The former will be mainly the data historian4 while the latter provides an environment for extracting the data from the historian and processing it for the purposes of operations productivity, analytics and control, knowledge management, and predictive maintenance.5 This environment would be ideal for implementing Bayesian methods for condition monitoring.1 R. Flage, D.W. Coit, J.T. Luxhøj, and T. Aven, Reliability Engineering & System Safety 102, 16 (2012).2 C.M. Bishop, Pattern Recognition and Machine Learning (Springer, 2016).3 O. Martin, Bayesian Analysis with Python: Introduction to Statistical Modeling and Probabilistic Programming Using PyMC3 and ArviZ, 2nd Edition (Packt Publishing Ltd, 2018).4 OSIsoft, OSIsoft PI System (n.d.).5 Applied Materials, Applied Materials Automation Software (n.d.).",27,3.0
"Currently, Industry 4.0 concepts are being applied to pharma industry to achieve Pharma 4.0 paradigm. Pharma 4.0 reduces the time and resources needed for continuous pharmaceutical manufacturing and also improves the product quality and production consistency. It has many advantages but also have bigger challenges on the advanced process control and cyber-physical security side since the process and CQA’s are not only needed to controlled but also need to be protected from any vulnerability, in real time [1-4]. The quality of the pharmaceutical products can be improved significantly via implementing the advanced model predictive control (MPC) system coupled with an RTD based control system if an appropriate cyber-physical security defense is in place. However, much less attention has been paid to implement the integrated control and cyber-physical security system into the continuous pharmaceutical manufacturing plant.In this work, an advanced model predictive control (MPC) system coupled with an RTD based control system has been implemented in the continuous pharmaceutical manufacturing (CPM) pilot-plant. The CPP’s and CQA’s are controlled in real time using advanced model predictive control (MPC) system while the none-confirming products are diverted in real time in waste to assure the final CQA’s of qualified tablet lots. This bi-layer coupled control strategy assures the final product quality, improves the production efficiency, minimizes the need of off line testing, and facilitated the real time release. The critical control variables that have been controlled in first layer using model predictive control (MPC) system are drug concertation, powder level before tablet press, main and pre compression forces, tablet weight and hardness. A novel control strategy for powder level control in a chute placed in between blender and tablet press unit operation of continuous tablet manufacturing process has been developed, implemented and evaluated. A noninvasive technique based on change in electric field concept has been used for real time monitoring of powder level in continuous manufacturing pilot-plant. The APC has been used to control feeder, blender and tablet press as well [1]. A systematic framework including the methods and tools for real time diversion of tablets have been developed and implemented into continuous pharmaceutical manufacturing process as a second layer assurance of product quality. In CM, the drug concentration is measured in real time before the tablet compaction (chute &/or feed frame) using PAT sensor. The proposed control strategy then uses this inlet concentration to determine a signal for the diversion strategy that can accurately be used to reject tablets that are out of tolerance limits at the outlet of the tablet press. It is also providing a suitable platform to handle the pandemics such as Covid-19 via rapid production of suitable drugs and its real time release, with less time and resources.A systematic framework including the methods and tools have been also developed for proactive identification and mitigation of potential cyber-physical attack risk on CPM. The cyber-physical security relevant software tools such as Snap 7, Wireshark, and Tripwire have been applied to CPM. A novel software tool named CPS (Cyber-Physical Security) has been developed for cyber-physical security of the continuous pharmaceutical manufacturing. The integrated commercially available and developed (in house) cyber-physical security tools have added an extra layer of security of our continuous pharmaceutical manufacturing pilot-plant for any unexpected attacks. All the relevant data generated during continuous manufacturing has been systematically collected, stored and organized in a data hub (OSI PI) and cloud system as per industry 4.0 standard.The objective of this presentation is to demonstrate the performance of integrated MPC and RTD based bi-layer control strategy together with cyber-physical security system implemented into our continuous pharmaceutical manufacturing pilot-plant facility.ReferencesBhaskar, A., Barros, F. N., Singh, R. (2017). Development and implementation of an advanced model predictive control system into continuous pharmaceutical tablet compaction process. International Journal of Pharmaceutics, 534 (1-2), 159-178.Bhaskar, A., Singh, R. (2018). Residence time distribution (RTD) based control system for continuous pharmaceutical manufacturing process. Journal of Pharmaceutical Innovation. DOI: 10.1007/s12247-018-9356-7.Singh, R. (2019). Systematic framework for implementation of RTD based control system into continuous pharmaceutical manufacturing pilot-plant. Pharma. Issue 34, 43-46.Singh, R. (2020). The cyber-physical security of pharmaceutical manufacturing processes. Pharma., Issue 38, 53-57.",27,4.0
"In recent years, mechanistic models have been applied to a broad range of applications in pharmaceutical R&D and Engineering, improving efficiency through reduced data requirements, facilitating scale-up and tech transfer, and enabling virtual design space exploration. Despite their successes in these areas, mechanistic models have typically been utilized by a small group of modeling experts, who are not necessarily the key stakeholders in the answers that the models provide. As a result, scientists and engineers who are not themselves modelers have limited access to these tools and are less likely to take advantage of their full potential. Some common barriers to adoption include a lack of training, support and know-how, a high amount of effort required to establish a fit-for-purpose model-based solution, and a limited access to software for installation. Some recent efforts in the modeling community have aimed at “democratizing” models – reducing the barriers to adoption for those without modeling expertise. Web-based deployment of simple modeling tools is proposed as a potential solution to this problem.In this work, model deployment for pharmaceutical applications to scientists and engineers will be discussed in the context of two case studies performed at Eli Lilly: a spray drying operation and a batch distillation and collection process. The modelling workflow requirements of scientists and engineers without prior modeling experience will be compared with those of modeling experts. For each application, a fit-for-purpose model was developed and deployed as a simple web interface to answer targeted questions of interest to scientists and engineers. Outcomes of the deployment exercise will be presented, and future perspectives on democratization of mechanistic models for pharmaceutical applications will be discussed.",27,5.0
"Real-time modeling of Crystal Nucleation, Growth, and Breakage in a Agitated Bioreactor We use GPU-based CFD/DEM simulations to model crystal nucleation, growth and break-up in an agitated tank in real-time. This approach, which pairs lattice-Boltzmann-based direct numerical simulations with fully resolved discrete element modeling (DEM), enables mechanistic and time-accurate modeling of crystal growth via solvent reactions and crystal growth via reactions and particle interactions. In addition to particle growth and agglomeration, particle break-up is modeled at the level of individual particles, providing insights into the competition between grown and break-up. Additionally, since the particle trajectories are modeled using Newtons second law, the effects of particle growth and break-up on settling on are also predicted. Since the particle field is solved in tandem with the fluid field, incorporate the effects of particle size and concentration into the local fluid viscosity when modeling fluid flow. Using this model, we can predict the spatiotemporal particle size distribution, and show how this distribution is informed by reactor operating conditions, solvent chemistry, particle interactions, particle settling, and particle break-up. In fluid mechanical systems, momentum transport can be described by the Navier-Stokes equations. The Navier-Stokes equations link fluid acceleration and the applied body and surface forces, which typically include gravity, buoyancy, and shear stress gradients. Although only a few exact solutions to the Navier-Stokes equations exist, many numerical solution approaches are available. One of the most general and computationally efficient approaches for solving the transient Naiver-Stokes equation is Lattice-Boltzmann. The Lattice-Boltzmann method scales exceptionally well on GPU-based architecture, enabling high-resolution large eddy simulation of turbulent flows at industrial scale. Following this approach, CFD simulations running lattice points at lattice updates per second are routine on a multi-GPU workstation. In solid granular systems, particle trajectories are governed by Newton’s second law. Newton’s second law links the acceleration of a particle to the sum of the forces acting on it, including gravity, buoyancy, fluid-interaction, and particle contact. Exact solutions are not available for generalized granular systems containing more the two particles. Computational approaches must be applied. One of the most general and computationally efficient approaches for solving this N-body problem is to use a velocity Verlet integration scheme for particle trajectories with a GPU-based BVH tree to account for neighbor-neighbor interactions. Following this approach, DEM simulations tracking interacting particles at particle updates per second are routine on a multi-GPU workstation. From a modeling perspective, it is practical to combine fluid mechanical systems and solid granular systems into a unified multi-phase system, wherein both transport processes are solved simultaneously. In this approach, the local and instantaneous properties of the fluid inform the local and instantaneous forces on the particles. Likewise, the properties of the particles inform the local properties and forces applied to the fluid. Since these processes are solved in real time and directly on the GPU, it is straightforward to model time-varying physics, including reactions, phase change, and particle growth/breakage within this unified model.",28,0.0
"The manufacture of active pharmaceutical ingredient (API) in the pharmaceutical industry typically involves a number of fluid mixing and reaction operations, many of which are conducted in mechanically stirred, glass-lined tanks and reactors provided with a torispherical bottom and equipped with a single baffle and a retreat-blade impeller (RBI). The use of glass lining is critical to provide corrosion resistance, allow ease of cleanliness, and reduce product contamination. Still it requires coating with glass not just the internal wall of the tank, but also all components that may come in contact with the liquid contents, including the impeller and the baffles. This often results in the use of partial baffling, typically a beavertail baffle, which is mounted from the top of the reactor and inserted between the shaft and the tank wall. However, in other industrially relevant systems the more conventional four vertical baffles are mounted on the vessel wall, although in some pharmaceutical product manufacturing and applications (e.g., crystallization, biopharmaceutical processes, solid suspensions, etc.) unbaffled stirred vessels can also be found.In previous work by our group, the power dissipated by the impeller, P, and the impeller Power Number, Po, which arecritical for scale-up purposes, especially when mass transfer operations are involved, were experimentally determined for systems commonly found in the pharmaceutical industry. This was achieved by measuring the power dissipated by an RBI under fully baffled, partially baffled, and unbaffled conditions, with different liquids, and under different hydrodynamic regimes (1<Re< 300,000) using a 61-Liter vessel that was built as an actual scaled-down replica of the glass-lined vessels typically used for API manufacturing in the pharmaceutical industry.In our present work, Computational Fluid Dynamics (CFD) was utilized to conduct detailed hydrodynamic simulations for the same systems, not only to determine the flow features of the different systems computationally but also to obtain relevant critical quantities such as the power dissipation and Power Number. Simulations were conducted for a variety of baffling configurations and operating conditions using different CFD approaches, turbulence models, and simulation software. The results indicate that the computational predictions typically compared well with the experimental data in all cases, thus validating the CFD approach.  The information provided by this approach to understanding the different fluid-dynamics features of vessel-baffling configurations and the relevant effect of operating parameters is clearly beneficial for the selection, design and use of suitable vessels and reactors for industrial applications, and it is expected to be of value to the industrial practice.",28,1.0
"IntroductionFiltration is extensively used in pharmaceutical manufacturing to recover the crystals of active ingredients and intermediates. This operation is often a slow processing step, accounting for a substantial quota of the process time. Furthermore, its design requires extensive experimental investigations at different scales as the impact of most driving factors is not sufficiently well understood to allow quantitative modeling of the process [1]. The structure of the filter cake is a result of the packing of its building blocks, i.e., the crystals and the way they arrange within the structure. The resulting network of pores ultimately affects the filtrate flow, hence the processing time. Here, we propose a combined modelling and experimental strategy to quantitatively understand the packing of powders based on their particle size and shape distributions. Needle-like crystals are here chosen as case study due to their extensive presence in pharmaceutical manufacturing [2,3]. Our results show how the structure of the cake can be linked to the performance of the filtration unit and therefore used to predict filtration times. MethodsIn this work we present a computational methodology to simulate the cake formation of needle-like crystals and to determine the filtration performance of the cake. Monte Carlo sampling is used to simulate the pressure driven packing of crystals. The spherocylinder, i.e. a cylinder capped by hemispheres at each end, is used as model shape [4]. Systems with statistically relevant number of particles and with polydispersities that resemble experimental distributions are used [5]. A graphical example of a filter cake produced with such methodology is presented in Fig.1(a).We also provide an experimental counterpart to analyze filter cake properties and to validate the simulation protocol. For this purpose, X-ray tomography is used in this work to determine the structure of the pore space and how this develops along the cake [6]. Furthermore, an in-house segmentation algorithm is used to identify the shape, positions and orientation of individual needle-like crystals forming the experimental cake structure. Fig. 1 b. presents a small section of a real filter cake and shows the segmented single particles with different colors.The structure of the pore space that forms inside filter cakes, both experimental and simulated, is then represented through a pore-network model [7,8]. Within pore network modeling, the connectivity of the pores is approximated via cylindrical channels, namely the throats, whose flow properties can be easily estimated using the Hagen-Poiseuille equation. An example of how the pores are distributed and interconnected inside the filter cake is presented in Fig. 2. This approach allows us to determine the permeability of the structure and therefore the filtration time. ResultsThe modeling and experimental methodology described above has been used to evaluate the filtration dependency on particle morphology for a series of compounds, some of which taken from current development projects of AstraZeneca. The results collected confirm the strong dependency of filtration performance on particle morphology. The ability of these simulation routines to account for diverse particle size and shape distributions enables us to explore the effect of polydispersity on cake structure. Polydispersity appears to have a substantial effect on cake porosity, comparable to the one of the particle aspect ratio. This highlights that size control strategies employed during crystallization can have a tremendous impact on filtration time and that crystallization control is paramount.The simulation strategy proposed in this work has been applied to investigate the packing of crystals of β L-Glutamic Acid with known particle size and shape distributions (determined in a previous experimental work [1]). The outcome of this investigation is reported in Fig. 3 [5]. The porosity of the simulated cake structures is compared to the one of the experimental cakes. It can be observed in Fig. 3(a) how both datasets follow the same trend, but that there is a constant offset between the two. The linear relation between the two (R2=0.93) is further highlighted by the parity plot in Fig. 3(b). This confirms that the model is able to accurately predict the porosity trend using exclusively information about the particle size and shape distributions. The constant shift between the datasets is likely caused by the fact that we are simulating smooth spherocylinders that have a significantly different morphology and surface roughness compared to real faceted crystals. Furthermore, our simulations do not cover substance and system-specific properties (particle-particle and particle-solvent interactions).Comparisons between simulation and experimental results also highlight that the porosity and permeability of the filter cakes are not directly related to each other. Our efforts using pore network modeling show that the properties of the filter cake structure (i.e., the throats sizes, their distribution and interconnectivity) play a vital role in determining the permeability as well. Using our combined approach of simulating packings and then processing them with the help of a pore network model, however, allows us to predict the permeability trend in dependence of the particle size and shape distributions of the underlying powders.Conclusions and outlookThe modeling protocol presented here sheds light on the crucial importance of the polydispersity of crystal populations on filtration performance. It also shows its ability to successfully predict the porosity of real cakes of crystals. Quantitatively knowing a priori the filterability of a population of crystals will ultimately allow pinpointing the optimal distribution of particle size and shape to minimize filtration time. This could then be used as a decision criterion in the process development phase where the gain in filtration time should be counter-weighed with the effort required in the crystallization process to obtain a tailored particle size distribution. Our contribution therefore opens up new avenues to integrate different unit operations (crystallization and filtration) into an overall optimized process with high productivity. References[1] Perini G., Salvatori F., Ochsenbein D.R., Mazzotti M., Vetter T., Sep. Purif. Technol., 211, 768–781 (2019)[2] Wilson D. et al., Powder Technol., 339, 641–650 (2018).[3] MacLeod C. S. and Muller F. L., Org. Process Res. Dev., 16, 3, 425–434 (2012).[4] Meng L., Lu P., Li S., Zhao J., Li T., Powder Technol., 228, 284–294 (2012).[5] Perini G., Avendaño C., Hicks W., Parsons A. R., Vetter T., “Predicting filtration of needle-like crystals: a Monte Carlo simulation study of polydisperse packings of spherocylinders”, in preparation.[6] Löwer E., Pham T.H., Leißner T., Peuker U.A., Powder Technol., 363, 286–299 (2020).[7] Lin C.L., Miller J.D., Chem. Eng. J., 77, 79–86 (2000).[8] Rabbani A., Jamshidi S., Salehi S., J. Pet. Sci. Eng., 123, 164–171 (2014).",28,2.0
"Monoclonal Antibodies (mAbs) are highly demanded therapeutic products that are used for the treatment of cancers, autoimmune diseases, infectious and microbial diseases[1]. It has been reported that mAb amounts for almost 37% of biologic drugs market[2]. For most mAbs, glycosylation process is critical to protein folding, secretion, signal induction, and it impacts the product quality and efficacy [3]. To understand and improve the glycoform distribution, first-principle models have been built to quantify the effects of operating conditions, including process feed-variables, temperatures, pH to protein production, and glycosylation[4-6]. First-principle models capture the complex biochemical pathways of cell metabolism and protein glycosylation and correspond to nonlinear dynamic set of equations that are usually computational expensive. To reduce the computational complexity, a dynamic surrogate model can be used to capture the input and output relations describing the bioreactor operation. Kriging is a nonparametric modeling technique that predicts the target point based on the weighted sum of the observed function values at sampling points around the predicted point [7, 8]. Among different data-driven model techniques, kriging in most cases, requires a limited number of samples while achieving better efficiency and accuracy. It also provides error estimation for its predicted value[8, 9].In this study, we build a dynamic kriging model to replace the first-principle model, which is used to capture the dynamic behavior of cell growth and simulate protein glycosylation with high computational efficiency. First, an unstructured kinetic model is built to capture the CHO cell culture process in fed-batch bioreactor and predict viable cell, glucose, lactate, and protein concentration. This model is coupled to a structured single-cell glycosylation model to determine the secreted glycoprotein fractions. Then a series of simulations are performed under different operating conditions based on a full factorial design, which provides inputs and outputs under all the time points to build the dynamic kriging surrogate model. By varying pH and the concentration of media supplements (MnCl2), the dynamic kriging model is able to capture time dependent responds of the system, including viable cell density, glucose, lactate concentrations and glycan fractions by solving the kriging iteratively at each time step [8, 10]. This model is used to find the optimal operating conditions for specific product specifications and integrate to a bioreactor model to achieve on-line product prediction and control.ReferenceNewswire, P.R., Monoclonal Antibodies (mAbs) Market Analysis by Source (Chimeric, Murine, Humanized, Human), by Type of Production, by Indication (Cancer, Autoimmune, Inflammatory, Infectious, Microbial, Viral Diseases), by End-use (Hospitals, Research, Academic Institutes, Clinics, Diagnostic Laboratories) and Segment Forecasts, 2018-2024. 2017: Market Research Report No. GVR-1-68038-280-8.Mullin, R., Contract services for biologics evolve. Chemical & Engineering News, 2017. 95(33): p. 34-35.Bieberich, E., Synthesis, Processing, and Function of N-glycans in N-glycoproteins. Advances in neurobiology, 2014. 9: p. 47-70.Sou, S.N., et al., Model-based investigation of intracellular processes determining antibody Fc-glycosylation under mild hypothermia. Biotechnol Bioeng, 2017. 114(7): p. 1570-1582.Kotidis, P., et al., Model-based optimization of antibody galactosylation in CHO cell culture. 2019. 116(7): p. 1612-1626.Villiger, T.K., et al., Controlling the time evolution of mAb N-linked glycosylation - Part II: Model-based predictions. 2016. 32(5): p. 1135-1148.Bhosekar, A. and M. Ierapetritou, Advances in surrogate based modeling, feasibility analysis, and optimization: A review. Computers & Chemical Engineering, 2018. 108: p. 250-267.Boukouvala, F., F.J. Muzzio, and M.G. Ierapetritou, Dynamic Data-Driven Modeling of Pharmaceutical Processes. Industrial & Engineering Chemistry Research, 2011. 50(11): p. 6743-6754.Lucifredi, A., C. Mazzieri, and M. Rossi, APPLICATION OF MULTIREGRESSIVE LINEAR MODELS, DYNAMIC KRIGING MODELS AND NEURAL NETWORK MODELS TO PREDICTIVE MAINTENANCE OF HYDROELECTRIC POWER SYSTEMS. Mechanical Systems and Signal Processing, 2000. 14(3): p. 471-494.Shokry, A. and A. Espuña, Sequential Dynamic Optimization of Complex Nonlinear Processes based on Kriging Surrogate Models. Procedia Technology, 2014. 15: p. 376-387.",28,3.0
"Islatravir is a novel nucleoside active pharmaceutical ingredient (API) currently in clinical trials for both HIV treatment and prevention. As reported recently in Science (Huffman et. al. 2019), the API synthesis under development involves a 9-enzyme 3-step in-vitro cascade. One of the enzymatic transformations involves a galactose oxidase enzyme performing a biocatalytic oxidation in the presence of oxygen.The oxidation reaction requires mass transfer of oxygen, delivered as air, from the gas phase to the liquid phase to achieve the desired reaction rate and conversion. The volumetric mass transfer coefficient, kLa, is therefore a critical scale-up parameter. Dynochem models were developed and used to characterize kLa in each vessel configuration that would be used to run the oxidation reaction. Scalability of the reaction performance at a given kLa was demonstrated successfully at scales ranging from 1 gram in the lab to 15 kg at the pilot scale. Dissolved oxygen and reaction kinetics were profiled to highlight the similarity of the reaction performance across scales at a specific kLa.Typical reactors used in small molecule API manufacturing are not equipped for air delivery or optimal gas-to-liquid mass transfer. Reconfiguration of these lab, pilot, and commercial reactors was required to outfit vessels with the capabilities to run the oxidation reaction and characterize the associated kLa. Air was selected for oxygen delivery as this was the safest and most readily available choice. In the selected commercial vessel, the standard reconfiguration to enable air sparging was not sufficient to obtain the required kLa given the reaction constraints. Modifications to the vessel, agitator, and air delivery method are being considered to improve the mass transfer at the commercial scale. In addition, process parameters, such as temperature and pressure, are being investigated in an attempt to increase oxygen solubility and compensate for low kLa with minimal capital investment. The alternative solutions for increasing or managing a low kLa will be demonstrated at the commercial scale to enable successful performance of the oxidation reaction long term.",28,4.0
"Friedel Crafts acylation reaction of aromatic substrates with different acylating agents produces ‘aryl alkyl ketones’. The acylation reaction of biomass derived furans and long-carbon chain (C12-C20) fatty carboxylic acids or anhydrides obtained by hydrolysis of plant -oil based triglycerides produces ‘alkyl furan ketones’. These ketone molecules are important precursors for the synthesis of renewable oleo-furan sulphonate (OFS) surfactants as demonstrated by Park et al. [1]. The exceptional stability of these OFS surfactants for use as detergents in hard water as well as other comparable detergent characteristics makes them a useful alternative to the petroleum-based surfactants. In this work, we elucidate the reaction kinetics of Friedel Crafts acylation reaction of 2-methylfuran and n-octanoic anhydride as a model system with Al-MCM-41 as a solid Brønsted acid catalyst in the presence of a non-protic solvent heptane. The apparent activation energy and the reaction rate orders are obtained in a flow reactor set-up. The experimental observations are explained using a potential mechanism and a kinetic model. The understanding of this C-C coupling reaction using long chain fatty acid derivatives will contribute to the scale-up of OFS surfactants as well as to the design of heterogeneous Friedel Crafts acylation catalysts.Reference:Park, D. S., Joseph, K. E., Koehle, M., Krumm, C., Ren, L., Damen, J. N., Shete, M. H., Lee, H. S., Zuo, X., Lee, B., Fan, W., Vlachos, D. G., Lobo, R. F., Tsapatsis, M. and Dauenhauer, P. J. ACS Cent. Sci. 2, 820–824 (2016).",29,0.0
"The lube manufacturing industry has benefited from remarkable advances in catalytic hydroprocessing technology. ExxonMobil and its affiliates worldwide have considerable technical know-how and experience in lubes manufacturing, and more than 75 years of rich history in applied catalysis. At the heart of the MSDWTM/MAXSATTM process for catalytic hydroisomerization and hydrofinishing, and the MWITM process for wax conversion, are systems of proprietary, shape-selective catalysts containing zeolites and noble metals. These catalysts display superior activity and selectivity, and unsurpassed tolerance to various contaminants.ExxonMobil has been a leader in applying state of the art technology in recent and current projects to meet the growing need for high quality basestocks. Recent ExxonMobil EHCTM Group II base stock investments include start-up of the Rotterdam hydrocracker unit early this year, and an upgrade to Group II quality to be completed in Singapore in the second quarter of this year. Together, these projects utilize the latest proprietary ExxonMobil catalyst and process technologies. Earlier this year, ExxonMobil also indicated that it had made a final investment decision on a multi-billion dollar expansion of its integrated manufacturing complex in Singapore that will produce higher-value products and expand lubricant base stocks production to meet growing market demand. That project will utilize a unique combination of proprietary ExxonMobil catalyst and process configuration technologies to upgrade and convert fuel oil into higher-value products. As part of the expansion, ExxonMobil will introduce a new, unique high viscosity Group II base stock into the marketplace.This presentation will overview the latest ExxonMobil catalyst and process technologies being deployed in these projects, including state of the art, high activity Celestia hydrotreating catalyst. It will also highlight how these technologies are combined to capture very significant value from their superior productivity, selectivity, run length, and ability to handle more challenged feedstocks.",29,1.0
"The transition from classical homogeneous catalysis to environmentally friendly, heterogeneous alternatives presents a unique challenge. Tert-butylation of phenol is a characteristic example due to its importance in industrial applications. An estimated 450,000 tonnes per year of tert-butyl phenols (TBP) are manufactured industrially for production of innumerable chemical commodities [1]. 2-TBP is utilized in production of pesticides and fragrances; 4-TBP functions as a flavoring agent and petroleum additive, among other compounds. However, the reaction is classically carried out in the presence of liquid acid catalysts [2], inducing economic and environmental concerns [3].Numerous research groups have carried out this reaction using heterogeneous catalysts in both vapor and liquid phases and in batch and continuous flow systems, though few have managed to perform tert-butylation of phenol under conditions relevant to industrial applications. As this reaction is catalyzed by Bronsted acidity, it is believed that solid superacid metal oxide catalysts with surface-bound sulfate species may be appealing.At low pressure and temperature, sulfated metal oxide catalysts as well as exhibited relatively high activity for tert-butylation of phenol. The observed activity for these catalysts follows the trend Amberlyst ® 15 > SnO2/SO42- > TiO2/SO42- > ZrO2/SO42-. At complete conversion, selectivity was observed to follow the same trend: ~60-65% 2-TBP, ~30-35% 4-TBP, 3-5% 2,4-TBP, and minor amounts of 2,6-TBP and 2,4,6-TBP. Production of TBPE, the O-alkylated product, was observed at very short reaction times. Catalytic regeneration and reuse were also evaluated. Remarkably, it was found that Amberlyst ® 15 could be regenerated simply by washing with ethanol and drying at 110°C; reuse was achieved 3 times without loss of conversion, quite contrary to the sulfated metal oxides. Given the high conversion of the alkylating agent and selectivity towards mono-alkylated products, the catalysts presented here offer an environmentally-friendly alternative to traditional homogeneous catalysts.",29,2.0
"Butadiene, a key raw material for the production of synthetic rubbers, and intermediate chemicals is conventionally manufactured via an energy intensive endothermic conversion of Butane. This process results in catalyst deactivation and low selectivity. Hence, an alternative, exothermic butadiene production pathway was proposed and is being studied extensively. In this study, an Artificial Neural Network (ANN) aided scale-up of a multi-tubular shell and tube reactor for the synthesis of Butadiene via the oxidative dehydrogenation of butylene was executed. A three-step solution strategy was adopted for the study. The step 1 comprised rigorous mathematical modelling of a lab-scale multi-tubular reactor using OpenFOAM. The developed model was used to generate data under varying process conditions (Feed composition and flowrates, coolant velocity, and temperature) for training an ANN surrogate model. The CFD results were validated against experimental data with minimal errors of less than 5%. In step 2, the generated data was fed to the ANN model for training and optimization. The optimized operating conditions were then used as inputs in the CFD model for simulation and cross-validation. Subsequently, the third and final step was implemented. In this step, the lab-scale multi-tubular reactor was scaled-up using the theory of dimensionality (π-theory), conventional strategies, and domain expert knowledge. Several parametric studies were carried-out by varying the baffle height, number and spacing, tube arrangement, number, length and diameter as well as the shell side length and diameter. Each of these case studies were simulated for data generation using the CFD model by leveraging expertise in advanced meshing techniques to reduce the computational mesh density without compromising the accuracy and numerical stability of the simulation results. The generated data was again fed to an ANN model for optimization. The optimized reactor configuration was then designed and simulated with the CFD model for comparison and validation.",29,3.0
"One of the commercially most important aromatic compounds is p-xylene, which is used for the production of various fine chemicals. Due to its cheap feedstocks, toluene alkylation with methanol is one of the most commercially attractive routes to produce p-xylene; however, challenges such as low catalyst stability and single pass p-xylene yield need to be addressed. Here we present a study of the catalytic properties of MWW-type zeolite catalyst, for toluene alkylation with methanol under high pressure conditions (600 psia).To understand the catalytic behavior of MWW, we deconvoluted structure-function relationships for different topological features (supercages, sinusoidal channels, and external surface pockets) having profound impact on catalyst performance. Deconvolution was achieved by selectively passivating the active sites in specific pore systems, and then comparing their activity with the parent MWW catalyst.Under high pressure operating conditions, the catalyst life increases multi-fold at high toluene conversion with side reactions such as methanol-to-hydrocarbon reactions being suppressed in the absence of water or hydrogen co-feed. Interestingly, the selectivity of xylene isomers changes significantly during the course of reaction, from thermodynamic equilibrium mixture to 55% p-xylene selectivity.The combination of structure-deconvoluting experiments and DFT calculations reveals that the active sites in surface pockets are unselective and fast isomerization on external surface leads the reaction towards the thermodynamically determined xylene product distribution, while microporous sinusoidal channels and/or supercages preferentially form p-xylene. Overall, this study introduces a commercially viable route for p-xylene production as well as new insight into the reaction mechanism over MWW zeolite catalysts to aid in the development of optimized processes for various alkylation reactions.",29,4.0
"Ammonia-oxidation(AMOX) over Pt-containing catalysts is an important reaction for eliminating NH3 in diesel-engine exhaust. Kinetic models have been developed for Pt-single crystals under ultra-high vaccum[1] and for Pt-gauze for HNO3 production[2,3] but these are of limited value for NH3-oxidation on Pt-supported catalysts for emission control. Here we describe a kinetic study for Pt/Al2O3 at atmospheric pressure and the development of a microkinetic model. We measured reaction-rate(TOF) dependence over wide range of NH3 concentration from 10 – 40,000ppm for Pt/Al2O3 powder and washcoated-monoliths. The data in Figs.1(a) and (b) show the presence of rate-maximum at 500ppm for unmilled-Pt/Al2O3 but at 10,000ppm for ball-milled Pt/Al2O3 powder and washcoated-monolith. Another unexpected observation is the enhanced activity of Pt/Al2O3 samples upon ball-milling, which resulted in lowering of light-off temperature(T50) by 50°C[Fig.1(c)]. Ball-milling reduces particle size, in-turn decreases the extent of diffusion-limitations. However, steady-state AMOX done over ball-milled Al2O3 followed by Pt addition did not show the same enhanced activity ruling out diffusion-limitations as the main cause of the activity enhancement. Further characterization of Pt/Al2O3 samples using XRD[Fig.1(e)] reveals an alteration in the distribution of Pt crystalline-planes; notably, Pt<211> increased and Pt<111> decreased. These data suggest that the higher activity of ball milled-Pt/Al2O3 is a result of this crystalline-plane transformation. Microkinetics by Rebrov[1] assumed Pt<111>plane which predicted no rate variation with NH3 concentration. This is due to NH3 and O2 adsorbing on independent sites ‘b’ and ‘a’ respectively. Contrary, experiments showed ‘+’ to ‘-’ NH3 reaction-order variation over wide range of NH3 concentration indicates site-competition[Fig-1(d)]. The model was modified by introducing site-competition between NH3 and O2 on site-a in addition to NH3 adsorption on site-b which best described the experimental data and kinetics. These findings show the close dependence of kinetics and reactivity on Pt crystal-planes.References:[1]Rebrov.E.V et al.,Chem.Eng.J,90,61-76(2002)[2]Hass.M et al.,ISCRE-25(2018)[3]Baerns.M et al.,J.Catal,232,226-238(2005)",29,5.0
"The use of multifunctional organocatalysts and continuous flow platform are commonplace in modern chemical transformation.[1-4] Herein, we describe a method for immobilization of trifunctional organocatalysts on porous composite hollow fiber and demonstrate their application as heterogeneous catalyst and continuous-flow microfluidic reactor for chemical transformation. The polyamide imide hollow fibers (PAIHF’s) are functionalized with aminosilanes and bromine source to immobilize covalently hydrogen-bond donor groups (-OH and -NH), and nucleophilic [Br−] species upon the fiber surface and provide trifunctional acid-base-nucleophilic organocatalysts and microfluidic reactors. The cooperative effects of Br/APS/PAIHF trifunctional organocatalysts are elucidated in the CO2 cycloaddition and hydroxyalkylation of aniline under batch and continuous flow synthesis. Our results indicated that the synergistic cooperative effect of trifunctional organocatalyst on PAIHF leads to a maximum 1-(phenylamino)propan-2-ol selectivity of 97.1% at 61% aniline conversion and 0.02 cm3/min flow rate. While knowledge of the acid-base-nucleophilic trifunctional cooperativity is still limited, these findings demonstrate useful structure-property trends that can be used to design more efficient organocatalysts for sustainable chemical transformation.References:1. A.-A. Alwakwak, Y. He, A. Almuslem, M. Senter, A. K. Itta, F. Rezaei, A. Rownaghi, Metal- and solvent-free synthesis of aminoalcohols under continuous fl ow conditions, React. Chem. Eng. 5 (2020) 289-299.2. Y. He, F. Rezaei, S. Kapila, A.A. Rownaghi, Engineering Porous Polymer Hollow Fiber Microfluidic Reactors for Sustainable C-H Functionalization, ACS Appl. Mater. Interfaces. 9 (2017) 16288–16295.3. Y. He, A. Jawad, X. Li, M. Atanga, F. Rezaei, A.A. Rownaghi, Direct aldol and nitroaldol condensation in an aminosilane-grafted Si/Zr/Ti composite hollow fiber as a heterogeneous catalyst and continuous-flow reactor, J. Catal. 341 (2016) 149–159.4. Y. He, A.K. Itta, A. Alwakwak, M. Huang, F. Rezaei, Aminosilane-Grafted SiO2-ZrO2 Polymer Hollow Fibers as Bifunctional Microfluidic Reactor for Tandem Reaction of Glucose and Fructose to 5-Hydroxymethylfurfural, ACS Sustain. Chem. Eng. 6 (2018) 17211–17219.",29,6.0
"From microspheres to tablets, microstructures are at the core of drug performance, formulation design, and process development. In this open discussion workshop, the DigiM team will showcase a suite of high resolution image-based tools for quantifying and correlating microstructures with drug performance and behavior. Novel techniques including AI microstructure quantification, image-based release prediction, and digital drug formulation will be presented. Applications of these techniques for complex dosage forms and solid dosage process development will be discussed.",30,0.0
"Comparison of Surrogate Modeling Techniques for Design Space Approximation and Surrogate-Based Optimization: Effect of Sampling Technique and Sample SizeBianca Williams, Selen CremaschiSession: Advances in Machine Learning and Intelligent SystemsSurrogate models, also known as response surfaces, black-box models, metamodels, or emulators, are simplified approximations of more complex, higher order models. These models are used to map input data to output data when the actual relationship between the two is unknown or computationally expensive to evaluate (Han & Zhang, 2012). Surrogate models can also be constructed for use in surrogate-based optimization when a closed analytical form of the relationship between input data and output data does not exist or is not conducive for use in traditional gradient-based optimization methods. Surrogate modeling techniques are of particular interest where high-fidelity, thus expensive, simulations are used (Han & Zhang, 2012) or when the fundamental relationship between the design variables and output variables is not well understood, such as in the design of cell or tissue manufacturing processes (Du et al., 2016).With all the surrogate modeling techniques currently available, there is a need for a systematic procedure for selecting the appropriate technique for a given application. Current common practices for selecting the appropriate surrogate model form rely on process-specific expertise. Numerous studies have been conducted comparing the performance of surrogate modeling techniques for approximation purposes (Bhosekar & Ierapetritou, 2018; Davis et al., 2017). The majority of these only compare a few models on a limited number of data sets or for specific applications (Ju et al., 2016; Luo & Lu, 2014). Progress has been made in recent works in generalizing the process for selecting a surrogate model to approximate a design space by using meta-learning approaches to build selection frameworks (Cui et al., 2016; Garud et al., 2018), avoiding expensive trial-and-error methods. However, few of the developed meta-learning frameworks take model complexity into account, which can lead to overfitting, or consider that multiple models might perform similarly to the one identified as best in terms of their accuracies. The selection of surrogate models for surrogate-based optimization remains an open challenge.Our goal is to comprehensively investigate and compare the performance of several different surrogate modeling techniques for both approximating functional relationships and for surrogate-based optimization to link that performance to the characteristics of the data involved in the application. Previous work on this topic has shown that the performance for approximation is dependent on data characteristics such as the input dimension and the underlying function shape (Davis et al., 2017; Williams & Cremaschi, 2019). The specific data characteristics being investigated in this study are the shape of the underlying function being modeled, the number of input dimensions, the sampling method used to generate the data, and the number of sample points in the dataset. The surrogate-modeling techniques considered include Artificial Neural Networks, Automated Learning of Algebraic Models using Optimization (ALAMO), Radial Basis Networks, Extreme Learning Machines, Gaussian Progress Regression, Random Forests, Support Vector Regression, and Multivariate Adaptive Regression Splines (MARS). These techniques are used to construct surrogate models for data generated using the 47 optimization challenge functions from the Virtual Library of Simulation Experiments (Surjanovic & Bingham, 2013). The sampling methods studied are Sobol sequence sampling, Halton sequence sampling, and Latin Hypercube sampling. Four performance measures are used to evaluate the accuracy of the surrogate models: root mean squared error (RMSE), maximum percent error (MPE), the R-squared value, and the adjusted R-squared value. The surrogate models’ ability to locate the extrema of the functions are evaluated by calculating the distance between the extreme point(s) estimated by the model and the actual function extrema. The results provide guidance on which surrogate models generate the best predictions and give general “rules of thumb.”Using information extracted from the surrogate modeling comparison experiments and building upon previous meta-learning approaches, we constructed a tool to provide recommendations for the appropriate modeling techniques for the datasets based only on the characteristics of the data being modeled. Characteristics, i.e., attributes, were calculated for each dataset with the goal of representing its overall behavior. Attributes were calculated based only on input and output values in the dataset. The attributes that have the strongest relationships with the performance metrics are determined using feature reduction methods, including the ReliefF algorithm (Kira & Rendell, 1992) and principal component analysis (Hotelling, 1933). These attributes were used as inputs, with designated performance metrics as outputs, to train models to make predictions on the performance of the surrogate modeling techniques. The performance metrics used as outputs for training the recommendation tool are the adjusted R-squared value for the design space approximation application and the normalized distance between the extreme point(s) estimated by the models and the actual extrema of the true model for surrogate-based optimization. The adjusted R-squared value takes into account both the surrogate model accuracy and its complexity (Miles, 2005). The tool identifies which surrogate modeling techniques are recommended for use for either approximating a design space or for surrogate-based optimization given a set of data.References:Bhosekar, A., & Ierapetritou, M. (2018). Advances in surrogate based modeling, feasibility analysis, and optimization: A review. Computers & Chemical Engineering, 108, 250-267.Cui, C., et al. (2016). A recommendation system for meta-modeling: A meta-learning based approach. Expert Systems with Applications, 46, 33-44.Davis, S., et al. (2017). Efficient Surrogate Model Development: Optimum Model Form Based on Input Function Characteristics. In A. Espuna, M. Graells & L. Puigjaner (Eds.), 27th European Symposium on Computer Aided Process Engineering (ESCAPE 27) (Vol. 40, pp. 457-462). Barcelona, Spain: Elsevier.Du, D., et al. (2016). Statistical Metamodeling and Sequential Design of Computer Experiments to Model Glyco-Altered Gating of Sodium Channels in Cardiac Myocytes. IEEE J Biomed Health Inform, 20, 1439-1452.Garud, S. S., et al. (2018). LEAPS2: Learning based Evolutionary Assistive Paradigm for Surrogate Selection. Computers & Chemical Engineering, 119, 352-370.Han, Z., & Zhang, K. (2012). Surrogate-Based Optimization. In O. Roeva (Ed.), Real-World Applications of Genetic Algorithms (pp. 343-362). Rijeka, Croatia: InTech Open.Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24, 498-520.Ju, Y. P., et al. (2016). Artificial intelligence metamodel comparison and application to wind turbine airfoil uncertainty analysis. Advances in Mechanical Engineering, 8.Kira, K., & Rendell, L. A. (1992). A Practical Approach to Feature-Selection. Machine Learning /, 249-256.Luo, J. N., & Lu, W. X. (2014). Comparison of surrogate models with different methods in groundwater remediation process. Journal of Earth System Science, 123, 1579-1589.Miles, J. (2005). R Squared, Adjusted R Squared. In Encyclopedia of Statistics in Behavioral Science: John Wiley & Sons Ltd.Surjanovic, S., & Bingham, D. (2013). Virtual Library of Simulation Experiments. In (Vol. 2018). Simon Fraser University.Williams, B., & Cremaschi, S. (2019). Surrogate Model Selection for Design-Space Approximation and Surrogate-Based Optimization. In S. Munoz, C. Laird & M. Realff (Eds.), Ninth International Conference on Foundations of Computer-Aided Process Design (FOCAPD-19) (pp. 353-358). Copper Mountain, CO, USA: Elsevier B.V.",31,0.0
"Membranous nephropathy (MN) is a glomerular disease that causes nephrotic syndrome among adult patients. Records indicate that approximately one third of the patients that possess MN further progress into end-stage renal diseases in 5 to 15 years; among patients that are above 60 years old, 20-30% of the cases are found to be associated with malignancy such as lung cancer and prostate cancer.[2][3]Diagnosis of MN is exclusively achieved by renal biopsy with a focus on the glomerular basement membrane area, which involves examination to the images of histochemically stained tissue samples captured by either light microscopy (LM) or electron microscopy (EM) [1]. For precise identification of glomerular features, the pathoanatomical examination often involves using whole-slide images (WSIs) of tissues stained in multiple histochemical approaches, which is a labor-intensive process.Motivated by this, we aim to develop an automated examination method based on artificial intelligence and investigate its efficacy towards reducing the workload of clinicians and improving the consistency of diagnostic results. The basis for this effort is artificial intelligence algorithms to design an efficacious data-driven classification model.In this work, a Convolutional Neural Network (CNN)[4] based model was structured and trained to determine the existence of MN within the images of glomeruli. To further address the issue of model accuracy not satisfying the requirement of the application, a method called high confidence acceptance (HCA) was designed to boost the precision of the model to an application specified target of 95\%. With HCA, we are aiming to use only strongly confident predictions as automated diagnostic results, leaving the relatively lower confidence instances to manual inspections performed by renal pathologists. In such scenario, a partial replacement of the manual examination by renal pathologists at the demanded accuracy can be accomplished. A metric called Acceptance Rate (AR) was accordingly defined and used in our scheme to measure HCA performance; AR represents the percentage of workload being automated and replaced by the CNN model.As a result, the model trained in this work which initially had an overall accuracy of approximately 86% demonstrated an AR of approximately 34%, with a precision that met target accuracy as high as 95%. This result implied that one third of the workload will be fully automated at target accuracy, and the rest of the less confident instances (declared inconclusive) will be sent to renal pathologists for manual inspection. Furthermore, the model developed in this work also presented an acceptance rate of 23% when applied to a dataset solely comprised of HE (Hematoxylin-Eosin) stained biopsy images, which indicated a potential reduction to the staining effort which conventionally requires up to 4 different staining methods and usage of excessive specialist labor. A further test on images of larger field of view was also carried out; a performance consistent with tests on glomeruli-level images was obtained and the practical value of the model and HCA method is thus illustrated.The result illustrated the feasibility of automatically diagnosing membranous nephropathy with CNN, which has considerable value to the relevant medical profession. Meanwhile, the HCA method is an universal technique that can be applied to improve and measure the applicability of any classification model that is not satisfying the required accuracy of the target application, and may thus further broaden the horizon of artificial intelligence in practical implementation.References:[1] Agarwal, S. K., Sethi, S., & Dinda, A. K. (2013). Basics of kidney biopsy: A nephrologist’sperspective.Indian Journal of Nephrology,23(4), 243–252. doi: 10.4103/0971-4065.114462[2] Couser, W. G. (2017). Primary membranous nephropathy.Clinical Journal of the AmericanSociety of Nephrology,12(6), 983–997. doi: 10.2215/CJN.11761116[3] Lai, W. L., Yeh, T. H., Chen, P. M., Chan, C. K., Chiang, W. C., Chen, Y. M., . . . Tsai,T. J. (2015, 2).Membranous nephropathy: A review on the pathogenesis, diagnosis,and treatment(Vol. 114) (No. 2). Elsevier. doi: 10.1016/j.jfma.2014.11.002[4] LeCun, Y., Haffner, P., Bottou, L., & Bengio, Y. (1999). Object Recognition with Gradient-Based Learning. In (pp. 319–345). Retrieved fromhttp://link.springer.com/10.1007/3-540-46805-619doi: 10.1007/3-540-46805-6{\}19",31,1.0
"Pressure swing adsorption (PSA) is an advanced technology widely used for gas separation and storage applications. Several types of adsorbents including zeolites, metal organic frameworks (MOFs) and activated carbon have been characterized and synthesized with a wide array of applications in PSA operations [1–4]. Numerous hypothetical zeolites and MOFs can be also conceptualized [5,6]. The available feedstock specifications and the desired product specifications determine the choice of adsorbent and process design. A majority of the previous material-screening studies either use computationally expensive methods, and are able to screen only a few prospective adsorbents, or use significant modeling assumptions thereby leading to uncertainties in the reported results. This incentivizes the search of more predictive methods and metrics that can correctly estimate material performance in a real processing operation. We have developed a high-throughput material screening framework that is capable of quickly screening millions of potential adsorbent materials. The framework integrates a high-fidelity adsorption process model with an artificial neural network (ANN) model to describe breakthrough dynamics in adsorber columns. Detailed Grand Canonical Monte Carlo (GCMC) simulations are performed for obtaining equilibrium adsorption capacity data for different adsorbent-adsorbent pairs. We introduce ANN-based surrogate models for predicting breakthrough dynamics in an adsorber column. The resulting models have much less complexity compared to the detailed first-principles model, and can be leveraged for rapid screening of a large adsorbent database. The developed framework can be used to evaluate material separation performance for different process designs, operating conditions and adsorbent materials. For example, it can predict the so-called “breakthrough time” that can be used as a single metric to capture the effects of both adsorption capacity and selectivity [7]. Breakthrough time also influences the column size and regeneration frequency. This metric is shown to facilitate a more efficient and realistic screening of adsorbent materials as it combines the effects of adsorption capacity and selectivity to indicate PSA performance. To demonstrate the utility of the framework, we screen 196 pure-silica zeolites in the Structure Commission of the International Zeolite Association (IZA-SC) database for post-combustion carbon capture and natural gas purification. Using the framework, we have identified zeolites WEI and JBW as the top two candidate adsorbents, as they are consistently ranked among the top zeolites for both applications.References[1] L.J. Murray, M. Dincua, J.R. Long, Hydrogen storage in metal--organic frameworks, Chem. Soc. Rev. 38 (2009) 1294–1314.[2] D. Farrusseng, Metal-organic frameworks: applications from catalysis to gas storage, John Wiley & Sons, 2011.[3] A.U. Czaja, N. Trukhan, U. Müller, Industrial applications of metal--organic frameworks, Chem. Soc. Rev. 38 (2009) 1284–1293.[4] J.-R. Li, R.J. Kuppler, H.-C. Zhou, Selective gas adsorption and separation in metal--organic frameworks, Chem. Soc. Rev. 38 (2009) 1477–1504.[5] A.N. Dickey, A.Ö. Yazayd\in, R.R. Willis, R.Q. Snurr, Screening \ce{CO}2/\ce{N2} selectivity in metal-organic frameworks using Monte Carlo simulations and ideal adsorbed solution theory, Can. J. Chem. Eng. 90 (2012) 825–832.[6] K. Sumida, D.L. Rogow, J.A. Mason, T.M. McDonald, E.D. Bloch, Z.R. Herm, T.-H. Bae, J.R. Long, Carbon dioxide capture in metal--organic frameworks, Chem. Rev. 112 (2012) 724–781.[7] R. Krishna, J.R. Long, Screening metal-organic frameworks by analysis of transient breakthrough of gas mixtures in a fixed bed adsorber, J. Phys. Chem. C. 115 (2011) 12941–12950.",31,2.0
"Particles play a key role in many industrial productions, where particle processes are frequently used for removal of insolubles, product isolation, purification and polishing. Because particle processes are quite complex and fundamental understanding of underlying phenomena is lacking, control and monitoring these processes are often challenging tasks. To overcome these challenges, it has previously been examined whether hybrid modelling could capture particle dynamics and be used to optimize these processes. Shallow neural networks have here been used to estimate particle phenomena kinetics and combined with low-order population balance models to produce predictions of particle size evolution. During the last two decades, hybrid models have been employed in various particle processes, including crystallization operations [1,2,3] and a pharmaceutical milling process [4]. While these models have shown good predictive capabilities, they have been rather case-specific and been trained either using indirect process variables or using fairly small data sets of particle size distributions due to historical limitations of particle analysis methods. With recent developments in particle monitoring tools, such as dynamic image analysis and focused beam reflectance measurements, it is now possible to measure particle properties with a much higher frequency, allowing for better capture of the process dynamics.In this work, a hybrid modelling framework is presented for particle processes that include all the common particle phenomena, including nucleation, growth, shrinkage, agglomeration and breakage. The hybrid model is trained using time-series data from an on-line/at-line particle analysis sensor, along with other measured process states that may have an impact on the phenomena kinetics. The particle phenomena kinetics are estimated using a deep neural network, where all the available measured and calculated process states are used as inputs. The kinetic model is combined with a medium-resolution discretized population balance model and mass balances, ensuring that physicochemical constraints are not violated. With the high flexibility and the automatic feature selection capabilities of the deep neural network, it becomes possible to model an arbitrary particle process without relying on excessive prior process understanding. The only requirement is that the process dynamics related to the particle phenomena and the particle size distribution can be measured during the process.The presented modelling approach utilizes automatic differentiation for training of the hybrid model, which significantly speeds up the model training from an order of O(2n) using a finite difference method to O(1), where n is the number of weights in the neural network. This opens up for training the hybrid model in real-time, where new sensor data can be continuously incorporated into the hybrid model during process operation. It furthermore allows for easy scaling of the neural networks to higher complexity without significant additional computational costs.The versatility of the hybrid modelling framework is demonstrated through three case studies of varying nature and scale, where on-line/at-line dynamic image analysis has been used to measure particle properties during process operation. The case studies include a lab-scale crystallization of lactose, an industrial scale crystallization of an active pharmaceutical ingredient and a lab-scale flocculation and breakage of silica particles. We demonstrate how data quality and quantity affects the model performance and compare with traditional first principles modelling.It is furthermore illustrated how computational chemistry methods can bring the framework an additional understanding of nanoscale particle interactions [5] by including first-principles model-based soft-sensors as inputs to the deep neural network. This includes estimations of solid-liquid interfacial tension (IFT), which can predict attractive or repulsive forces between particles in a robust way, much faster than experimental measurement of surface forces.Lastly, a couple of future perspectives are presented, including the use of the hybrid model in model predictive control (MPC) where the hybrid model can be trained in parallel with solving the MPC problem.[1] P. Lauret, H. Boyer, J. Gatina, Hybrid modelling of the sucrose crystal growth rate, Int. J. Model. Simul. 21 (2001) 23-29. doi:10.1080/02286203.2001.11442183.[2] V. Galvanauskas, P. Georgieva, S. F. D. Azevedo, Dynamic optimisation of industrial sugar crystallizationprocess based on a hybrid (mechanistic+ann) model (2006). doi:10.1.1.124.7855.[3] Y. Meng, S. Yu, J. Zhang, J. Qin, Z. Dong, G. Lu, H. Pang, Hybrid modeling based on mechanistic and data-driven approaches for cane sugar crystallization, J. Food Eng. 257 (2019) 44-55. doi:10.1016/j.jfoodeng.2019.03.026.[4] P. K. Akkisetty, U. Lee, G. V. Reklaitis, V. Venkatasubramanian, Population balance model-based hybrid neural network for a pharmaceutical milling process, J. Pharm. Innov. 5 (2010) 161-168. doi:10.1007/s12247-010-9090-2.[5] N. Nazemzadeh, L. W. Sillesen, R. F. Nielsen, M. N. Jones, K. V. Gernaey, M. P. Andersson, S. S. Mansouri, Integration of Computational Chemistry and Artificial Intelligence for Multi-scale Modeling of Bioprocesses, Accepted in 30th European Symposium on Computer Aided Process Engineering, 2020.",31,3.0
"Increased market volatility, sustainability concerns and process flexibility are a few of the main challenges contemporary process industries are facing. At the same time major manufacturing areas, like the pharmaceutical industry, have started to investigate the continuous processing paradigm as more profitable and efficient. Moving in these directions, integration of control with operations has received increasing attention over the last decade [1,2]. Integrated planning, scheduling and control (iPSC) aims to exploit the inherent reciprocal information flow among the three problems so as to enhance feasibility, robustness and profitability of the operations in the process industries. The resulting problem tends to be computationally intensive and so far has been studied mostly under deterministic assumptions [3,4] with few exceptions who can be employed under restrictive assumptions [5]. As far as the incorporation of control considerations within these problems, the use of model predictive control [2,3], multi-parametric programming [6,7] as well as surrogate-based and data-driven approaches have been proposed [8,9]. Despite the research effort in this area the development of systematic, computationally efficient and data-driven methods remains an open challenge. On top of that,  the underlying dynamic optimization of the control problem suffers from three conditions: (i) there is no precise known model for most industrial-scale processes (plant model mismatch), leading to inaccurate predictions and convergence to suboptimal solutions; (ii) the process is affected by endogenous uncertainty (i.e. the system is stochastic) & (iii) state constraints must be satisfied due to operational and safety concerns. Therefore constraint violation can be detrimental. To solve the above problems, Reinforcement Learning (RL) Policy Gradient method is proposed, which satisfies chance constraints with probabilistic guarantees [10]. The resulting optimal policy is a neural network designed to satisfy the optimality conditions, and the optimal control actions can be evaluated fast as the policy requires only function evaluations. In this work, a novel framework for closed-loop integration iPSC under dynamic disturbances and uncertainty is proposed. The key elements are the integration of novel RL-based optimal policy for the control task of an uncertain dynamic physical system with the optimization-based algorithm for the efficient rescheduling that mitigates the impact of exogenous disturbances on the real-time implementation. Finally, the proposed framework is tested on the iPSC of a nonlinear industrial process illustrating its merits and providing key insights about the interdependence of the iPSC decisions and the importance of RL for its real-time implementation.  Grossmann, I. E. (2012). Advances in mathematical programming models for enterprise-wide optimization. Comput. Chem. Eng., 47, 2-18. Chu, Y., and F. You. (2015) Model based integration of control and operations: Overview, challenges, advances, and opportunities. Comput. Chem. Eng. , 83, 2-20. Dias, L. S., & Ierapetritou, M. G. (2016). Integration of scheduling and control under uncertainties: Review and challenges. Chem. Eng. Res. Des., 116, 98-113. Georgiadis, G.P., Elekidis, A.P. and Georgiadis, M.C. (2019). Optimization-Based Scheduling for the Process Industries: From Theory to Real-Life Industrial Applications. Processes, 7, 438. Charitopoulos, V. M., Aguirre, A. M., Papageorgiou, L. G., & Dua, V. (2018). Uncertainty aware integration of planning, scheduling and multi-parametric control. Comput. Aid. Chem. Eng. 44, 1171-1176. Charitopoulos, V. M., Papageorgiou, L. G., & Dua, V. (2019). Closed-loop integration of planning, scheduling and multi-parametric nonlinear control. Comput. Chem. Eng., 122, 172-192. Burnak, B., Katz, J., Diangelakis, N. A., & Pistikopoulos, E. N. (2018). Simultaneous process scheduling and control: a multiparametric programming-based approach. Ind. Eng. Chem. Res., 57(11), 3963-3976. Du, J., Park, J., Harjunkoski, I., & Baldea, M. (2015). A time scale-bridging approach for integrating production scheduling and process control. Comput. Chem. Eng., 79, 59-69.  Dias, L. S., & Ierapetritou, M. G. (2019). Data-driven feasibility analysis for the integration of planning and scheduling problems. Optim. Eng., 20(4), 1029-1066. Petsagkourakis, P., Sandoval, I. O., Bradford E., Zhang, D. & del Rio-Chanona, E. A. (2020). Constrained Reinforcement Learning for Dynamic Optimization under Uncertainty. Accepted for publication in 21st IFAC World Congress",31,4.0
"Safety considerations have been integrated into feedback control system design to yield cooperative actions to ensure both operational safety and economic performance of system operation via control Lyapunov-barrier functions (CLBF) [1,2]. Specifically, in [2] a CLBF-based model predictive control scheme was proposed to optimize process performance accounting for process stability and safety requirements. While MPC systems implemented in an industrial setting often utilize linear data-based empirical models to compute control actions to maintain optimal process operation while accounting for process and control actuator constraints, chemical processes are inherently nonlinear and often require nonlinear models in order to be controlled efficiently. Modern machine learning modeling tools provide an efficient way to nonlinear process modeling, taking advantage of advances in training, efficient coding and computational power [3, 4, 5].In this work, we develop a machine-learning-based CLBF-MPC based on an ensemble of recurrent neural network (RNN) models that are widely-used to model nonlinear dynamic systems for prediction to control an input-constrained nonlinear process accounting for stability and safety considerations. Specifically, RNN models are first developed to model a general class of nonlinear systems using process operating data, and sufficient conditions that account for bounded modeling error between the RNN model and the actual nonlinear process are provided to achieve closed-loop stability and safety for the nonlinear process under CLBF-MPC. Additionally, following the design of machine-learning-based CLBF-MPC, the CLBF-based economic MPC using RNN models is proposed to optimize process economic benefits as well. Moreover, to handle model uncertainty issue in real-time implementation of controllers, on-line learning of RNN models is also employed within CLBF-MPC and CLBF-EMPC to update process models in the presence of time-varying disturbances. [1] Romdlony, M. Z., and Jayawardhana, B. Stabilization with guaranteed safety using control Lyapunov–barrier function. Automatica, 66, 39-47, 2016. [2] Wu, Z., F. Albalawi, Z. Zhang, J. Zhang, H. Durand and P. D. Christofides, ""Control Lyapunov-Barrier Function-Based Model Predictive Control of Nonlinear Systems,'' Automatica, 109, 108508, 2019. [3] Kosmatopoulos, E. B., Polycarpou, M. M., Christodoulou, M. A., and Ioannou, P. A. High-order neural network structures for identification of dynamical systems. IEEE transactions on Neural Networks, 6, 422-431, 1995[4] Mohanty, S. Artificial neural network based system identification and model predictive control of a flotation column. J. Process Control, 2009, 19, 991−999.[5] Wu, Z., A. Tran, D. Rincon and P. D. Christofides, ""Machine Learning-Based Predictive Control of Nonlinear Processes. Part I: Theory,'' AIChE J., 65, e16729, 2019.",31,5.0
"The optimization of chemical processes presents distinctive challenges to the stochastic systems community given that they suffer from three conditions: There is no precise known model for most industrial scale processes (plant model mismatch), leading to inaccurate predictions and convergence to suboptimal solutions.The process is affected by disturbances (i.e. it is stochastic). State constraints must be satisfied due to operational and safety concerns, therefore constraint violation can be detrimental or even dangerous.To solve the above problems, we propose a Reinforcement Learning (RL) Policy Gradient method, which satisfies chance constraints with probabilistic guarantees. Machine learning is helping to address complex problems in the chemical and process industries, such as process optimal control [1,2], estimation and online monitoring [3,4]. However, less studies have been conducted to investigate the applicability and efficiency of RL in process engineering, and none include the efficient handling of constraints. RL would be a natural choice to address nonlinear, uncertain and stochastic process control problems as it effectively addresses stochastic environments [5]. Unfortunately, present RL algorithms fail to reliably satisfy state constraints even when initialized with feasible initial policies [6]. Various approaches have been proposed in the litera-ture, where usually penalties are applied for the constraints.Such approaches can be very problematic, easily losing op-timality or feasibility [7] especially in the case of a fixed penalty. The main approaches to incor-porate constraints in this way make use of trust-region and fixed penalties [9],as well as cross entropy [7]. As it is observed in [8], when penalty methods are ap-plied in policy optimization, depending on the value of the penalty parameter the behaviour of the policy may change.We propose a constrained RL algorithm which guarantees the satisfaction of joint chance constraints. To accomplish this, we propose the introduction of backoffs, which are computed simultaneously with the feedback policy. Backoffs are adjusted with Bayesian optimization using the empirical cumulative distribution function, which can, therefore, guarantee the satisfaction of joint chance constraints.[1] Bradford, E.; Schweidtmann, A. M.; Zhang, D.; Jing, K. and del Rio-Chanona, E. A., Dynamic modeling and optimization of sustainable algal production with uncertainty using multivariate Gaussian processes, 118, 143-158, 2018[2] del Rio-Chanona, E. A.; Fiorelli, F.; Zhang, D.; rashid Ahmed, N.; Jing, K.; and Shah, N, An efficient model construction strategy to simulate microalgal lutein photo-production dynamic process, 114(11), 2518-2527, 2017[3] do Carmo Nicoletti, M. and Jain, L. C., Computational Intelligence Techniques for Bioprocess Modelling, Supervision and Control, Volume 218 of Studies in Computational Intelligence, Springer Science & Business Media, 29 Jun 2009[4] Xiong, Z. and Zhang, J., Modelling and optimal control of fed-batch processes using a novel control affine feedforward neural network, Neurocomputing, 61, 317-337, 2004[5] Petsagkourakis, P.; Sandoval, I. O.; Bradford E.; Zhang, D. and del Rio-Chanona, E. A., Reinforcement Learning for Batch Bioprocess Optimization, 133, 2020[6] Wen, M., Constrained Cross-Entropy Method for Safe Reinforcement Learning, Neural Information Processing Systems (NeurIPS), 2018[7] Ray, A.; Achiam, J. and Amodei, D., Benchmarking Safe Exploration in Deep Reinforcement Learning, Deep RL Workshop NeurIPS 2019, arXiv:1910.01708, 2019[8] Achiam, J; Held, D.; Tamar, A. and Abbeel, P., Constrained Policy Optimization, International Conference on Machine Learning (ICML) 2017[9] Tessler, C.; Mankowitz, D. J.; and Mannor, S.; Reward Constrained Policy Optimization, International Conference on Learning Representations (ICLR) 2019",31,6.0
"The efficacy of chemotherapeutics such as temozolomide (TMZ) and its metabolic product 5-aminoimidazole-4-carboxamide (AIC) is often affected by the timing, quantity and frequency of dosages. There is strong interest in facilitating the ability to monitor efficacy in individual patients for specific subtypes of cancer. Real-time, dynamic measures of potency may also supplement or in some cases replace reliance on bio-imaging. Towards this end, in this work we develop new, synthetic molecular recognition sites for TMZ, and its decomposition product AIC, grafting them onto near infrared fluorescent nanoparticles capable of forming optode or other biosensor interfaces to monitor drug efficacy in real-time. Infrared fluorescent single-walled carbon nanotubes, wrapped using specific DNA oligonucleotides, are encapsulated within biocompatible, poly(ethylene glycol) diacrylate hydrogels and enable the selective recognition of an anti-cancer drug, TMZ, on U-87 MG human glioblastoma cells. In both solution phase and hydrogel form, the sensors exhibit a fluorescence response to TMZ with a detection limit of 30 M. Furthermore, U-87 cells exhibit no changes in viability for 7 days in contact with the hydrogel. The sensors were used to track the progression of glioblastoma death following TMZ administration. In addition, the sensors were able to monitor in vivo AIC and TMZ dynamics in SKH-1E mice. These results enable real-time detection of chemotherapeutic concentration and metabolism at the cellular and subcellular levels, with potential to increase the efficacy of cancer treatments.",32,0.0
"The glucose biosensor, built upon the redox enzyme glucose oxidase, is the most commercially successful and studied enzymatic biosensor. However, the lack of available and functionally validated enzymes is prohibiting the development of redox-based sensors for other important analytes. Herein, we present the development and assessment of an electrochemical nicotine biosensor, using genomic screening to identify the gene for a known nicotine catabolizing redox enzyme. The resulting nicotine biosensor demonstrated a specific, sensitive, and stable operational profile with a limit of detection of 27 μM over the range of 0-200 μM. This range is well within the physiological concentrations of nicotine present in smoker urine. Specificity and cross-reactivity were measured against structurally similar compounds to nicotine as well as to known physiological by-products. Our results highlight that this novel enzymatic electrochemical nicotine biosensor possesses operational capabilities for monitoring of nicotine in physiologically relevant conditions. The screening methodology can be generalized for the discovery of enzymes for novel sensor development.",32,1.0
"Electroenzymatic biosensors for neurotransmitters are uniquely compact, sensitive, and selective devices that provide the neuroscience community with a method to monitor fluctuating chemical concentrations in the brain. Many studies primarily consider glutamate, the most common excitatory neurotransmitter, and study the relation between sub-second neurotransmitter dynamics and various stimuli, behaviors, or diseases. Challenges, however, have remained in the optimization of sensor design and in clarifying the significance of assumptions made during in vivo use. Accordingly, a number of simulations were carried out to guide the development of glutamate sensors and to test the limitations of the most prominent assumption, that sensor response can be linked to the immediate extracellular neurotransmitter concentration using a predetermined calibration factor. Recommended modifications to sensor design were developed and successfully implemented, improving sensitivity six-fold (up to 320 nA/µM/cm2) with an accompanying order of magnitude decrease in response time (down to ~80 ms). Since even this improved response time is an order of magnitude slower than the timescale of synaptic neurotransmitter release, it is clearly wrong in many cases to assume that the use of a calibration factor to translate sensor response to neurotransmitter concentration without any signal deconvolution is accurate. To investigate the significance of this assumption and the conditions for which this practice remains reliable, the model was expanded with a new set of partial differential equations to describe the extracellular space of the brain. Considering the effects of variations in the local biological environment bordering a sensor, a range of representative parameters for some biological processes were selected. Comparisons were made between the theoretical responses expected from in vitro calibrations, tonic steady-state glutamate presence, and bolus-type glutamate releases that mimic synaptic release. Surprisingly, in steady-state cases, the sensor response could indicate a concentration over twice the actual value, while in cases where sensors were 10 µm or more from the source of glutamate being detected, sensor response was < 50% of the expected value. In some cases, transient concentrations due to a single synaptic-type release of neurotransmitters at a moderate distance from the sensor were not detected by the sensor at all, and when they were, the time course of signal decay did not match the rates of reuptake expected in the brain without significant signal deconvolution. Ultimately, results show that additional characterization of sensor performance in vivo may be necessary to enable proper interpretation of sensor data.",32,2.0
"Electrochemical immunosensors integrate biological recognition molecules (e.g., antibodies) with redox enzymes (e.g., horseradish peroxidase (HRP) to combine the advantages of immunoassays (extremely high sensitivity and selectivity) with those of electrochemical biosensors (reproducible, quantitative electrical output). Mechanistic mathematical models that describe the multiple mass-transfer and chemical reaction steps that give rise to the electrical output are needed to help design, optimize, and validate electrochemical immunosensors for medical applications. In this study, we developed such a model of an electrochemical immunosensor that used HRP as a reporter enzyme on the secondary and validated it experimentally. The model consists of a system of non-linear mass-balance equations that describe simultaneous mass transfer and chemical reactions of the reactants (catechol and hydrogen peroxide) that HRP oxidizes, as well as the HRP product, O-quinone, which is reduced at the working electrode to give the amperometric output. A bi-substrate, ping-pong reaction mechanism was used to describe the enzymatic reaction, and the Butler-Volmer kinetic model was used to describe the reduction of o-quinone back to catechol at the working electrode’s surface. The system of equations was solved numerically using the BVP4C function in MATLAB.Experimental data for model validation was obtained using electrochemical immunosensors for a surrogate protein antigen (mouse IgG). The immunogens interface was fabricated on the gold working electrode of DropSens screen printed electrode arrays. The steady-state amperometric output of the resulting immunosensors was measured over a range of four independent variables: the concentrations of the two reactants, the voltage of the working electrode, and pH. The experimental results were used to determine optimal operating conditions and to validate the model. The validated model was then used to calculate Damkohler numbers and flux-control coefficients to identify mass-transfer and reaction steps that most strongly affected the magnitude of the immunosensors’ output and the output’s sensitivity to analyte concentration.",32,3.0
"HTL Derived Biochar and Graphene Nanoplatelets for Biosensor ApplicationBharath K Maddipudi, Vinod Amar, Hope Dosch, Anuradha Shende and Rajesh V Shende*Department of Chemical and Biological Engineering, South Dakota School of Mines and Technology, Rapid City, South Dakota 57701, United States of America*Corresponding Author: Rajesh Shende, Department of Chemical and Biological Engineering, South Dakota School of Mines and Technology, Rapid City, South Dakota 57701, United States of America (E-mail: Rajesh.Shende@sdsmt.edu)AbstractBiosensors with Au electrode layer immobilized with peptides are currently being developed for the detection of microbial contamination. It is anticipated that the use of a low-k dielectric layer modified with graphene will improve the sensor sensitivity. The low-k dielectric layer can be modified with the biochar derived from hydrothermal liquefaction of different biomass substrates. In this investigation, HTL of lignocellulosic substrates was performed under non-catalytic and catalytic conditions and by-products were analyzed. The biochar was thermally treated in a tubular reactor under inert environment from 400-1100oC without any pretreatment. In other set of experiments, HTL derived biochar was acid treated and subsequently, thermal activation was performed. After the acid pretreatment and thermal treatment steps, the porosity, the specific surface area (SSA) and the morphology of the char was analyzed using BET and SEM/EDX. The char obtained after thermal treatment was combined with low-k dielectric layer and biosensor was fabricated. The sensor output was compared with the biosensor fabricated with low-k and graphene nanoplatelets. In addition, the sensor output was correlated with the char materials obtained under different thermal conditions. The results obtained on hydrothermal liquefaction of biomass substrates, characteristics of biochar before and after thermal treatment, biosensor fabrication and sensor output will be presented.",32,4.0
"A red emission BODIPY-based lysosome-targeted viscosity probe (Lys-VBOD) was synthesized for monitoring lysosomal viscosity in live cells. The increase of viscosity can restrict the twisting of double bond between indoleand BODIPY fluorophore, also restrict the rotating of single bond between quinoline and BODIPY, which lead to the enhancement of fluorescence intensity in high viscosity environment. The changes of fluorescence intensityexhibit a good linear relationship with viscosity values. Besides, Lys-VBOD is quite stable in various pH solvents with given viscosity, suggesting that Lys-VBOD is suitable for applying in a wide pH range. Moreover, Lys-VBODcan visualize the changes of lysosomal viscosity in live cells under the stimulus of dexamethasone. BSA titration experiment demonstrated that Lys-VBOD is stable in the presence of macromolecules and proteins. Notably, thefluorescence lifetime of Lys-VBOD in large viscosity solvents is extremely high. These results indicate that LysVBOD is an excellent tool for detecing viscosity changes in vitro and live cells.",32,5.0
"Free radicals are recognized as essential molecules for upholding the function of normal cells. Immune systems, such as bacteria elimination and virus inactivation, are examples of the crucial role that free radicals play in human cells. Even though free radicals have benefits, an optimum production is key to maintain their advantages, as an excess can cause the development of oxidative stress conditions, such as cancer, Alzheimer’s, and Parkinson’s diseases. Among all free radical species, hydroxyl radicals (•OH) are the most reactive and dangerous, and they can be used as biomarkers for the detection of those diseases at their initial stages. Thus, a very sensitive analytical sensor is needed to detect and monitor low concentrations of •OH. The integration of an electrochemical technique with a sensing element is regarded as a promising method for •OH detection due to a rapid and direct measurement without the pretreatment of samples. Thus, in this study, a glassy carbon electrode was modified with a sensing element comprised of ultrasmall cerium oxide nanoclusters (<2 nm) grafted onto a superconductive carbon (Figure 1a). XPS results showed 46.3% Ce3+ sites, which are those able to react with •OH. Cyclic voltammetry (CV) was implemented to characterize the interaction of the composite sensor with •OH, and they showed that the composite sensor was able to differentiate between solutions with and without •OH by an increased redox response in presence of •OH (Figure 1b). Furthermore, we observed a linear relationship between the redox response and the concentration of •OH in the range of 0.1 mM to 10 mM, with a limit of detection as low as 0.1 μM when using only 0.5 wt% CeOx loading.1(1) A.C. Alba-Rubio, D.S. Kim, S. Duanghathaipornsuk; Super sensitive sensor for the detection of hydroxyl free radicals with scavenging properties. Application number 62/977,945 (2020).",32,6.0
"Infections in a clinical setting can result from biofilm-forming species, which presents two key challenges: increased antimicrobial resistance, and poor diagnostic approaches to categorize antimicrobial susceptibility. Traditional antimicrobial susceptibility assay approaches are built upon planktonic bacteria (bacteria that are growing in solution). Biofilms are bacteria that grow in a community on a surface, and this community structure changes resistance to antimicrobials that is not reflected in these planktonic assays. While current biofilm-based assays exist, they are both time and material intensive, and can still provide antibiotic susceptibility results that may not reflect the actual biofilm response. Here we present our nanosensor based approach to monitor biofilm metabolic response during the administration of antibiotics. We grow biofilms with oxygen-monitoring nanosensors embedded throughout the film, and these nanosensors continuously report out the metabolic state of the biofilm. As we add antibiotics, we can measure what concentration can stop metabolism, and infer the minimum biofilm inhibitory concentration (MBIC) of the specific strain. Our diagnostic approach functions in 96 well plates with minimal materials and time (important for high throughput screening), and works with both lab and clinically relevant strains (demonstrated with Pseudomonas aeruginosa).",32,7.0
"Zeolites are nowadays abundantly used as adsorbents and catalysts for many industrial applications. Computational studies have recently complemented the widely available experimental data and observations on zeolites by shedding light on reaction mechanisms and the active sites. The computational studies of zeolites, however, are challenging due to the numerous possible active sites. A zeolite structure can have many unique tetrahedral sites (T). Substitution of a trivalent aluminum atom to a tetravalent silicon atom in the zeolite framework leads to a net negative charge that can be compensated by a proton or a cation. However, there are numerous possibilities for the placement of Al and the compensating proton or cation. The number of possibilities grow exponentially for higher Al/Si ratios in the framework. Due to this complexity, most theoretical studies have focused on a limited number of T sites based on physical intuitions. For example, many studies of the MFI zeolite, which has 12 T-sites, have focused only on one or two T sites based on the accessibility of guest molecule to be the active site. These choices are justifiable given the high computational cost of accessing the full descriptor space, however, they probably miss promising active sites. In this work, we attempt to thoroughly explore the possible active sites using a high-throughput approach. We apply our approach to the problem of NO adsorption on metal-exchanged zeolites. It is known that Pd-exchange CHA, for example, is an effective adsorbent to limit NOx emissions. Full exploration of this problem requires enumerating the possible Al-Al pairs on the starting zeolites, identifying the favorable sites for the compensating protons and cations and searching for a global minimum in each case, which means thousands of calculations even for simple zeolites. The problem is further compounded when the cation is a metal that might have multiple oxidation states. We show how we systematically explore the possible sites, starting with a lower level of theory on the initial calculations, and then narrowing the potential candidates and using a higher level of theory afterwards. The automation of this calculation reduces the amount of manual labor and highly improve the efficiency of the workflow. It presents a new approach to tackle the complicated problems related to zeolite catalysis.",33,0.0
"The controlled synthesis of advanced colloidal nanomaterials is limited by the currently available reactors utilized for their synthesis and the disjointed workflow between experimentation and data processing. Manual flask-based experimentation strategies, although applied across most research laboratories, are unlikely to sufficiently capture and control the complex and highly sensitive reaction systems of many colloidal nanomaterial syntheses. These batch reactors are slow, labor-intensive, and subject to batch-to-batch variations in experimental data sets. In response, over the last decade different microfluidic synthesis strategies have been developed and effectively applied towards the controlled synthesis of different classes of colloidal nanomaterials. Such flow synthesis strategies notably enhance the efficiency of colloidal synthesis studies, while simultaneously unlocking previously unattainable precision in synthesis parameter control. Despite the effectiveness of flow synthesis microreactors in accelerating reaction condition screening of colloidal nanomaterials, they still rely on user-directed experiment selection. Broad reaction space screening using design of experiments is materially inefficient, and expertly guided experiments are often too slow to effectively cover a synthesis space within the lifetime of unstable chemical precursors. These bottlenecks can be addressed through development of self-optimizing fluidic reactors and integrating fully automated experimentation, materials diagnostics, and data analysis with artificial intelligence-guided experiment-selection algorithms. Such autonomous materials synthesis platforms will be able to isolate optimal reaction parameters entirely without user intervention.Colloidal semiconductor nanocrystals (known as quantum dots) are a particularly suitable testbed for autonomous materials development strategies. Quantum dot syntheses possess a large and highly sensitive reaction parameter space.[1] Proper control of their reaction conditions is difficult to attain both within a single set of precursors and across research labs (batch to batch variation). As a result, despite extensive efforts and constantly growing applications, many synthesis paths are either unexplored or not fully optimized. One of the recently emerging classes of quantum dots is colloidal metal halide perovskites, which have highly favorable properties for applications in optoelectronic devices. Due to the combinatorially increasing synthesis space of metal halide perovskite quantum dots, many of their possible synthesis routes are novel and unexplored. Therefore, these reaction systems present an exciting opportunity for exploration within an autonomous flow synthesis platform.In this work, we present the first machine learning guided-fluidic reactor for the controlled synthesis of high-quality colloidal quantum dots. While the strategies and techniques detailed here may be easily applied to a large number of solution-processed materials, as a case study we analyzed the halide exchange of cesium-lead-bromide perovskite quantum dots with zinc halide salts and two different surface ligands.[2] We developed a novel in situ material diagnostic module to attain accurate real-time access to peak emission energy, emission linewidth, and photoluminescence quantum yield of the in-flow synthesized perovskite quantum dots. These output parameters, corresponding to the physicochemical and optoelectronic properties of quantum dots, were then simultaneously optimized through an objective function integrated within a custom neural network ensemble model and various decision-making policies (used to select conditions from the model). Utilizing the developed autonomous materials development platform, we evaluated the performance of eight different experiment selection algorithms (spanning machine learning to evolutionary algorithms) over the course of more than 1400 experiments. The controlled syntheses of perovskite quantum dots were performed without user intervention and required less than 400 µL of starting quantum dot solution per experimental condition. The methods detailed in this work offer autonomous reaction optimization across multiple vital performance parameters. Further use of similar self-optimizing systems in the field of colloidal nanoscience would reduce the cost and expedite the process of materials development and discovery.References[1] R. W. Epps, K. C. Felton, C. W. Coley, M. Abolhasani, Lab Chip 2017, 17, 4040.[2] K. Abdel‐Latif, R. W. Epps, C. B. Kerr, C. M. Papa, F. N. Castellano, M. Abolhasani, Adv. Funct. Mater. 2019, 29, 1900712.",33,1.0
"Within the framework of green chemistry, Deep Eutectic Solvents (DES) have been identified as promising candidates for use in many applications, including battery electrolytes. DES are characterized by two or three materials that associate with each other through hydrogen bond interactions, resulting in a eutectic mixture whose freezing point is below that of the individual materials. This design space is overwhelmingly large and poses a challenge for screening a vast and diverse set of materials. Here we present a strategic approach consisting of high throughput experimentation (HTE) coupled with data science driven analysis to identify exceptional DES candidates based on key physiochemical and electrochemical properties. Much of our HTE adopts methods that are already used frequently in the biotech and pharmaceutical industries, most notably performing parallel syntheses and analyses in 96-well-plate formats. DES samples are first synthesized using an open-sourced automated liquid handling robot. DES melting points are then determined by monitoring the melting process with an infrared camera and identifying the temperature at which the thermal conductivity of the samples changes abruptly. The solubility of battery redox-species is determined via UV-VIS well-spectrophotometers. Finally, the electrochemical stability window and cycling properties of DES electrolytes are measured in high-throughput by using screen-printed electrodes on 96-well plates adapted for use with a standard potentiostat. The ability to rapidly and efficiently collect data also creates a need for the development and use of automated processes for data analysis, which have been developed in an open-sourced format by our group. This approach to HTE also allows for the incorporation of data science techniques, such as feature extraction and machine learning, that further aid in probing a design space that is ultimately too large for experimental methods alone.",33,2.0
"Mathematical modeling plays a critical role in the design, optimization, and control of chemical processes [1]. In order to apply models to a wide range of process conditions, it is often desirable to describe the system using a first principles model. However, the formulation of such models can be challenging if the underlying mechanisms are only partially understood [2, 3, 4]. Under such circumstances, a data-driven model can be built from measured data, and the resultant model can describe the system adequately, even when mechanistic understanding is limited [5, 6]. However, a data-driven model has narrow applicability as it is tailored to describe the input-output relationships contained in the training datasets [7]. As an alternative, a hybrid modeling approach that combines first-principles and data-driven modeling has been proposed to describe a partially known process [7, 8]. Hybrid models have better prediction capabilities than first-principle models, and thus will have better generalizability and interpretability than data-driven models.In this work, we proposed a new numerical scheme to construct a hybrid model for a system that has partial mechanistic information. A hybrid model was constructed by introducing correction terms for estimating process-model mismatches between mechanistic model predictions and experimental measurements [9]. Here, a nominal model refers to a mechanistic model based on our prior knowledge of the system of interest, and the correction terms are assigned to differential equations of the nominal model so that the resultant hybrid model has improved prediction capabilities. The proposed methodology consists of three steps. First, observability analysis was performed for the nominal model to determine a subset of states that are observable from the available measurements. This step is critical to reduce the computational load of the estimation and avoid potential overfitting issues. Second, the available measurements were used to estimate the correction terms associated with the observable states through solving a L2-regularized least-squares problem [10]. Lastly, once the correction terms were estimated, the trajectories of the states were simulated based on the hybrid model. Then, an artificial neural network (ANN) model was developed for predicting the correction terms for a given set of state values, and the developed ANN was used within the hybrid model. Through incorporating the ANN, the hybrid model was able to predict dynamic responses to an input, which was not in the measurement data used for estimating the correction terms.The validity of the proposed method was demonstrated by constructing a hybrid model for the NFκB signaling pathway induced by both lipopolysaccharide (LPS) and brefeldin A (BFA). The signaling dynamics induced by LPS have been well studied, and hence, a previously published model was used as the nominal (mechanistic) model [11]. Since the dynamics initiated by BFA are less characterized, the nominal model could not capture the signaling dynamics induced by both BFA and LPS well. By implementing the proposed methodology, the developed hybrid model was able to predict the NFκB signaling dynamics induced by both LPS and BFA accurately. References[1] Hangos, K. M.; Cameron, I.T. Process modelling and model analysis. Academic Press: London, 2001.[2] Cho, K.-H.; Choo, S.-M.; Jung, S.H.; Kim, J.-R.; Choi, H.-S.; Kim, J. Reverse engineering of gene regulatory networks, IET Systems Biology 2007, 1, 149 - 163.[3] Willis, M.J.; von Stosch, M. Inference of chemical reaction networks using mixed integer linear programming, Computers and Chemical Engineering 2016, 90, 31-43.[4] Lee, D.; Jayaraman, A.; Kwon, J.S. Identification of a time-varying intracellular signalling model through data clustering and parameter selection: application to NF-κB signalling pathway induced by LPS in the presence of BFA, IET Systems Biology 2019, 13, 169-179.[5] Thompson, M.L.; Kramer, M.A. Modeling chemical processes using prior knowledge and neural networks, AIChE Journal 1994, 40, 1328-1340[6] Narasingam,A.; Kwon, J.S. Development of local dynamic mode decomposition with control: Application to model predictive control of hydraulic fracturing, Computers & Chemical Engineering 2017, 106, 501-511.[7] von Stosch, M.; Oliveira, R.; Joana Peres, J.; de Azevedo, S.F. Hybrid semi-parametric modeling in process systems engineering: Past, present and future. Computers & Chemical Engineering 2014, 60, 86-101.[8] Faizan Bangi, M.S.; Kwon, J.S. Deep hybrid modeling of chemical process: Application to hydraulic fracturing, Computers & Chemical Engineering 2020, 134, 106696.[9] Engelhardt, B.; Fröhlich H.; Kschischo, M. Learning (from) the errors of a systems biology model, Scientific Reports 2016, 6, 20772.[10] Howsmon, D.P.; Hahn, J. Regularization Techniques to Overcome Overparameterization of Complex Biochemical Reaction Networks, IEEE Life Sciences Letters 2016, 2, 31 – 34.[11] Lee, D.; Ding, Y.; Jayaraman, A.; Kwon, J.S. Mathematical modeling and parameter estimation of intracellular signaling pathway: application to LPS-induced NFκB activation and TNFα production in macrophages, Processes 2018, 6, 21.",33,3.0
"Background: The modern lab produces more data than it can analyze using traditional experimental techniques. For neuroimaging labs, terabytes of imaging data can easily be generated in the course of a single experiment. However, only a small portion of this data is accessible for extracting results based on existing methods. To enhance the portion of data that can be analyzed, many labs are turning to data science for high throughput processing and analysis. However, to synthesize highly skilled domain scientists with an effective data science strategy, a robust method for developing well-designed and data informed experimental pipelines must be followed. We developed a framework for fluorescent neuroimaging based experimental routines (FFIBER) that implements key practices of data science, software engineering, chemical engineering, and neuroimaging. The goal of this pipeline is to produce experimental workflows for high throughput neuroimage acquisition and analysis. For each experiment we follow a six-step process: (1) develop a data awareness, (2) design a data management plan, (3) determine an optimal experimental pipeline, (4) build out supporting data science infrastructure, (5) perform primary and supplemental imaging, and (6) produce interpretable visualizations of results. The objective of this project is to apply our FFIBER method during initial experimental stages and retroactively to generate, process, and analyze high volumes of neuroimaging data without an overall increase in labor-hours. Methods: To develop a data awareness, we performed a data assessment of relevant metadata, data storage, and data relationships. When designing a data management plan, we identified the optimal data storage locations, file structure for raw data and results, and personnel responsibilities. To determine an optimal experimental pipeline, we performed software reviews, addressed labor-hour bottlenecks, and identified necessary programs, packages, and scripts. Then we built out the supporting data science infrastructure utilizing Python and Jupyter notebooks as the base script for pipelines, electronic laboratory notebook integration, and version control with the team through GitHub and Google Drive. After the pipeline was built, we completed the primary set of experimental imaging using confocal microscopy of brain slices and fed the data through the pipeline for analysis. We then completed supplemental imaging to experimentally support a representative set of results obtained through FFIBER. Finally, we wrote Python scripts to output consistent interpretable visualizations of our analyses and align these with other experimental results. Results: We applied FFIBER to a data science experiment analyzing the phenotype of microglial cells in ex vivo brain slices to assess extent of glial cell activation. We were able to create and modify a pipeline during the initial stages of (1) an oxygen-glucose deprivation experiment and retroactively for (2) an inflammation-sensitized model using E. coli derived lipopolysaccharide, (3) and a neonatal hypoxia-ischemia (HI) model. A preliminary data assessment revealed a storage necessity of 5 GB per experiment with five individual file types from five separate personnel. We completed the data assessment and built a data management plan with university-supported Google Drive as our storage location, .tiff, .csv, and .ipynb as our file types, and established a slice-based, double-blind file structure. We wrote Python scripts automating image upload, cleaning and automatic feed to our analysis pipeline that reduced labor-hours from five hours per image set to less than ten minutes. Our software review led us to integrate a published cell morphometrics package, VAMPIRE, with our diff_register package for skeletonized morphometrics of microglia. Our image analysis pipeline developed with FFIBER split uploaded confocal images into fourths, segmented, skeletonized and identified the microglia in the images, and uploaded the images into our integrated morphometric analysis. FFIBER structured the development of our work pipeline that decreased work hours from a full day to less than an hour to analyze an entire experiment. Finally, our FFIBER structured pipeline enabled us to detect and quantify shape differences between non-treated and injured ex vivo slices – providing additional information for our initial oxygen-glucose deprivation experiment and retroactive insight to our inflammation sensitized and neonatal HI models. Our first application of FFIBER decreased our human labor time from > 24 hours per 5 GB image set to less than an hour, decreased our storage budget to $0, and produced information about cell morphometrics previously unavailable to our lab due to lack of analysis software and time constraints.Conclusions: By applying FFIBER, we were able to create workflow pipelines both during initial stages of experimental design and retroactively in neuroimage sets already collected. We decreased labor hours for data sets from 24 hours to less than an hour through using Python scripts rather than manual work and a well-designed pipeline. Our framework creates a high throughput approach for experimental design in research dependent on neuroimaging by integrating traditional experimental methods with modern data science applications. We can further iterate the methodology to include metadatabases for easier data access and data connections, budget vs storage matrices for determining best data storage location, and templates for electronic lab notebooks, file structures, data relationships, and generalized processing pipelines. Additionally, while FFIBER was developed for fluorescent neuroimaging datasets, the current workflow is robust, and flexible enough for application to other image analysis techniques, other organs, and other fluorescent imaging methods.",33,4.0
"Synthetic materials such as cationic polymers are poised to replace engineered viruses as vehicles for therapeutic nucleic acids. Polymeric materials offer a vast design space where desired material properties can be engineered by deft optimization of chemical composition and polymer architecture. However, rational polymer design is impeded by the “curse of dimensionality” since systematic examination of numerous variables such as composition, architecture, length and formulation parameters is paramount. To accelerate polymeric vector discovery, we need to develop models that predict the impact of key design attributes such as phase behavior, basicity, surface charge, nucleic acid binding, and hydrophobicity on the ultimate biological responses observed. In this work, we eschew low-throughput hypothesis-driven design of polymers in favor of a data-driven approach that exploits combinatorial polymer libraries and high-throughput experimentation. Thereafter, we employ statistical learning to extract the molecular attributes of the hit polymers that promoted gene editing efficiency and apply this knowledge to provide experimental guidance on designing subsequent polymer libraries.Drawing inspiration from high-throughput experimental approaches used in the pharmaceutical industry to screen drug libraries for therapeutic activity, we employed parallel polymer synthesis, formulation and well plate based biological assays for rapid screening of gene editing efficiency We synthesized a chemically diverse library of copolymers combining different ratios of 1) cationic monomers bearing amines spanning a broad range of basicity and 2) neutral monomers of varying hydrophilicity. Our combinatorially designed library allows for systematic investigation of the effect of amine basicity by studying variations in polymer pKa resulting from the use of 4 cationic monomers, in ratios of 100, 75, 50 and 25 %. Subsequent to parallel polymer synthesis, extensive physicochemical characterization was completed using automated tools -: composition and molecular weight analysis, pKa, polyplex size distribution, binding assays, and ζ-potential measurements were acquired in high-throughput modes to generate a rich dataset. Polymers were complexed with ribonucleoprotein (RNP) payloads and gene editing was quantified by estimating the proportion of mCherry positive cells. To resolve the trade-off between sensitivity and experimental throughput, we employed image cytometry and developed an image processing algorithm to quantify mCherry expression in a robust and automated fashion from a bank of images arising from 200 unique formulations. At the end of the high-throughput screening campaign, we obtained a high-performing hit polymer that outperformed state-of-the-art synthetic transfection reagents.Having identified a hit formulation from our library, the challenge was to: 1) unravel the relationship between polymer attributes and gene editing efficiency. 2) build on these structure-activity relationships to guide the design of future polymer libraries that will yield a higher “hit rate”. In order to derive these predictive relationships, we sought to understand how 10 polymer descriptors influenced biological performance. We turned to principal component analysis (PCA), to deal with the dimensionality challenge created by our dataset. Through PCA, we concluded that a single polymer descriptor cannot be used in isolation to guide the design of future libraries. Rather, complex non-linear relationships between several molecular attributes were responsible for editing performance. Discarding preconceived notions of how various chemical functional groups will influence delivery, we screened a large chemically diverse polymer library to discover design guidelines that do not conform to traditional heuristics as well as a promising hit polymer, which may not have been accessible through hypothesis-testing. If statistical learning and automated experimental workflows are applied in tandem, we can overcome challenges originating from the complexity of structure-function relationships governing polymeric gene delivery.",33,5.0
"Compliant strain sensing devices are widely applied in real-time mechanical feedback for soft robotic systems. However, it remains challenging to reach customized sensing devices with high sensitivity coordinated with designated working windows due to the complex and unclear structure-composition-property relationships which are hard to predict and pre-design. In this work, through active learning and developed algorithms, we propose machine assisted full-map structure-composition-property understandings of strain sensors consist of classical nanocomponents of Ti3C2Tx MXene, carbon nanotube, and poly(vinyl alcohol). Important variables of nanocomponent weight loading, nanocoating thickness, and nanocoating surface morphology are systematically investigated, and an informative data base (5000+ experimental resistance points, 500+ nanocoatings, and 200+ sensing devices) is constructed for training of machine learning models. Besides, comprehensive algorithm optimizations are performed for better machine performance. Ultimately, the machine learning models show low average scattered numerical error of 24% and give customized predictions in 1) sensing performance in a given material recipe; 2) suitable material recipes of sensing devices with high sensitivity and designated working windows in soft robotic systems. The reported strategies here open new perspectives towards design-free high-performance strain sensing devices and customized applications in soft robotic systems. More specifically, in our material systems, at least 33,700 non-repeating recipes are available for strain sensing devices fabrication although we limit the step length in each variables of material recipes. With such a complex system and very limited understanding, there are no available in silico simulation methods to interpret or predict the sensing devices. In addition, it is nearly impossible to screen the whole design space via Edison approach because the sensing needs are customized every time. Herein, we have proposed a data-driven framework to exploratively learn the full-map of the strain sensing devices. The data is collected with thirteen active learning rounds which investigate both the mean minimal distance (L2) between suggestion points and existing data points, and the numerical uncertainty of well-trained models during each round. After a representative dataset is constructed, optimization strategies including GA combination, customized loss function, and data augmentation are used for improving the model performance. This best model has achieved a low average scattered numerical error ((Predicted value - Real value)/Real maximum value in device) of 24%.",33,6.0
"Numerous important properties of solid oral therapeutics are associated with the chemical potential of the active pharmaceutical ingredient (API) in the solid form. These properties include aqueous solubility, dissolution rate, and physical stability in its crystalline form among others. Molecular modeling of the crystal structures of APIs to estimate the chemical potentials has become an increasingly popular approach in recent years for confirming the suitability of API solid forms for further development in silico. However, due to the computational expense of these approaches, the models are limited to time and length scales far smaller than macroscopic materials, and rely on simplifications such as microscopic cells with periodic boundary conditions, as well as cut-offs for long-range interactions. These approximations can have a significant effect on the chemical potential of organic small molecule crystals, particularly at small system sizes.Here we systematically examine the effect of supercell size and long-range cutoff distance on the computed chemical potentials of pharmaceutically relevant molecular crystals using classical molecular dynamics simulations. The Gibbs free energies of the crystal polymorphs and liquid phases of carbamazepine and sulfamerazine were computed from 0 K to 600 K for various system sizes and long-range cutoff radii. We compare the estimated chemical potentials with small systems and short cutoffs to those of the corresponding larger systems and show how these differences change with temperature. Furthermore, we show the effects that these small perturbations to the chemical potentials have on the macroscopic observables of enantiotropic transition temperature and solid-liquid melting temperature. Finally, we make recommendations for future molecular simulations of API crystal structures to balance the trade-off of chemical accuracy and computational efficiency.",34,0.0
"Solid dose pharmaceuticals continue to be the most common means of drug delivery worldwide; because of this, pharmaceutical production is dependent on effective crystallisation systems. The majority of industrial crystallisers are solvent based – hence, decisions surrounding solvent choice can drastically affect the outcome of the crystallisation process. Although attaining a high crystal yield of the active pharmaceutical ingredient (API) is typically the dominant objective during the solvent selection process, solvent and crystal properties can also influence health, safety, environmental, and operational performance. Indeed, the shape of the crystalline product can greatly impact downstream processes1; filtration can be hampered by needle- or plate-like particles that form dense cakes, high aspect ratio crystalline material may not readily flow during transportation, and tabletting via direct compression can fail if the solid product fractures during compaction. Moreover, crystal morphology can affect the in-vivo efficacy of the final pharmaceutical product.Despite the many material, time and cost constraints present during initial design stages, solvent selection for the crystallisation of novel API molecules is currently performed via time-consuming and expensive experiments, heavily reliant on past experience and rules-of-thumb2. Over the last decades, computer-aided molecular design (CAMD) methods have been developed to overcome such difficulties3, with the aim of guiding lab-based experiments towards optimal candidate molecules for crystallisation4,5. Whilst it is generally agreed that crystal shape has a significant role when designing optimal solvents, the integration of shape considerations within CAMD methods has principally focused on empirical approaches4 combining an understanding of the underlying chemistry with prior experimentation. Direct, quantitative methods, where mechanistic interpretations of crystal growth are considered6, have not yet received significant attention in formulating the solvent design problem. This is likely due to the absence of applicable group contribution approaches to these models, as well as the additional complexity of formulating and resolving a mixed-integer optimisation problem to represent these design choices.We extend the recent work of Watson et al.7, in which the SAFT-γ Mie equation of state8 was employed in a CAMD framework to identify optimal solvent blends and process conditions for integrated cooling and antisolvent crystallisation, to include the target morphology of the pharmaceutical product. By assuming a low supersaturation growth environment – a standard condition for industrial crystallisation – it is possible to calculate the relative growth rates of corresponding crystal faces6, thus providing a quantitative measure of aspect ratio of the crystalline API. The design formulation is implemented in gPROMS and is applied to the design of solvent mixtures for the crystallisation of ibuprofen, whereby the impact of crystal shape on the optimal solvent molecules and process conditions is investigated.Dandekar, P., Kuvadia, Z.B., Doherty, M.F., 2013. Annual Review of Materials Research 43, 359-386.Brown, C.J., McGlone, T., Yerdelen, S., et al., 2018. Molecular Systems Design & Engineering 3(3), 518-549.Gani, R., 2004. Computers & Chemical Engineering 28(12), 2441-2457.Karunanithi, A.T., Achenie, L.E., Gani, R., 2006. Chemical Engineering Science 61(4), 1247-1260.Jonuzaj, S., Gupta, A., Adjiman, C.S., 2018. Computers and Chemical Engineering 116, 401-421.Li, J., Tilbury, C.J., Kim, S.H., Doherty, M.F., 2016. Progress in Materials Science 82, 1-38.Watson, O.L., Galindo, A., Jackson, G., Adjiman, C.S., 2019. Computer Aided Chemical Engineering 46, 949-954.Papaioannou, V., Lafitte, T., Avendano, C., et al., 2014. The Journal of chemical physics 140(5), 054107.",34,1.0
"IntroductionSolubility is one of the most important properties in the development and design of drugs. It plays a big role in pharmacokinetics and pharmacodynamics, where it relates to dosing, toxicity and the effect of drugs on their targets. In the field of process design, knowledge on the solubility of drugs and other intermediate products are crucial in the design of separation techniques and for the transitioning from batch to flow processes. Computer-aided screening of the solubility of API’s can dramatically shorten the drug discovery cycle and the costs related to the development and manufacturing of new drugs.State of the art solubility models focus on the prediction of octanol-water partitioning coefficients (logP) and solid aqueous solubility (logS) because of their role in the evaluation of the drug-likeness of API’s in an initial screening process. Although models for octanol-water partitioning coefficients have improved a lot the last couple of years, the prediction of aqueous solubility for a broad range of drug-like compounds remains challenging. In view of downstream manufacturing and process intensification, the interest in solubility prediction is not limited to octanol or water as a solvent. Computer-aided screening of potential solvents can contribute significantly to a reduction of waste streams and ease the upscaling towards continuous processing. However, models for solubility predictions in other organic solvents are limited by the scarce availability of experimental data.This works aims at using machine learning techniques and physical relationships to predict the solubility of gases, liquids and solids in a variety of solvents. Theoretical quantum calculations are used to compensate for scarce and biased experimental databases. Furthermore, the physical relation between gas-liquid solvation energies, liquid-liquid partitioning coefficients and solid solubility is used to aid solubility predictions for drug-like molecules in a broad range of solvents.MethodologyGraph convolutional neural networks are used to encode the molecular structure of both the solute and solvent. A molecular embedding is learned separately for the solute and solvent with the directed message passing neural network, as implemented in Chemprop. The solute and solvent embeddings are concatenated and fed through another dense layer for the property prediction.Quantum chemical calculations are done to determine the gas-liquid solvation free energies for various solute and solvent combinations. These calculations are used to pretrain the message passing neural network, such that physical interactions between solvents and solutes can be learned. Contrary to the experimental gas-liquid solvation free energies, this dataset is not biased towards certain solvents (eg. water). Using theoretical data to pretrain the model also reduces the effect of aleatoric errors, caused by data noise, on the final model predictions. The dataset can be extended towards classes of solutes and solvents for which no or limited experimental data is available, improving the generalizability of the model.For the prediction of liquid-liquid partitioning coefficients and solid solubilities, the physical relationship with the well-established gas-liquid solvation free energies is enforced. Liquid-liquid partitioning coefficients are directly related to the ratio of the gas-liquid partitioning coefficients of the same solute in both solvents. Some error is introduced by the mutual solubility of the solvents, which will be accounted for in the loss function. The solid solubility is related to the gas-liquid partitioning coefficients and the fusion free energy of the crystal structure. To account for the fusion free energy, an additional model is trained on an experimental dataset of melting temperatures and fusion enthalpies.ResultsA preliminary theoretical dataset is constructed with COSMOtherm. It consists of >20k data points and includes 78 solvents and 244 solutes. Solvation free energies of the theoretical dataset are predicted by the model with a RMSE of 0.08 kcal/mol and a MAE of 0.04 kcal/mol. Analysis of the solute and solvent molecular embeddings demonstrate the learned physical interactions. A principal component analysis of the solvent molecular embeddings clusters solvents according to their physical properties relevant to solvation. Analysis of the atom messages for some solute molecules highlights atoms with an important hydrogen bonding/donating character.The model trained on the theoretical dataset is further refined using experimental data for solvation free energies. The experimental dataset is gathered from different sources including the Minnesota solvation database, the Compsol database and different publications related to the regression of Abrahams solute and solvent parameters. The total experimental dataset has more than 8900 entries for solute-solvent combinations with over 300 different solvents. The machine learning model uses molecular embeddings for the solute and solvent molecules that are learned from the theoretical dataset. The dense layer for property prediction is initialized by the learned weights from the theoretical dataset and optimized with the experimental solvation free energies. Solvation free energies in a randomly selected test set can be predicted with a RMSE of 0.60 (±0.04) kcal/mol and MAE of 0.27 (±0.02) kcal/mol. These results are similar compared to those of a model without pretraining. However, if no pretraining is done on a theoretical dataset, analysis of the molecular embeddings do not directly demonstrate learning of any physical interactions. Moreover, the predictions on “unseen” solvents, i.e. those not included in the training set, improve significantly if pretraining is applied.For the prediction of partitioning coefficients, a dataset available in OChem is used. This dataset includes 3600 entries for partitioning coefficients between water and various solvents including chloroform, cyclohexane, toluene and hexadecane. The model trained on theoretical gas-liquid solvation energies is used only for prediction of the experimental logP data with a RMSE 1.18 and MAE 0.66. Further efforts on improving predictions of partitioning coefficients will include (1) collection of a broader dataset (2) refine a pretrained model on experimental data enforcing the physical relation with gas-liquid solvation energies.Solid solubility can be related to gas-liquid solvation energies of the solute in the respective solvent and the solute fusion free energy. The fusion free energy of a crystal structure can be approximated with a known melting temperature and fusion enthalpy. Experimental databases on latter two properties are gathered from different sources, including OChem and NIST. The database is used to train a machine learning model with a directed message passing neural network to construct the molecular embeddings, as implemented in Chemprop. Fusion enthalpies are predicted with a RMSE of 1.69 (±0.14) kcal/mol and melting temperatures with a RMSE of 33.7 (±1.6) K. These predictions will be combined with the model for gas-liquid solvation energies to create a model for solid solubilities in a broad range of solvents.ConclusionsA machine learning model is developed for the prediction of gas-liquid solvation free energies. The molecular embeddings for solvent and solute are done with a directed message passing neural network. Experimental solvation free energies are predicted using the new model, pretrained on theoretical data, with a RMSE of 0.60 kcal/mol. The pretraining on a theoretical dataset ensures that the model captures the physical interactions between solvent and solute molecules and improves the performance of the model on unseen solvents.The physical relationship between gas-liquid solvation energies, partitioning coefficients and solid solubility will be used to enable the prediction of those properties in a broad range of solvents. The solute fusion enthalpy and melting temperature are required for the solid solubility predictions. Those are predicted by another model with a RMSE of 1.69 kcal/mol and 33.7 K respectively.FigureFigure 1. Parity plots for the predictions of gas-liquid solvation free energies ΔGsolv, fusion enthalpies ΔHfus and melting temperatures Tm. ",34,2.0
"In this talk, we will present a fully transferable deep learning potential that is applicable to complex and diverse molecular systems well beyond the training dataset. Recently we introduced ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies) or ANI in short. ANI is a new method and sampling procedure for training neural network potentials that utilizes a special kind of symmetry functions to build single-atom atomic environment vectors (AEV) as a molecular representation.The AI methods that focuses on the use of large and diverse data sets in training new potentials, has consistently proven to be universally applicable to systems containing the atomic species in the training set. Focusing on parametrization for organic molecules (with CHNOSFCl atoms so far), we have developed a universal neural network potential which is highly accurate compared to reference QM calculations at speeds 107 faster. The potential is shown to accurately represent the underlying physical chemistry of molecules through various test cases including: chemical reactions (both thermodynamics and kinetics), thermochemistry, structural optimization, and molecular dynamics simulations. The results presented in this talk will provide evidence of the universal applicability of deep learning to various chemistry problems involving organic molecules.",34,3.0
"The digital design of manufacturing processes using mechanistic models has been playing an increasingly important role in drug substance process development. It enables the study of unknown kinetics using experimental data, risk assessment at extreme process conditions where experimentation may be infeasible, and efficient exploration of decision space for critical process parameters. Calorimetry studies are an integral part of process development of synthesis steps in the pharmaceutical industry. Experimental kits, such as the Mettler-Toledo RC1 are used for this purpose with the typical aim of understanding the heat of the reaction, heat evolution, adiabatic temperature rise and therefore, safety of the chemical reactions under scrutiny. They can also see usage in reactive systems to regress kinetics of reactions, which are otherwise difficult to detect through offline composition analysis. In order to assess the thermal risks associated with a chemical reaction, both the desired and undesired (i.e., decomposition) reactions must be studied. Synthesis steps in pharmaceutical processes can be accompanied by significant heat release and must be understood well to manage them successfully at pilot and plant scales. In this study, we use mathematical modeling to predict the behavior of a reaction under different heat transfer profiles and simulate what would happen in the event of process disturbances such as a cooling jacket failure, or loss of control of fed batch (e.g. reactant added too quickly). The reaction was carried out in a 1L stirred tank reactor and the reactor temperature was maintained by a cooling jacket. When the reactor temperature is well regulated, only the primary reaction occurs, however in the event of a cooling failure, an undesired decomposition reaction occurs. Experimental data from RC1 heat flow measurements were obtained using experiments executed at different process temperatures. A fed-batch reactor model was developed and simulated in gPROMS FormulatedProducts. Reaction kinetics for the primary reaction were extracted using parameter estimation by fitting the RC1 data (Figure 1).In order to study the effect of temperature control failure, a pre-defined decomposition reaction was added to the model. Global system analysis (GSA) is an effective tool to explore design space, determine safe operating regimes for exothermic reactions, quantify effects of uncertainty, manage risk and identify opportunities for optimization. This tool was used to perform a sensitivity analysis by varying the heat transfer coefficients and initial temperatures within a specific range. The results of GSA give a clear picture of: 1) what the maximum temperature in the reactor would be if temperature control fails and hence the operation conditions that can trigger synthesis of undesirable products and impurities; 2) when the cooling failure occurs during the process, i.e. during the reactant addition or following reaction. The output from this may help to inform the control strategy for this reaction process, such as the setting of maximum reactant addition flow rates to ensure safe operation.",34,4.0
"Mathematical modelling of chemical reaction kinetics has proven to be a very effective tool for pharmaceutical drug synthesis development, where complex organic reaction networks are often encountered. The chemical and molecular complexity of modern drug molecules has been rapidly increasing, and this requires design and optimization of extensive synthesis routes which presents a challenge to process scientists. First-principles modeling can play an important role in streamlining this process. It helps with analyzing, interpreting and discerning the relationship between various process parameters and reaction profiles with minimal experimentation requirements.In this presentation, a modelling workflow to understand the Buchwald-Hartwig cross-coupling reactions is described. These reactions are ubiquitous in process chemistry. This is a non-homogeneous, metal-catalyzed reaction system encompassing multiple partially soluble reactants, a biphasic liquid mixture, and homogenous catalytic cycle in the organic phase. Challenges arise from subtle case-to-case variability, with different influential factors like mass transfer limitations, catalyst activation and water sensitivity.Using this case, the presentation will illustrate the following steps in a typical mechanistic modeling lifecycle: 1) Design and perform kinetic experiments; 2) Recreate experiments in silico; 3) Estimate reaction kinetic parameters; 4) Evaluate model predictions; 5) Postulate new mechanisms, kinetic rate expressions and/or perform additional targeted experimentation until predictions are satisfactoryHPLC measurements were available for key reactants and products from experiments conducted for a range of operating conditions. A mathematical model was developed in gPROMS FormulatedProducts starting from preliminary fitting of primary reaction pathway(s) assuming simple reaction mechanisms and solubility correlations.Using a systematic parameter estimation approach this was further expanded to a more rigorous model that includes effect of particle size distribution, mass transfer effects and relative concentration effects. The regression of kinetic parameters takes into account uncertainty in measurements and process parameters, which provides a robust model that can be used to make process predictions, and aid scale-up/technical transfer decisions.The model can be further used to understand the behavior of the system over a wide range of input conditions. This becomes particularly important to make scale up decisions and identify critical operating regimes early in the development of a pharmaceutical compound. Global system analysis was used to explore the design space and identify key factors that influence reaction selectivity and impurity generation. This allows for better understanding of the process and thus provides useful insights to support control strategy, optimization goals, and risk assessment.Keywords: reaction kinetics, parameter estimation, mechanistic modelling, digital design, catalyst",34,5.0
"Background: Ensure adequacy of mixing to assure uniformity and homogeneity and assure in-process testing and finished product conforming to specifications has been essential. As such, innovation on in-process monitoring and control of powder blending represents one exciting area for oral solid dosage form development and manufacturing. Over the past 18 years, quantitative calibration modeling approach using Process Analytical Technology (PAT) tools has been well documented in public literatures and regulatory submissions. In contrast, there are much less documents in the public domain regarding development of qualitative calibration-free approach for pharmaceutical applications. In this presentation, we will discuss a case study of integration of Design of Experiments (DOE) and PAT real time process monitoring for pharmaceutical blending end-point determination by calibration-free approach.Method: Following a Quality-by-Design (QbD) and risk-based approach to illustrate key steps of in-process control strategy establishment for powder blending case study: (a) identification of most influential formulation and process variables by applying multivariate data analysis techniques (such as principal component regression (PCR) to the powder blend characterization data; (b) further narrowing down to the most critical process parameters for in-process monitoring and control development by comparing the multivariate approach and univariate approach for powder segregation tendency modeling; (c) Integration of Design of Experiments (DOE) and Near Infrared (NIR) real time process monitoring for a narrow therapeutic index (NTI) drug powder blending process for process endpoint determination via calibration-free approach, recognizing that all blend quality attributes and process performance matrix are functions of time and will approach steady state values gradually. Two calibration-free approaches, i.e., moving block standard deviation method across the entire wavelength range, and examining the dynamics of specific API characteristic peak over time, are discussed.Results and discussions: Results from the chemometric modeling in this work suggests that moving window standard deviation (MWSD) method in conjunction with appropriate spectral data preprocessing algorithms, can be an effective means to evaluate the overall progression of the powder blending and blending process dynamics. Significant blending dynamics was observed for batches with large particle size difference between excipients. By monitoring specific NIR peak intensity of the API, it can further improve the method specificity.Conclusions: This work illustrated the feasibility of using integrated real time PAT monitoring to establish a calibration-free approach for end-point determination of a challenging powder blending process. It covers critical parameter identification via multivariate data analysis techniques, implementation of a real time PAT monitoring for a narrow therapeutic index (NTI) drug powder blending with particle size difference as the main factor, chemo-metrical data analysis and modeling with initial MWSD method and enhanced specific NIR peak method.",35,0.0
"Process analytical technology in the pharmaceutical industry often requires monitoring of critical quality attributes (CQA) through calibrated models. The development, implementation, and maintenance of these quantitative models are both resource and time-intensive[1].The methodology proposed in this study is based on Iterative Optimization Technology[2], and it requires minimum calibration and allows simultaneous prediction of the entire formulation instead of only the API with just one standard and pure component spectral data. Unlike Partial Least Squares (PLS), which needs tedious development of standards to incorporate variations in the process, this non-destructive methodology minimizes significant calibration effort by developing a mathematical model that uses only one standard and spectral information of pure powders present in the tablet. In continuous manufacturing, this approach will enhance real-time monitoring of CQA by predicting entire formulation using single calibrated model. The spectra of pure components do not require additional preparation as physical information from tablets can be extracted from the standard. It is a two-step calibration-minimum approach of which the first step is to compute the model parameter (λn) for each wavelength using a single standard tablet spectrum (Xmix) and spectra of pure powders (Xpure, i) as model development followed by the second step as predicting formulations (ri). In terms of model maintenance, it is flexible in incorporating factors such as change of spectroscopic instruments, variations in raw materials, environmental conditions, and methods of tablet preparation. Robustness of the proposed approach for variation in compaction (physical changes) and variation in composition (chemical changes) was evaluated. A PLS method was also constructed for reference.[1] Shi Z., et al., Applied Spectroscopy, 2012. 66(9): p. 1075-1081.[2] Muteki K., et al., Industrial & Engineering Chemistry Research, 2013. 52(35): p. 12258-12268.",35,1.0
"To continue the shift from batch operations to continuous operations for a wider range of products, advances in real-time process management (RTPM) are necessary. The key requirements for effective RTPM are to have reliable real-time data of the critical process parameters (CPP) and critical quality attributes (CQA) of the materials being processed, and to have robust control strategies for the rejection of disturbances and setpoint tracking.Real-time measurements are necessary for capturing process dynamics and implement feedback control approaches. The mass flow rate is an additional important CPP in continuous manufacturing compared to batch processing. The mass flow rate can be used to control the composition and content uniformity of drug products as well as an indicator of whether the process is in a state of control. This is the rationale for investigating real-time measurement of mass flow of particulate streams. Process analytical tools (PAT) are required to measure particulate flows of downstream unit operations, while loss-in-weight (LIW) feeders only provide initial upstream flow rates. The X-ray based sensor investigated by Ganesh et al.[1] and a novel capacitance-based sensor offered by Tech4Imaging LLC, (Columbus, OH), which have recently been investigated in our group both demonstrated the ability to effectively measure powder mass flow rates as well as to capture solid flow dynamics in the downstream equipment.Robust control strategies can be utilized to respond to variations and disturbances in input material properties and process parameters, so CQAs of materials/products can be maintained and the amount of off-spec production can be reduced. The hierarchical control system (Level 0 equipment built-in control, Level 1 PAT based PID control and Level 2 optimization-based model predictive control) was applied in the pilot plant at Purdue University and it was demonstrated that the use of active process control allow more robust continuous process operation under different risk scenarios compared to a more rigid open-loop process operation within predefined design space.[2-3] With the aid of mass flow sensing, the control framework becomes more robust in mitigating the effects of upstream disturbances on product qualities. For example, excursions in the mass flow from an upstream unit operation, which could force a shutdown of the tablet press and/or produce off-spec tablets,[4] can be prevented by proper control and monitoring of the powder flow rate entering the tablet press hopper.In the first part of this study, the impact of mass flow sensing on the control performance of a direct compaction line is investigated by using flowsheet modeling implemented in MATLAB/Simulink to examine the effects of sampling time, measurement precision and time delay. In the second part of this study, pilot plant studies are reported in which the mass flow sensor is integrated into the tableting line at the exit of the feeding-and-blending system and system performance data is collected to verify the effect of mass flow sensing on the performance of the overall plant-wide supervisory control. References[1] Ganesh, S., Troscinski, R., Schmall, N., Lim, J., Nagy, Z., & Reklaitis, G. (2017). Application of X-ray sensors for in-line and noninvasive monitoring of mass flow rate in continuous tablet manufacturing. Journal of pharmaceutical sciences, 106(12), 3591-3603.[2] Su, Q., Moreno, M., Giridhar, A., Reklaitis, G. V., & Nagy, Z. K. (2017). A systematic framework for process control design and risk analysis in continuous pharmaceutical solid-dosage manufacturing. Journal of Pharmaceutical Innovation, 12(4), 327-346.[3] Su, Q., Ganesh, S., Moreno, M., Bommireddy, Y., Gonzalez, M., Reklaitis, G. V., & Nagy, Z. K. (2019). A perspective on Quality-by-Control (QbC) in pharmaceutical continuous manufacturing. Computers & Chemical Engineering, 125, 216-231.[4] Martinetz, M. C., Rehrl, J., Aigner, I., Sacher, S., & Khinast, J. (2017). A continuous operation concept for a rotary tablet press using mass flow operating points. Chemie Ingenieur Technik, 89(8), 1006-1016.",35,2.0
"Powder blending is a critical unit operation in the manufacture of solid dosage forms. The uniformity of final blend has a direct impact on the unit dose uniformity of the finished drug product. Since blending is a complex operation that involves a large parametric space including processing parameters (i.e., blender RPM, fill level, loading order, and blending duration), equipment design, and material attributes, assessing powder blending uniformity (BU) is critical to ensure delivering the intended dose of the drug to the patient consistently. In-process testing of powder blends to demonstrate “adequacy of mixing” is a CGMP requirement (21 CFR 211.110). Since the withdrawal of FDA’s draft guidance document for industry “Powder Blends and Finished Dosage Units—Stratified In-Process Dosage Unit Sampling and Assessment” in 2013, various approaches have been developed to assess blend and/or dosage unit uniformity and submitted to the FDA in NDAs and ANDAs by applicants.In this presentation, we will provide case studies on in-process controls (IPCs) of blending uniformity and content uniformity (BU/CU) as well as FDA’s assessment on the IPCs as an integral part of the overall control strategy of the drug product manufacturing process. Our focus will be on how our assessment the proposed IPCs is linked to the critical quality attributes (CQAs) of the finished drug product. Additionally, considering the recent publication of ICH Q12, we will also discuss how ICH Q12 will affect our assessment of process parameters and IPC testing, especially with regards to applicant’s overall control strategies on scale-up for commercial production. By analyzing these cases, we wish to shed some light on FDA assessors’ current thinking on these important topics.",35,3.0
"IntroductionContinuous pharmaceutical manufacturing lines containing a wet granulation unit operation - like twin screw granulation - require the implementation of a subsequent drying step in order to achieve the desired granule moisture. Several continuously and semi-continuously operated dryers are available to the pharmaceutical industry, e.g., [1–3]. The semi-continuously operated ones [1,3] have in common, that the continuous material stream is split into separated mini-portions of material, each of them being dried in an individual cell of the dryer. In this study, the ConsiGma 25TM dryer will be investigated in detail. It consists of six drying cells that are consecutively filled and – after drying has finished – emptied.The most important material property during the drying process is the moisture content of the granules. If this quantity would be available for all the dryer cells, sophisticated feedback control strategies could be developed and implemented, ensuring that the granule moisture after drying is the same for all the processed granules. However, the systems that are commonly installed in industry only allow for the measurement of granule moisture in one cell and not in all of the cells. The moisture is mainly affected by the granules initial moisture content, the inlet air temperature, the air volume flow and the drying time. Therefore, a mathematical model that uses this data for predicting the moisture content in the individual cells is of interest. Further, the model should not be computationally demanding in order to allow its real-time execution in parallel to the process. This work presents such a model.MethodA mathematical model has been developed, which uses available process data in a ConsiGma 25TM drier for estimating the granule moisture content. The proposed model is based on mass- and energy balances. Some parameters of the model are known (e.g., material properties like specific heat capacity of water), whereas others need to be identified (e.g., heat and mass transfer coefficients). Since up to six cells need to be simulated simultaneously, the modeling approach was kept as simple as possible by proposing appropriate simplifications (e.g., lumped parameter model, perfect mixing). A set of experiments was planned and executed and the generated process data was used to identify the model parameters. A second set of experiments - at different process settings - was conducted for validating the proposed model.ResultsThe proposed model could predict the granule moisture of the validation dataset by an error of approximately 1wt.% (wet basis) of moisture, which is considered to be sufficiently accurate for the intended purpose. Further, it was demonstrated that the simulation could be executed approximately 400 times faster than real-time on an Intel XEON E3-1245V2, 3.4Ghz, desktop computer, suggesting that its execution is also feasible in real-time on a less powerful PLC.ConclusionA model for granule moisture prediction in a ConsiGma25TM drier has been successfully developed and implemented in simulation. By means of that model, in a next step advanced process control concepts can be realized in order to keep the granule moisture close to the desired value.ReferencesGlatt MODCOS line Available online: https://www.glatt.com/en/products/continuous-technologies-pharma/ (accessed on Apr 30, 2019).Bohle Qbcon 1 Available online: https://www.continuous-production.com/continuous-granulation-continuous-drying (accessed on Apr 29, 2020).GEA ConsiGma line Available online: https://www.gea.com/en/products/consigma-ctl.jsp (accessed on Apr 30, 2019).",35,4.0
"In recent years the pharmaceutical industry has been putting a lot of effort and resources for operating in a more continuous fashion [1]. Continuous manufacturing of solid oral dosage forms presents many benefits with respect to traditional batch production, and has been encouraged by the U.S. Food and Drug Administration [2]. One important bottleneck for this shift of paradigm is the implementation of an efficient feeding system, as any disturbance coming from the feeders will critically affect the downstream units and the quality of the product, even irreversibly [3,4].Loss-in-weight feeders are usually resorted to in drug product manufacturing. They are made up of a hopper, where the powder is collected and eventually refilled, an agitator, connected to a motor, and a screws system. Realtime monitoring of the feeder operation is inherently challenging, as the only available process measurement is the net weight of powder in the hopper. The mass flowrate of powder delivered by the feeder can only be obtained indirectly, as a discrete time derivative of the mass measurements in the hopper. The propagation of noise from the measured mass to the inferred mass flowrate can compromise the control of the powder flow and the monitoring of the whole manufacturing line. Commercial feeders tackle the problem by providing the user with a smoothed version of the indirect measurement of the mass flowrate, but this introduces a time delay with respect to the actual mass flowrate fed downstream. In presence of gross errors in the measurement of the mass in the hopper, the inaccuracy in the calculation of the mass flowrate is even greater.In this study we improve the accuracy of estimation of the mass flowrate coming out of a feeding unit by the implementation of a state estimator [5]. State estimators are widely employed in the process industry, but few contributions in the literature consider their application to continuous pharmaceutical manufacturing [6]. The implemented state estimator is based on a recently developed hybrid model [7] and is tested on experimental data coming from a feeding system. The estimation framework can accurately estimate the states of the system, namely the mass of powder in the hopper (deprived of the measurement noise), the actual mass flowrate provided by the feeder and the effective density in the hopper (an additional variable that cannot be measured online but that is important for process monitoring). The state estimator demonstrates to be robust to the presence of gross measurement errors and faults. The estimated states can be used for detecting faults in the feeding unit and to monitor the downstream operation.References[1] Ierapetritou, M., F. Muzzio and G. Reklaitis (2016). Perspectives on the continuous manufacturing of powder‐based pharmaceutical processes. AIChE J., 62, 1846–1862.[2] Lee, S. L., T. F. O’Connor, X. Yang, C. N. Cruz, S. Chatterjee, R. D. Madurawe, C. M. V. Moore, L. X. Yu and J. Woodcock (2015). Modernizing Pharmaceutical Manufacturing: from Batch to Continuous Production. J. Pharm. Innov., 10, 191–199.[3] García-Muñoz, S., A. Butterbaugh, I. Leavesley, L. F. Manley, D. Slade and S. Bermingham (2017). A flowsheet model for the development of a continuous process for pharmaceutical tablets: an industrial perspective, AIChE J., 64, 511–525.[4] Blackshields, C. A. and A. M. Crean (2018). Continuous powder feeding for pharmaceutical solid dosage form manufacture: a short review. Pharm. Dev. Technol., 23, 554–560.[5] W. H. Ray (1981), Advanced process control, McGraw-Hill, New York (U.S.A.).[6] Liu, J., Q. Su, M. Moreno, C. Laird, Z. Nagy, G. Reklaitis (2018). Robust state estimation of feeding–blending systems in continuous pharmaceutical manufacturing. Chem. Eng. Res. Des., 134, 140–153.[7] Bascone, D., F. Galvanin, N. Shah and S. García-Muñoz (2020). A hybrid mechanistic-empirical approach to the modelling of twin screw feeders for continuous tablet manufacturing. Ind. Eng. Chem. Res., in press",35,5.0
"Integrated process design provides many benefits such as supplying fundamental process knowledge, identifying important scaling parameters to ensure quality performance upon scale-up, and detecting safety and quality risks within the process. Herein, the principles of integrated process design were employed in the development and scale-up of an acetal protection reaction and subsequent crystallization. The challenges associated with the acetal protection reaction were instability of the product at end-of-reaction, solvent related impurity formation, and side reaction between reagents. Kinetic, thermodynamic, and multivariate statistical models, constructed through data-rich experimentation using parallelized reactors equipped with automated sampling, elucidated key sensitivities controlling reaction rates and selectivity. This enabled optimization of the reagent loadings and conditions to achieve reasonable reaction rates and stability at the end of reaction. In the subsequent crystallization, particle agglomeration and attrition caused slow filtration. Process analytical technology in scale-down studies of shear rates revealed shear sensitivity and particle attrition of agglomerates as well as the effect of the particle size distribution on filtration rate. Mixing models guided manufacturing operating conditions to minimize particle nucleation and attrition to achieve fast filtration. These process conditions were successfully scaled-up.",36,0.0
"The β-lactam antibiotics amoxicillin and cephalexin are among the most widely used antibiotic Active Pharmaceutical Ingredients (API) [1]. Penicillin G Acylase (PGA) can catalyze the reaction between an activated acyl donor (e.g. 4-hydroxyphenylglycine methyl ester) and a nucleophilic molecule containing the β-lactam core (e.g. 6-aminopenicillanic acid) to produce the final antibiotic (e.g. amoxicillin). Similar to many other industrial enzymatic processes, working with immobilized enzyme is essential for downstream separation of the catalyst from the product and retaining the valuable enzyme in continuous manufacturing processes. Working with immobilized enzyme necessitates understanding the effect of transport limitations on the enzyme activity and reaction rate. Moreover, this process meets difficulties such as potentially low conversion and yield due to hydrolysis of the API by the same enzyme such that the desired product is an intermediate in the overall reaction network. One way to improve this process and to protect the antibiotic from being hydrolyzed by the enzyme is isolating it through crystallization. The reactive crystallization also provides process intensification, as a separate downstream crystallization process for purification is no longer required [2].The complex interplay of three phenomena determine the process attributes: enzyme-catalyzed chemical reaction, species transport into and out of the biocatalyst, and crystallization of the API molecule. In this talk, we will describe our efforts to develop a mathematical model for the enzymatic synthesis of β-lactam antibiotics using immobilized enzyme. The effective diffusion coefficients for two types of commercially available supports (agarose and methacrylic-based beads De=1-5×10^-10 m2/s) were measured experimentally and the results were used to describe the effect of pore diffusion on the apparent enzyme activity and reaction rate. Furthermore, coupling the reaction-diffusion module with a crystallization population balance, a mathematical model that encompasses all three phenomena is developed. Application of the formulated model for model-based design and optimization will be discussed. In particular, this approach can aid in design of a biocatalyst to optimize the process attributes of conversion, selectivity, and productivity. To optimize the biocatalyst design into a systematic procedure, three directly controlled variables for biocatalyst are defined: (1) enzyme loading, (2) support type, and (3) support size. The interaction among these factors can be probed using the model for selecting the optimal immobilization condition. As one might expect, using an immobilization support with a radius smaller than a critical value Rc = F(enzyme kinetic, support porosity), selectivity and conversion comparable to that of the soluble enzyme can be achieved (Fig.1). On the other hand, increasing the biocatalyst size above an API-dependent value leads to significant mass transfer limitations and deviation from inherent kinetics. Furthermore, our analysis illustrates that two often neglected nonidealities in the enzyme immobilization procedure, nonuniform enzyme distribution and variance in bead size distribution, may affect the apparent reaction rate and the catalyst performance and cause deviations from targeted operating point (Fig.2).[1] Giordano, R. C., Ribeiro, M. P., & Giordano, R. L. (2006). Kinetics of β-lactam antibiotics synthesis by penicillin G acylase (PGA) from the viewpoint of the industrial enzymatic reactor optimization. Biotechnology advances, 24(1), 27-41.[2] McDonald, M. A., Bommarius, A. S., Rousseau, R. W., & Grover, M. A. (2019). Continuous reactive crystallization of β-lactam antibiotics catalyzed by penicillin G acylase. Part I: Model development. Computers & Chemical Engineering, 123, 331-343.",36,1.0
"During late stage pharmaceutical process development, objectives such as cost, environmentalsustainability, and risk to product quality, are important to consider. Optimization at this stage balancesthese objectives against the complexity of evaluating and incorporating process adjustments. Here, wepresent a series of process improvements made during late stage development of a small-molecule drugsubstance. These improvements were supported by empirical and mechanistic process understandingincorporating measures of risk, cost, and sustainability.To direct the investment of limited development resources, a quantitative analysis was done tocalculate key process metrics. A key insight from this analysis was that the process to produce onespecific intermediate was estimated to account for three quarters of the drug substance’smanufacturing cost. This analysis directed investment into improving the process yield across the eightchemical transformations, involving a wide variety of unit operations to make this intermediate.Specifically, significant cost reduction was achieved through multivariate optimization of an oxidationreaction, identification and mitigation of mass loss during extraction and distillation, and multivariateoptimization of yield vs. impurity purge during crystallization.Another improvement later in the synthesis involved the reduction of significant process riskpresent in a three-reaction telescope through the development of an additional isolation after the firstreaction in this sequence. The reduced process risk was quantitated as a failure rate, using multivariateBayesian modeling to ensure process robustness despite potential parameter variations. Finally, changeswere made to the penultimate process step that improved sustainability and reduced cost withoutincreasing process risk. These changes were enabled through generation of mechanistic models thatenhanced kinetic and thermodynamic understanding of the reaction and the subsequent water uptakeof the product. Developing fundamental engineering understanding and enhancing it with quantitativemetrics both informs where to focus development resources and provides a fully comprehensive datapackage summarizing knowledge, residual risks, and documenting improvements in a format that is easyto communicate to stakeholders and enables quality by design.",36,2.0
"In recent years, pharmaceutical companies have been making a transition from traditional batch manufacturing to continuous production mode, due to the well-known advantages of continuous operation [1]. Even though continuous implementations already exist for many pharmaceutical unit operations [2-3], major limitations leave continuous end-to-end manufacturing lagging behind, especially regarding the isolation of the drug substance [4]. In this work we address the state-of-the-art challenges in continuous filtration and drying for upstream manufacturing through the modeling, design and optimization of a novel intensified filtration-drying carousel. The unit is based on the prototype manufactured by Alconbury Weston Ltd [5], which has already been successfully tested in an integrated continuous crystallization-filtration experimental framework [6-7]. The carousel consists of five cylindrical ports, where slurry loading, filtration, washing, deliquoring and drying are carried out simultaneously. After a fixed time interval, the carousel rotates, moving each port to the next processing position. In position 1 the slurry from the crystallizer is introduced into the port, while in positions 2-4 the filtration, washing and deliquoring steps are carried out, with the order designed by the operator. A final gas drying step is carried out in position 5, before the product is discharged. Strict conditions on the solvent and impurity content of the discharged dry cake must be satisfied. The introduction of the gas drying equipment is a novel feature of the carousel, which was not present in previous applications [5-7]. Operating the carousel requires deciding on the value of multiple decision variables, namely the pressure drop for filtration and deliquoring, the type and flowrate of the washing solvent and the flowrate and temperature of the gas flow for the drying step. Above all, due to the intrinsic carousel mechanism, the rotation time has an important impact on the final product quality. Hence, the total cycle time has to be regulated considering a trade-off between the residence time at the processing positions. All this considered, the traditional design, based on empirical knowledge, of the filtration-drying operating conditions is very challenging and time-consuming. For this reason, in this work we develop a comprehensive mathematical model for the unit, to investigate an optimal design for the operating conditions of the carousel. The employed equations are standard macroscopic and microscopic mass, energy and momentum balances. We consider the effect of the crystal size distribution of the solid phase coming from the crystallizer on the cake properties, such as porosity [8] and specific resistance [9]. Finally, upon validation with experimental data, we use the model for determining the optimal values of the decision variables of the carousel for different API and solvent systems. Future work will involve model-based online control of the carousel.References[1] Lee, S. L., T. F. O’Connor, X. Yang, C. N. Cruz, S. Chatterjee, R. D. Madurawe, C. M. V. Moore, L. X. Yu and J. Woodcock (2015). Modernizing Pharmaceutical Manufacturing: from Batch to Continuous Production. J. Pharm. Innov., 10, 191–199.[2] Teoh, S. K., C. Rathi, and P. Sharratt (2016). Practical Assessment Methodology for Converting Fine Chemicals Processes from Batch to Continuous. Org. Process Res. Dev., 20(2), 414–431.[3] Su, Q., M. Moreno, C. Laird, Z. Nagy and G. Reklaitis (2019). A perspective on Quality-by-Control (QbC) in pharmaceutical continuous manufacturing, Comput. Chem. Eng., 125, 216–231.[4] Simon, L. L., A. A. Kiss, J. Cornevin, and R. Gani (2019). Process engineering advances in pharmaceutical and chemical industries: digital process design, advanced rectification, and continuous filtration. Curr. Opin. Chem. Eng., 25, 114–121.[5] Ottoboni, S., C.J. Price, C. Steven, E. Meehan, A. Barton, P. Firth, A. Mitchell and F. Tahir (2019). Development of a Novel Continuous Filtration Unit for Pharmaceutical Process Development and Manufacturing. J. Pharm. Sci., 108(1), 372–381.[6] D. Acevedo, R. Peña, Y. Yang, A. Barton, P. Firth, and Z. K. Nagy (2016). Evaluation of mixed suspension mixed product removal crystallization processes coupled with a continuous filtration system. Chem. Eng. Process. Process Intensif., 108, 212–219.[7] Liu, Y. C. , A. Domokos, S. Coleman, P. Firth, and Z. K. Nagy (2019). Development of Continuous Filtration in a Novel Continuous Filtration Carousel Integrated with Continuous Crystallization- Org. Process Res. Dev., 23(12), 2655–2665.[8] Ouchiyama, N., and T. Tanaka (1986). Porosity estimation from particle size distribution. Ind. Eng. Chem. Fundam., 25(1), 125–129.[9] Bourcier, D., J.P. Féraud, D. Colson, K. Mandrick, D. Ode, E. Brackx and F. Puel (2016). Influence of particle size and shape properties on cake resistance and compressibility during pressure filtration. Chem. Eng. Sci., 144, 176–187.",36,3.0
"The pharmaceutical industry is starting to adopt continuous active pharmaceutical ingredient (API) manufacturing in order to reduce production costs, improve manufacturing flexibility, reduce infrastructure costs, reduce manufacturing lead time (from typically 6 months to 10 days) and to improve sustainability. A further driver is reduction of variance in API quality critical attributes. To facilitate the complete transition from batch to continuous manufacturing it is necessary to “smartly” integrate single continuous unit operations to achieve a continuous material flow from synthesis to formulation. To achieve this smart integration of unit operations, a combination of modelling, online measurement and advanced control techniques are vital to predict product property outcomes, to monitor and control processes and to reduce the risk of non-conforming products.Another challenge the pharmaceutical industry faces is to reduce the quantity of material consumed during process development. A stretch goal is to consume just 100g of API (and the corresponding precursors) and to complete development in 100 days. Digital design of continuous API manufacturing offers a path to achieving this goal. This includes modelling and predicting process performance as a function of the operating conditions for both individual continuous unit operations and for the integrated processes with the aim of optimizing process design and reducing the laboratory time and cost needed to develop new products. Whilst a few examples of modelling integrated continuous unit operations using flowsheet models, have been published, these are mainly focused on secondary drug product manufacture rather than API synthesis and isolation.The team in the Continuous Manufacturing and Advanced Crystallization (CMAC) Future Manufacturing Research Hub have addressed this gap by modelling primary drug manufacturing using an integrated unit operations approach. A digital tool capable of transferring material property information between operations to predict the product attributes in integrated synthesis and purification processes has been developed. The focus of the work reported here combines filtration and washing operations used in API purification and isolation by combining predicted and experimental data generated during upstream crystallization process. The model approach described is subdivided into two levels of increasing system complexity where the filtration model approach is held constant, while different washing modelling approaches are used for the two levels in order to describe different washing scenarios. The filtration model approach considers the case where the different components of the input suspension are separated between the retained filter cake with residual impure mother liquor and the impure liquid filtrate removed during filtration. The assumptions considered for the washing modelling described in level 1 are that the impure mother liquor is fully miscible with the wash solvent selected and no changes in solid phase are considered (no particle dissolution or growth). Level 1 thus can describe three different washing mechanisms: pure displacement (level 1a), dilution with perfect liquid mixing (level 1b) and dilution with axial dispersion (level 1c). The more complex level 2 allows, additionally, the possibility of cake and impurity dissolution or precipitation during washing. The level 2 washing model considers cake dissolution due to solubility variation during washing, the possibility of API or impurity precipitation from solution due to a drastic solubility drop (the antisolvent effect occurring during washing) as an instantaneous process. This level presumes that kinetic aspects can be neglected and equilibrium is reached instantaneously. In detail; the level 2a model describes a washing process where the system is assumed to be homogeneous and the same solvent composition is assumed for the entire cake. In level 2b, a more realistic washing mechanism is modelled in which the system shows vertical heterogeneity and there is a composition gradient along the cake height, which is described in the modelThe integrated modelling tool uses information on the product crystal suspension characteristics predicted using gPROMS/Matlab to predict filtration time, filtrate flow rate and the composition of the filter cake and filtrate generated during filtration. The washing of the wet filtered cake is then simulated to predict; washing efficiency and to generate washing curves, cake and filtrate composition, indicate the probability of particle size variation caused by cake dissolution, and residual cake moisture content and composition. To validate the scenarios described using the integrated models a design of experiments (DoE) approach was used and two experimental case studies were investigated using mefenamic acid and paracetamol as representative isolation processes. In each case the objective of the work was to meet a product purity specification and minimize changes to the crystalline particle attributes occurring during the isolation process. The validation is also useful to identify which model level best reflects the experimental data, with the aim of identifying the minimum criteria a filtration and washing model needs to have to simulate experimentally verified isolation outcomes.",36,4.0
"Continuous manufacturing has many advantages over batch processing, including smaller footprints, faster turnaround times, and better control of process parameters and product quality. Direct compression is a minimal setup for a continuous tableting line and consists only of three unit operations: feeding of the individual components, mixing and tableting. The main goal of the mixing step is to achieve content uniformity in the final product, which is a critical quality attribute (CQAs).This work focuses on a vertical continuous mixing device termed Continuous Mixing Technology (CMT). The main of goal CMT goal is to mix incoming powder feed so that the content uniformity of the exiting blend stays in specification even if there are variations in the mass flows of the individual components from the feeding process. To enhance the process understanding of this mixing process and expedite the development time with minimal material consumption, we have been developing computational model of CMT device using Discrete Element Method. Our previous work focused on (DEM) simulations to gain a very detailed process insight using the commercial software package XPS (Extended Particle System) at low mass throughput of 10kg/h [1].Recently, DEM model has been extended to simulate production-scale processes with a hold-up mass of up to 1000g and mass throughput of 30kg/h with 5.7million particles [2]. These simulations yield particle-level residence times, making it possible to create RTDs as function of process parameters (e.g. hold-up mass, throughput, impeller speed) and material parameters (e.g. different material lots with different flowability). Extensive validation of these simulations is carried out by comparing against RTD data collected through tracer experiments over a wide operating space. This validation study show that simulation results are in good agreement with the experimental data establishing DEM model as a Digital Twin of the physical process.References:[1] Toson et al., Int. J. Pharm. 552, 1–2 (2018), 288–300. doi:10.1016 IJP pharm 2018.09.032.[2] Toson et al., AICHE meeting 2019.",37,0.0
"Continuous manufacturing processes have seen increasing uptake in the pharmaceutical industry with a number of notable demonstrations of their utility within the industry resulting in increased process intensity, simplification in scale-up, in both drug substance and drug product production. Applications of new CM platforms within industry to date have maintained the traditional break between drug substance and drug product mirroring batch operations where API crystallization and isolation almost uniformly demarks a transition from Drug Substance to Drug Product manufacturing, occurring in separate facilities. This study investigates new architectures that forgo traditional API isolation and instead utilize isolation free crystallization for purification to deliver a purified liquid stream to an integrated spray coater to directly produce highly engineered drug product intermediates.To this end a novel confined suspension agitated bed crystallization process that produces a purified API solution effluent has been applied to purify a model API with high purity and yield. The purified API solution from this process was then deposited onto microcrystalline cellulose (MCC) beads via fluidized bed spray coating. This process is shown schematically in Figure 1. Fluidized bed spray coating operations onto excipient carriers are capable creating final bulk powders with excellent flow properties in addition to incorporating, disintegrants, controlled release polymers and coatings enabling direct isolation of final highly engineered drug product intermediates ready for example capsule filing or direct compression in a DS facility.As spray coating is a complex process with a large number of interconnected process parameters, a DOE based optimization approach was used to develop a robust process with regard to loading efficiency and the degree of drug crystallinity on the coated beads. A five-factor two-level fractional factorial study was produced to screen for the parameters with the most significant influence on the spray coating process. The mass of API sprayed was kept constant for each treatment and factors investigated were process temperature (50 to 60 °C), solvent volume (50 to 100 ml), binder mass (1 to 3 g), binder type (PVP or HPMC) and solvent type (methanol or ethanol). From this study, spraying conditions were identified that maximised both the amount of API loaded onto the beads and the degree of crystallinity and were then used to spray purified crystallization product directly onto the MCC beads (Figure 2). The product from the spray coater allows for both direct formulation of the product and for further processing and coating in order to mediate drug release. As both processes are easily scalable and the spray coating transferable to an analogous continuous operation, this work demonstrates a promising method for isolation free purification and direct formulation of Drug Products.Acknowledgements:This work is supported by the Irish Research Council Government of Ireland Postgraduate Ireland Postgraduate Research Scholarship under grant number (GOPI/2017/1980)",37,1.0
"IntroductionHot melt extrusion (HME) is a continuous manufacturing process primarily carried out using co-rotating intermeshing twin-screw extruders (TSE). The process is mostly used in order to produce amorphous solid dispersions of poorly soluble active pharmaceutical ingredients (APIs) by dispersing them in polymer carriers. In addition, it can also facilitate the development and production of products with crystalline API embedded into a polymer matrix and nanopharmaceuticals. Formulation screening for HME based drug product presents a significant challenge in the product development cycle. On the one hand this can be attributed to the complex formulation, process and final product relationship where it is not entirely clear how the chosen formulation is transformed into the final dosage form under selected HME process conditions. On the other hand, being a continuous manufacturing technology, HME traditionally requires a large amount of API (kilograms of premix vs grams available in the early development stages) for successful formulation screening and early process setup. The process setup itself is modular, allowing the process to be tailor-made for any formulation. This might however be problematic in the context of choosing the appropriate and efficient setup for an unknown formulation and it is an additional reason for relatively high quantities of material usually needed for the screening. Effectively solving the multidisciplinary challenge of formulation development, early process screening, effective scale-up and transfer to GMP production represents one the key challenges of the pharmaceutical industry and is especially challenging for novel and imported production technologies like HME.MethodologyAs a response to these challenges, our group has focused on developing scientific tools that allow for a fast and minimum-risk development of HME-based formulations using several advanced tools. These include advanced material screening, small-scale formulation test beds, the design of small-scale processes and the scale-up to GMP production of clinical batches. During the formulation development and screening phase traditional approaches (e.g., assessment of biopharmaceutical and physicochemical properties of the formulation) or modern in-silico approaches (e.g., molecular dynamics, machine learning). Since to this date most of the process setup and scale-up activities are performed experimentally and empirically, one of the goals of our group was to create in silico tools for a rational, science-based process setup and scale-up, while addressing other important aspects, such as an API degradation and overall product quality[1], [2]. The fundamental idea behind process modelling is the break down and the detailed analysis of the key process aspects, among which the most prominent might be the analysis of flow patterns developed as a result of the rotation and geometry of the individual screw element pairs[3]–[5]. A further step is the development of numerical models aimed at understanding and predicting the connection between the independent process variables like the screw speed, throughput, barrel temperature, and screw configuration to the dependent process variables like the resulting melt temperature distribution, SMEC distribution and local and overall residence time distribution[1], [6]–[8]. Once an accurate prediction of the internal process state is possible, a push towards connecting the internal process state, the formulation characteristics and resulting product quality can be made.ResultsFollowing this approach formulation and process development will be discussed and analyzed. A detailed analysis of the different extruder elements and equipment scales used, based on the results of detailed smoothed particle hydrodynamics (SPH) simulations will be presented and discussed. Moreover, a detailed process analysis based on mechanistic 1D simulations will be performed, comparing various configurations and equipment scales, in the context of scalability and resulting process quality. The overall goal is to present a systematic and holistic framework for rapid, reliable and waste free product development of HME based formulations. Following the Quality by Design (QbD) framework, the pharmaceutical product development should be based on sound scientific principles and quality risk management, with an emphasis on predefined objectives, scientific product and process understanding and process control. Hence, our approach aims at connecting the different formulation properties, desired product quality attributes and the critical process parameters with the help of mechanistic process simulations and advanced formulation design, laying the foundation for automated process setup and scale-up with a prescribed product quality in mind.Literature[1] J. Matić, A. Witschnigg, M. Zagler, S. Eder, and J. G. Khinast, “A novel in silico scale-up approach for hot melt extrusion processes,” Chem. Eng. Sci., vol. 204, pp. 257–269, Aug. 2019.[2] J. Matić, A. Paudel, H. Bauer, R. A. L. Garcia, K. Biedrzycka, and J. G. Khinast, “Developing HME-Based Drug Products Using Emerging Science: A Fast-Track Roadmap from Concept to Clinical Batch,” AAPS PharmSciTech.[3] A. Eitzlmayr and J. G. Khinast, “Co-rotating twin-screw extruders: Detailed analysis of conveying elements based on smoothed particle hydrodynamics. Part 1: Hydrodynamics,” Chem. Eng. Sci., vol. 134, pp. 861–879, Sep. 2015.[4] A. Eitzlmayr and J. G. Khinast, “Co-rotating twin-screw extruders: Detailed analysis of conveying elements based on smoothed particle hydrodynamics. Part 1: Hydrodynamics,” Chem. Eng. Sci., vol. 134, pp. 861–879, Sep. 2015.[5] A. Eitzlmayr, J. Matić, and J. G. Khinast, “Analysis of flow and mixing in screw elements of corotating twin-screw extruders via SPH,” AIChE J., vol. 63, no. 6, pp. 2451–2463, Jun. 2017.[6] A. Eitzlmayr et al., “Experimental characterization and modeling of twin-screw extruder elements for pharmaceutical hot melt extrusion,” AIChE J., vol. 59, no. 11, pp. 4440–4450, Nov. 2013.[7] A. Eitzlmayr et al., “Mechanistic modeling of modular co-rotating twin-screw extruders,” Int. J. Pharm., vol. 474, no. 1–2, pp. 157–176, Oct. 2014.[8] R. Baumgartner, J. Matić, S. Schrank, S. Laske, J. G. Khinast, and E. Roblegg, “NANEX: Process design and optimization,” Int. J. Pharm., vol. 506, no. 1–2, pp. 35–45, Jun. 2016.",37,2.0
"Biopharmaceuticals, which are also widely known as biologics or biologic drugs, are products derived from biological organisms for treating or preventing diseases. The global sales of biopharmaceuticals have continually increased for many years, with hundreds of approved products on the market and over 7000 medicines in development. Monoclonal antibodies (mAbs) are the highest selling class of biopharmaceuticals due to their specific action and reduced immunogenicity. With continued growth in sales of existing mAb products and a growing pipeline of mAb product candidates being developed, the total sales of mAb products and all biopharmaceuticals will continue to increase in the coming years. The development of mAbs is expected to grow further as more diseases are understood at molecular and cellular levels [1].Most biomanufacturing unit operations use empirical (black box) or semi-empirical (grey box) models based on experimental data because biomanufacturing processes are challenging to model using first principles. However, data-based models are not suitable for process prediction and optimization because they perform poorly on extrapolation from their original data set and are not based on any process understanding [2]. First-principles models, on the other hand, are based on understanding of fundamental physical phenomena such as conservation laws, thermodynamics, chemical kinetics, and transport phenomena. These models are useful for process development because less experimental data is needed. Integrated first-principles models for the entire biomanufacturing plant are needed to consider that individual unit operations do not operate in isolation, and changes in a unit operation can affect a process further downstream [3,4].This presentation describes a software tool for carrying out high-fidelity dynamic simulations of integrated continuous mAb manufacturing plants. The simulations include first-principles models of individual biomanufacturing unit operations, including bioreactors, chromatography columns, crystallizers, viral inactivation units, and filtration units. Thoroughly validated models were not available for the quantitative prediction of some relationships. The lack of validated models was addressed by using the best models available in the literature and then validating them using experimental data collected for the individual unit operations from an automated integrated continuous manufacturing platform at MIT. These validated individual units are then combined into a plant-wide dynamic model that includes the effects of model parameter uncertainties and disturbances, which is then used to validate the integration of the individual models and to map the raw materials and operations to the critical quality attributes and other variables of interest anywhere in the system.Plant-wide first-principles predictive models can be used to design, compare, and evaluate various control and real-time release testing strategies [5]. This approach enables evaluation of the impact of choosing the underlying manufacturing model and control strategy on process performance. The plant-wide simulation software is designed to make the replacement of individual models and unit operations seamless. This plug-and-play ability enables the simulation-based evaluation of multiple process options before equipment is swapped in and out of the real physical manufacturing plant, including for less established technology such as protein capture and purification via crystallization [1,6].References:[1] M. S. Hong, K. A. Severson, M. Jiang, A. E. Lu, J. C. Love, and R. D. Braatz. Challenges and opportunities in biopharmaceutical manufacturing control. Computers and Chemical Engineering, 110:106–114, 2018.[2] D. B. Boedeker and D. J. Magnus. Opportunities and limitations of continuous processing and use of disposables. American Pharmaceutical Review, 20(1):66+, 2017.[3] Z. Xing, B. M. Kenty, Z. J. Li, and S. S. Lee. Scale-up analysis for a CHO cell culture process in large-scale bioreactor. Biotechnology and Bioengineering, 103(4):733–746, 2009.[4] R. P. Harrison, S. Ruck, N. Medcalf, and Q. A. Rafiq. Decentralized manufacturing of cell and gene therapies: Overcoming challenges and identifying opportunities. Cytotherapy, 19(10):1140–1151, 2017.[5] M. Jiang, K. A. Severson, J. C. Love, H. Madden, P. Swann, L. Zang, and R. D. Braatz. Opportunities and challenges of real-time release testing in biopharmaceutical manufacturing. Biotechnology and Bioengineering, 114(11):2445-2456, 2017.[6] B. Smejkal, N. J. Agrawal, B. Helk, H. Schulz, M. Giffard, M. Mechelke, F. Ortner, P. Heckmeier, B. L. Trout, and D. Hekmat. Fast and scalable purification of a therapeutic full‐length antibody based on process crystallization. Biotechnology and Bioengineering, 110(9):2452-2461, 2013.",37,3.0
"Biopharmaceuticals, such as monoclonal antibodies, represent an important field in the pharmaceutical industry. Development and production of these valuable and complex substances is costly and the resulting price on the market for a single dose can reach more than thousand euros. This highlights the demand of processing these biopharmaceuticals in a sensitive way to preserve their activity and stability. The majority of biopharmaceutics is formulated as liquid solutions. Proteins are more susceptible to physico-chemical degradation in liquid state, thus various stabilizers are needed to prevent self-aggregation and denaturation. The conversion of these biologics from the liquid to the solid state by removing their natural environment does not appear to be intentional at first glance. However, removing water decreases the proteins mobility and stabilizes the protein. Additionally, dried protein powders convince in eased handling, storage and transportation.Freeze drying is currently the gold standard, when protein formulations are transferred into dry solid powders. Briefly, freeze drying of a protein formulation is based on converting the liquid protein formulation to a solid state by freezing and eventually removing the water by applying vacuum (i.e. sublimation). However, freeze drying is time intensive during processing and handling. A promising alternative is spray drying of proteins. The protein formulation is atomized by a spray nozzle. The finely dispersed droplets are dried by a heated gas flow (e.g. air, nitrogen) and subsequently collected. Spray drying allows for a vast decrease in processing time, processing at atmospheric pressure and mild drying temperatures, the control of powder particle properties, and remaining water content.We developed a stable drying process for different model proteins by optimizing the yield and residual moisture. The structural stability, activity and properties in solid state of the protein were assessed by a standardized set of methods. The talk will highlight the lessons learned and provide further steps towards understanding the impact of spray drying. An outline to support industrialization of the process design and development for spray drying as alternative technology will be presented to fully exploit its benefits.",37,4.0
"Hydrogen peroxide decontamination is the key technology to sterilize isolators or clean rooms to produce injectables. Typically, injectables are manufactured batch-wise, where decontamination serves as the change-over operation between batches. Decontamination is known to be time-intensive, especially the aeration part to reduce the residual H2O2 to a target concentration (e.g., 1 ppm, or even lower). Decontamination is becoming relevant, along with the market growth of biopharmaceuticals/vaccines that are usually provided as injectables, and also with the trend of small lot-size production and frequent change-overs.We analyzed the impact of H2O2 sorption by polymers on the duration of aeration. Five polymers, which are typically used as materials/parts in sterile isolators, were investigated: chlorosulfonated polyethylene, silicone, polyethylene, polyoxymethylene, and polyvinyl chloride. For assessing different polymers, we defined the storage capacity and diffusion coefficients of H2O2 in the polymer as the indicators. Experiments were conducted to assess the abovementioned five polymers, and also to develop models to estimate the required duration of aeration, given the target H2O2 concentration. We extended the model calculation to map-out the duration, given the properties of the polymers. In the simulated setup, polyoxymethylene and silicone showed the tendency to require long duration for aeration. The result suggests that the application of these polymers in the isolator requires attention. Also, the target H2O2 concentration affected the superiority of the polymers largely, which is another practical finding from the analysis. This result suggests the benefit of integrating product information into the design of isolators, in order to achieve a rapid cycle of decontamination and aeration.",37,5.0
"Prior work has shown the differences in energy dissipation zones in different portions of the vessel, e.g. in the trailing vortex, far from the impeller, etc. The path that the fluid or particle takes between these zones is difficult to measure experimentally. Computational Fluid Dynamics is an attractive approach to directly predict the path of the fluid or particles in the system and measure the history of exposure to the differing energy dissipation regimes. In particular, the large eddy simulation model is particularly suited for describing the random transient behaviors inside a fully turbulent tank.Following previous work [1], we will present the time history of energy-dissipation in mixing tanks across multiple scales using the LES model. We will extend the prior work by extending the range of systems studied including changing pumping direction and impeller types.First, the time-averaged energy-dissipation predictions will be compared to literature reported values, and the accuracy and limitations of the LES modelling approach will be discussed. The energy dissipation history can be filtered through signal processing operations that leads to an understanding of power being applied to the fluid at different scales. We will show that there are fundamental relationships that collapse the time history information onto a universal curve. We will show the effect of changing the system beyond a geometrical scale up, including by varying impeller type.[1] Flamm, Raudenbush, Sirota, Cote. AICHE Annual Meeting 2019.",38,0.0
"Mixing is an integral operation used throughout the manufacturing process train of pooling, formulation, and primary container closure filling for large molecule sterile drug products. As such, evaluating appropriate mixing parameters and associated ranges is necessary to understand their impact on product uniformity and physical stability. For this work, a broad toolbox of at-scale, computational, and scale-down approaches were used to demonstrate successful mixing operating ranges. Saline and water were used as a model system to evaluate mixing at-scale (65 L) with agitation rates ranging from 37 to 100 RPM. In parallel, computational fluid dynamics (CFD) was used to predict at-scale homogeneity with further refinement and validation from the at-scale mixing results. It was observed that an agitation rate of 80 RPM yielded a uniform mixture within 30 minutes. At these mixing conditions, the computational model predicted maximum shear rates in the impeller region to be approximately 1,000 s-1. A scale-down approach was developed using a rheometer to subject active large molecules to uniform shear rates based on the predicted range. The stressed molecules were then analyzed for particle aggregates as an indication of physical stability at shear rates similar to those shown in the CFD models. This scale-down approach is particularly useful during early development to evaluate the shear sensitivity of large molecules under anticipated mixing conditions when material quantities are limited. Setting the operating parameters for the scale-down model required the predictions from the CFD models, which were optimized with data collected from at-scale mixing. When used together, the toolbox of at-scale, computational, and scale-down approaches implemented within this work are broadly applicable to evaluate and define successful mixing parameters for the manufacture of large molecule sterile drug products.",38,1.0
"This study discusses the complex mixing considerations arising throughout the time course of filling a shear-thinning suspension drug product formulation. As feed tank working volume is drawn down, rotational speed of the off-set, bottom mounted mixer must balance air entrainment, regions of stagnation, and apparent active particle buoyancy and creaming in the transitional regime without impacting potency control of the suspension drug product being filled. M-Star Simulation’s® Digital Mixing Tank (DMT) is employed to characterize the complex phenomena to inform mixing profile optimization.",38,2.0
"Reaction and crystallization processes often need to balance competing rates in order to achieve the desired results. Mixing can affect certain rate processes, so understanding the role of mixing is a critical part of process development and scaling. Process developers in the chemical and pharmaceutical industries need efficient ways to understand how mixing matters to their processes. Since it was introduced in 2003, the Bourne Protocol has been used to efficiently understand the effect of impeller speed, feed time, and feed location on semi-batch or gradual-addition processes. Bourne’s simple experimental plan creates a framework to understand if micromixing (mixing at the molecular or particle length scale) or mesomixing (mixing at feed plume length scale) or macromixing (mixing at the length scale of the vessel) matters in a process. Applications and enhancements to the Bourne Protocol have been discussed in a variety of venues over the past decade (see References). Most recently, we obtained more appropriately visualized regions of acceptable process performance by plotting Bourne Protocol results in a micromixing-mesomixing space (Sarafinas, 2019). That study looked at examples from the literature and showed how regions of different process failures could be characterized in the micromixing-mesomixing space, enabling improved and efficient understanding of the acceptable process operating window for scale-up or scale-down. There are many potential undesirable process responses that can be affected by mixing: low yield or conversion, poor selectivity, unacceptable purity, unmanageable particle size, poor crystal morphology, inappropriate polymorph, etc. Understanding how a process responds to different levels of micromixing and mesomixing can enable more successful scaling, suggest potential process improvements, and speed troubleshooting of industrial processes. This paper continues the exploration of the micromixing-mesomixing space to characterize our processes. Beginning with how to apply the Bourne Protocol to efficiently show how mixing matters in a process, we explore how process practitioners can expand the Protocol to better understand their process for scaling (up or down) in development and troubleshooting. This approach results in achieving more robust processes and more optimal equipment designs, more quickly. Further literature examples and some new examples will be shown. References:Bourne J.R., “Mixing and the Selectivity of Chemical Reactions”, Org. Process Res. Des. Dev.; 2003; 7(4) pp. 471-508.Sarafinas, A. “Efficiently Characterize Process Scalability in the Micromixing-Mesomixing Space Using the Bourne Protocol,” AIChE 2019 Annual Meeting, Orlando, FL, November 2019.Sarafinas, A. “Test Process Mixing Sensitivities Using the Bourne Protocol,” Scientific Update Webinar, 13 November 2018.Sarafinas, A. “Using the Bourne Protocol to test mixing sensitivities in the lab,” DynoChem Guest Webinar, 8 August 2018.Sarafinas, A. “A further look at the Bourne Protocol for efficient investigation of mixing sensitivities during process development,” AIChE 2017 Process Development Symposium, Toronto, Canada, June 2017.Sarafinas, A. and C.I. Teich in Kresta, S.M., A.W. Etchels III, D.S. Dickey, V.A. Atiemo-Obeng, Advances in Industrial Mixing: A Companion to the Handbook of Industrial Mixing, John Wiley and Sons, 2016.Teich, C.I., A. Sarafinas, P.M. Morton, AIChE 2010 Annual Meeting, Salt Lake City, UT, November 2010 (2 papers):“Can this process be saved? A search for understanding using the Bourne Protocol and advanced process development tools”“Taking the min to the max: a case study in small scale process development using on-line reaction calorimetry and in-situ particle characterization”",38,3.0
"Industrial mixing success is measured by practical and economical product production, not by extensive description of mixing behavior. Most often the cost and time required for a scientific research project to find the best mixing option is not feasible. The majority of industrial mixer applications are for formulation processes that combine and blend components, as opposed to complicated chemical reactions with multiple possible products. The ability to conveniently quantify mixing intensity can be an effective path to improving or solving many process problems.A further limitation to most industrial mixing processes is the requirement to use existing mixing equipment. A new mixer is rarely an option for process improvement or even new product production. The successful use of existing equipment often depends on the ability to predict the capabilities of the mixer for the planned product or modified process. A 1 to 10 Scale of Agitation for liquid mixing intensity was introduced in a series of articles published in Chemical Engineering in 1975 and 1976. That intensity scale was linked to a ""bulk fluid velocity"" and applied to the selection of mixing equipment. Because the concept focused on new mixing equipment, it was presented in a way that was difficult to apply to existing equipment. A convenient and consistent method for predicting mixing intensity for turbine style mixers in liquid applications has been developed to facilitate the evaluation of mixing equipment.The concept of a 1 to 10 scale of mixing intensity has been reduced to the straightforward calculation of a Mixing Index. This index is based on otherwise obvious variables that influence mixing success, including liquid volume, impeller diameter, rotational speed, and impeller power number. The Mixing Index (MI) can be calculated using the following formula for turbulent conditions: (equation - see uploaded file)The expression for MI can be rearranged to solve for a rational speed based on an impeller diameter and volume or solved for an impeller diameter at a speed and volume. The simple expression can be used for either evaluation of existing mixing conditions or mixer design based on the selection of other variables. The expression needs a correction factor for reduced impeller pumping at higher viscosity. The correction factor can be used over a typical range of Reynolds numbers for turbine mixers.The calculation of a MI with a value between 1 and 10 can be compared with CFD images or used for scale-up calculations. A non geometric scale-up example and associated CFD plots will be used to demonstrate the ability of the MI to calculate or compare mixer capabilities. The basic equation can be used to adjust for different volumes and with different impeller types. Scale-up can even keep multiple mixing variables constant by solving multiple equations with multiple unknowns.While not sophisticated or precise, the MI gives a sufficient estimate of mixing intensity for many industrial applications. It successfully parallels other methods of mixer evaluation and can be applied using spreadsheet calculations and basic engineering skills. Like any other method for solving mixing problems, the MI has its limitations, but it does provide a flexible method for getting estimates of mixing capacities quickly and cost effectively.",38,4.0
"Scale-up of hydrogenation reactions can uncover mixing sensitivities due to heterogeneous catalyst use or catalyst deactivation by impurity adsorption. This presentation will describe the control of carbon dioxide (CO2) formed as a by-product during a palladium-on-carbon catalyzed transfer hydrogenation reaction. At small scale, CO2 was adequately purged via nitrogen sparging to ensure sufficient mass transfer of reagents to the catalyst. This resulted in a first-order kinetic model with respect to the limiting reactant. However, during an initial 5x scale-up, zeroth-order reaction kinetics were obtained and were attributed to CO2 inhibition of the catalyst. The reaction had been scaled to maintain nitrogen mass transfer rate coefficient (kLa) through adjustment of agitation and reactor configuration. While this achieved the desired kLa, the headspace turnover time (i.e. headspace volume over nitrogen flow rate) was much longer on-scale. This resulted in increased CO2 concentrations in the headspace and, therefore, the batch. Utilizing Process Analytical Technology and data from scale-down experiments, a coupled kinetic and mass transfer model was developed to account for CO2 generation, vapor-liquid equilibrium, and headspace purge. The kLa and headspace turnover requirements predicted by the model were then successfully demonstrated at scale. This model allows accurate prediction of reaction times and should be considered for other reactions where catalyst inhibition is a risk.",38,5.0
"In this work we showcase a procedure to troubleshoot a reaction that had very good outcome in the lab (an over-reaction key impurity was maintained at ≤1.2% area, which allows to meet the dry solid specification of 0.15% in area) while in the first large scale batch (30 kg batch size), the level of this impurity reached 30% area at reaction end-point, which did not allow its purge during downstream, failing to meet the impurity specification in the dry solid. The outline course of action was to build a kinetic model for the formation of product and the key impurity and use historical data to follow as close as possible the Bourne protocol (Bourne 2003) to understand the impact of mixing on the reaction outcome.According to the reaction mechanism (A + B --> P, P + B --> Imp), the only way for very low levels of Imp impurity is for its formation reaction rate to be much lower than the product formation reaction rate, since they are competing-consecutive reactions. Even though this seems obvious, we wanted to confirm this hypothesis with a kinetic model.A set of experiments was carried out to better understand the formation of the impurity, namely adding to a solution of product P, a solution of 1 eq. of reagent B, at two temperatures; Additionally, four other experiments were carried out namely adding a solution of B (1eq, 3 vol) to a 3 vol solution of starting material A at the same two temperatures and two feeding durations (2h and 1h, respectively), and the reverse addition (adding a solution of A to a solution of B, with the same temperature and feeding durations described previously). Furthermore, we also added data from a DoE (which explored reaction temperature, agitation speed and feeding duration) to give more robustness to the kinetic model. Data obtained from an RC1 experiment was used as well to assess the reaction exothermicity and, thus, determine the ΔHr, which is a critical aspect for scaling-up this reaction.To assess the impact of the agitation on the reaction, we used some of the experiments from the previously described DoE, to apply the Bourne protocol (Bourne 2003), and thus help provide the needed insights to understand which of the micro-, meso- or macro-mixing mechanism would be the controlling mechanism, and thus have a science based approach to scale-up the mixing conditions of this reaction.It was proven that by resorting to a mechanistic modelling approach and a reduced number of experiments we were able to successfully investigate the root cause of the failed batch and scale-up the reaction to large scale conditions and avoid further failed batches in the future.ReferencesBourne, John R. 2003. “Mixing and the Selectivity of Chemical Reactions.” Organic Process Research and Development 7 (4): 471–508. https://doi.org/10.1021/op020074q.",38,6.0
"Proteins families that evolve to bind particular ligand also contain diverse members that exhibit a wide range of affinities for that particular ligand. However, this variance in affinity resulting from evolution of the family members is usually at odds with the high sequence and structure similarity shared by its members. In this talk, we discuss three such protein families i.e. human protein kinases, Cannabinoid receptors and Strigolactone receptors in plants. These proteins show several orders of magnitude difference in their ligand affinity but share high sequence and structural similarity. The characterization of the free energy landscapes for ligand binding starting could shed light on the molecular origin of their ligand selectivity mechanisms. In order to achieve this goal, we have develop computational methods for evaluation of their binding mechanism using molecular simulations. These examples served as model system for obtaining generalizable insights about the free energy landscapes of ligand binding that could help design of drugs with desired selectivity profiles. ",39,0.0
"Modern drug discovery is extending into chemical spaces with increased complexity. It is desirable to explore these spaces more efficiently. In order to maximize the information gained from testing new molecules, the concept of molecular library has been widely adopted in medicinal chemistry. Molecular libraries are collections of compounds that have, or expected to have, similar structures or properties. Accessing all molecules in a molecule library can enable the rapid generation of structure-activity relationships during the hit-to-lead or lead optimization phases, which can greatly enhance the efficiency of drug discovery. Small molecule libraries can be constructed in many different ways. A library may be a collection of compounds extracted from the literature with similar functionality;1,2 or it may be designed by enumerating possible side groups as decorations of a common core scaffold;3–5 it may also be designed using any one of the increasing number of in silico tools for the generation of drug-like compound libraries.6–8 Despite the significant progress in molecular library generation, there is a usually a tradeoff between accessibility and diversity – libraries that are constructed by simply substituting functional groups on the same scaffold are usually limited in diversity, and libraries with larger structural diversity tend not to be all accessible through similar reactions. In this work, we leverage informatics tools to maximize the diversity of molecular libraries as well as their synthetic accessibility. Starting from a given molecule, using retrosynthesis analysis and reaction prediction models, we can generate libraries that can be accessed through alternative late-stage functionalization. The diversity of the molecular libraries is assessed by average pairwise similarity, so that the most diverse molecular library can be identified. This work provides insight on expanding the exploration of surrounding chemical space of a molecule with minimal additional effort. 1 U. F. Röhrig, S. R. Majjigapu, P. Vogel, V. Zoete and O. Michielin, J. Med. Chem., 2015, 58, 9421–9437.2 B. T. Xin, E. M. Huber, G. De Bruin, W. Heinemeyer, E. Maurits, C. Espinal, Y. Du, M. Janssens, E. S. Weyburne, A. F. Kisselev, B. I. Florea, C. Driessen, G. A. Van Der Marel, M. Groll and H. S. Overkleeft, J. Med. Chem., 2019, 62, 1626–1642.3 K. Kannan Sivaraman, A. Paiardini, M. Sieńczyk, C. Ruggeri, C. A. Oellig, J. P. Dalton, P. J. Scammells, M. Drag and S. McGowan, J. Med. Chem., 2013, 56, 5213–5217.4 R. Fleeman, T. M. Lavoi, R. G. Santos, A. Morales, A. Nefzi, G. S. Welmaker, J. L. Medina-Franco, M. A. Giulianotti, R. A. Houghten and L. N. Shaw, J. Med. Chem., 2015, 58, 3340–3355.5 G. M. Keseru, D. A. Erlanson, G. G. Ferenczy, M. M. Hann, C. W. Murray and S. D. Pickett, J. Med. Chem., 2016, 59, 8189–8206.6 R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams and A. Aspuru-Guzik, ACS Cent. Sci., 2018, 4, 268–276.7 M. H. S. Segler, T. Kogej, C. Tyrchan and M. P. Waller, ACS Cent. Sci., 2018, 4, 120–131.8 A. Gupta, A. T. Müller, B. J. H. Huisman, J. A. Fuchs, P. Schneider and G. Schneider, Mol. Inform., 2018, 37, 1700111.",39,1.0
"Cytokines are soluble factors that signal through stimulation of their cognate transmembrane receptors on target cells to perform critical biological functions, particularly those related to immune homeostasis. As a result of their essential immune activities, cytokines have great potential as immunotherapeutics, both to activate the immune response to fight diseases such as cancer and infectious diseases, as well as to suppress the immune response to treat autoimmune disorders or for transplantation medicine. However, the pleiotropic activities and unfavorable pharmaceutical properties of natural cytokines have limited their clinical performance. Recent efforts to re-engineer native cytokines to improve their selectivity and stability have significantly advanced progress in cytokine therapeutic development. However, there are inherent challenges to developing therapeutics from natural proteins. First, most natural cytokines are only marginally stable, hence mutations aimed at increasing efficacy can decrease expression or cause aggregation, making manufacturing and storage difficult. More substantial changes, such as the deletion or fusion of functional or targeting domains, can also dramatically alter pharmacokinetic properties and tissue penetration. In addition, immune responses targeting a variant of a natural protein may cross-react with the endogenous molecule with potentially catastrophic consequences. Finally, the high potency and multifarious functions of cytokines hinder their efficacy and can result in harmful off-target effects and systemic toxicities. Here, we combine a best-in-class computational protein design software with directed evolution technologies to generate de novo cytokine mimetics that circumvent limitations for naturally-derived cytokine drugs. Rather than modifying existing molecules, we adopt a bold new approach to create exceptionally stable proteins with customized receptor interaction properties as robust and efficacious targeted therapeutics. We applied this strategy to develop a mimetic of the interleukin-2 (IL-2) cytokine, which plays a pivotal role in T and natural killer (NK) cell function. We created a hyper-stable de novo protein that is biased toward activation of IL-2 signaling pathways on immune effector cells, and showed that this molecule inhibits tumor growth and prolongs survival in mouse models of colorectal cancer and melanoma, while also mitigating the toxic side effects typically associated with IL-2 therapy. We built upon this exciting advance to design de novo cytokines that mimic IL-4, which promotes pro-regenerative immune programs following tissue damage. We demonstrated the extreme thermal stability and biased receptor activity of our engineered IL-4 mimetic, and showed that it elicited pro-regenerative immune responses in both cellular and animal models. Collectively, our work pioneers a novel cytokine engineering platform that integrates computational and experimental approaches to establish a general paradigm for advancing the clinical translation of cytokine therapeutics. ",39,2.0
"Currently, there are thousands of chemicals in common use; however, only a small number of them have sufficient toxicity evaluation, and even less information for the specific embryo-related toxicity field: embryotoxicity and developmental toxicity. Embryonic stem cell test (EST) is the only accepted in vitromethod for assessing embryotoxicity without animal sacrifice. However, EST for regulatory embryotoxicity screening are impeded by its complexity, time-consuming, limited differentiation lineage endpoints, and poor availability for use to develop a reliable prediction model (PM) for embryotoxicity assessments.A key step in the understanding of embryo development is to construct a systematic and comprehensive model of network with the development-related pathways involved. In our research, we mainly focus on the essential markers in the pathways related to ESCs apoptosis and proliferation, pluripotency, and specific-lineage differentiation. Quantitative structure-activity relationship (QSAR) models which predict the biological activities from chemical structures information is an efficient and less time-consuming strategy compared to experimental-based studies. Therefore, an integrated prediction pipeline for in vitrohigh throughput screening (HTS) based on engineered murine embryonic stem cells (mESCs) expressing a reporter, enhanced green fluorescent protein (EGFP), driven by human promoters for key genes in the pathways associated with apoptosis, pluripotency, and tissue-specific differentiation cultured in novel multi-well plates is developed and used to obtain new embryotoxicity and developmental toxicity data needed for the training of machine learning models such as support vector machines (SVM) and deep neural network (DNN). The application of advanced machine learning algorithms can significantly improve the accuracy and efficiency of the PM for embryotoxicity assessment of compounds with unknown developmental toxicity. This study will provide the technology needed for fast and accurate embryotoxicity assessments. The data obtained from our integrated analysis can provide the information needed for the regulation of industrial and emerging chemicals.",39,3.0
"Antimicrobial resistance (AMR) is increasingly a threat to global public health [1]. AMR-related infections limit the efficacy of lifesaving antibiotic therapy necessary for the treatment of infectious disease and limit the success of advanced surgical procedures like organ transplants [2, 3]. To combat AMR pathogens, either new antibiotics or the cocktails of existing antibiotics with inhibitors of antimicrobial resistance proteins should be explored. Since the development of new antibiotic has been slowed down significantly recently, identifying compounds/antimicrobials with synergistic effects becomes the major approach to addressing the AMR crisis. In this work, we demonstrate a computational approach to identifying plant compounds that have synergistic effect with Fosfomycin to inhibit Listeria monocytogenes, a food-borne pathogen that could cause listeriosis disease, especially on immune-compromised people [4, 5]. Experimental results further validate the our finding.L. monocytogenes was found to infect and adversely affect patients’ liver and spleen. In addition, L. monocytogenes can penetrate blood-brain barrier and blood-placenta barriers to harm the central neural system of pregnant woman and infant [4]. Since L. monocytogenes is an intracellular pathogen, antimicrobials used to treat listeriosis should be able to be transported into host cells. Penicillin, ampicillin and amoxicillin were commonly used antibiotics in the treatment of listeriosis [6]. However, antibiotic resistance genes have been continuously found in Listeria strains. For example, the strain that has resistance to Penicillin G was isolated from vegetables in 2016 [4, 7]. As a natural product, Fosfomycin was found effective against clinical isolates of L. monocytogenes and used as a novel therapeutic antibiotic for listeriosis clinical treatment [8]. In addition, Fosfomycin is able to penetrate the blood-brain barrier and reach clinically relevant concentrations. Thus, it has the potential to eliminate L. monocytogenes which would cause neuron damage[9]. However, stronger Fosfomycin resistance was found in the L. monocytogenes isolates with Fosfomycin resistance proteins detected [10]. In particular, a resistant gene FosX (LMO1702,402bp) was identified and expressed in L. monocytogenes EGDe (strain ATCC BAA-679), a typical well-studied strain [11]. The FosX enzyme catalyzed the hydrolysis of Fosfomycin and resulted in the Fosfomycin resistance in L. monocytogenes EGDe [5, 10]. Therefore, there is an urgent need to identify compounds that can inhibit FosX enzyme to revive the efficacy of Fosfomycin to treat L. monocytogenes. Automated molecular docking is the most commonly used computational approach that evaluates the binding of small-molecule ligands like compounds to a target receptor with a known crystal 3D structure [12]. Molecular docking provides an avenue for a high-throughput virtual screening of ligands, and it has been widely implemented in drug discovery research for hit identification[13]. Docking programs have been improved recently to provide more accurate prediction on ligand-target binding by optimizing docking algorithms and scoring functions [14]. Among those existing docking programs, Molsoft ICM was evaluated with 93% accuracy in flexible docking and 90% successful rate in covalent docking. This was significantly better than the performance of Autodock, DOCK, FlexX, Gold, FITTED and MOE [13-15]. Since structures and activities of the FosX protein in L. monocytogenes have been well studied, we used an integrated ICM-docking and experimental approach to identify FosX inhibitors that are of synergistic effect with Fosfomycin in treating resistant L. monocytogenes. Specifically, automated ligand docking was implemented to perform virtual screening of Indofine natural-product database and FDA-approved drugs to identified potential inhibitors. In vitro bacterial growth inhibition test was then utilized to verify the effectiveness of identified compounds combined with Fosfomycin on inhibiting the resistant L. monocytogenes. We demonstrated that two phenolic acids, i.e., caffeic acid and chlorogenic acid, were predicted as high-affinity FosX inhibitors from the ligand-docking platform. Experiments with these compounds indicated that: the cocktail of either caffeic acid (1.5mg/mL) or chlorogenic acid (3mg/mL) with Fosfomycin (50mg/L) was able to significantly inhibit the growth of the pathogen [16]. The finding of this work implies that the combination of Fosfomycin with either caffeic acid or chlorogenic acid is of potential to be used in the clinical treatment of Listeria infections. ReferencesVelez, R. and E. Sloand, Combating antibiotic resistance, mitigating future threats and ongoing initiatives. J Clin Nurs, 2016. 25(13-14): p. 1886-9.Michael, G.B., et al., Emerging issues in antimicrobial resistance of bacteria from food-producing animals. Future Microbiol, 2015. 10(3): p. 427-43.Davies, J. and D. Davies, Origins and evolution of antibiotic resistance. Microbiol Mol Biol Rev, 2010. 74(3): p. 417-33.Radoshevich, L. and P. Cossart, Listeria monocytogenes: towards a complete picture of its physiology and pathogenesis. Nat Rev Microbiol, 2018. 16(1): p. 32-46.Scortti, M., et al., Epistatic control of intrinsic resistance by virulence genes in Listeria. PLoS Genet, 2018. 14(9): p. e1007525.Temple, M.E. and M.C. Nahata, Treatment of Listeriosis. Annals of Pharmacotherapy, 2000. 34(5): p. 656-661.de Vasconcelos Byrne, V., et al., Occurrence and antimicrobial resistance patterns of Listeria monocytogenes isolated from vegetables. Brazilian Journal of Microbiology, 2016. 47(2): p. 438-443.Lepe, J.A., et al., In vitro and intracellular activities of fosfomycin against clinical strains of Listeria monocytogenes. International Journal of Antimicrobial Agents, 2014. 43(2): p. 135-139.Falagas, M.E., et al., Fosfomycin. Clin Microbiol Rev, 2016. 29(2): p. 321-47.Fillgrove, K.L., et al., Mechanistic diversity of fosfomycin resistance in pathogenic microorganisms. J Am Chem Soc, 2003. 125(51): p. 15730-1.Fillgrove, K.L., et al., Structure and mechanism of the genomically encoded fosfomycin resistance protein, FosX, from Listeria monocytogenes. Biochemistry, 2007. 46(27): p. 8110-20.Meng, X.Y., et al., Molecular docking: a powerful approach for structure-based drug discovery. Curr Comput Aided Drug Des, 2011. 7(2): p. 146-57.Kitchen, D.B., et al., Docking and scoring in virtual screening for drug discovery: methods and applications. Nat Rev Drug Discov, 2004. 3(11): p. 935-49.Lionta, E., et al., Structure-Based Virtual Screening for Drug Discovery: Principles, Applications and Recent Advances. Current Topics in Medicinal Chemistry, 2014. 14(16): p. 1923-1938.Scarpino, A., G.G. Ferenczy, and G.M. Keserű, Comparative Evaluation of Covalent Docking Tools. Journal of Chemical Information and Modeling, 2018. 58(7): p. 1441-1458.Zhang, F., et al., Synergistic Effect of Chlorogenic Acid and Caffeic Acid with Fosfomycin in Growth Inhibition of a Resistant Listeria monocytogenes Strain. ACS Omega, 2020. https://dx.doi.org/10.1021/acsomega.0c00352.",39,4.0
"There now exists a consensus that RNA based gene inference techniques such as Anti-Sense Oligonucleotides and siRNA represent a significant therapeutic modality in the pharmaceutical industry. While the applications and understanding of the underlying biology of these systems for use as therapeutic platforms have advanced rapidly, little change in the commercial synthesis of these compounds has occurred. Metric tonnes per annum of oligonucleotides are projected to be required due to the high number of oligonucleotide-containing drugs in late phases of clinical trials and also projected for individual ASO therapies as larger patient populations are targeted in clinical trials following a number of clinical successes in small orphan patient populations.[1][2] The existing state-of-the-art synthesis of oligonucleotides is carried out with the use of a solid phase support and batch sizes currently limited to approximately 5 kg due to the constraints of solid phase synthesis.[3] In contrast, liquid phase supports reduce many of the limitations that are associated with solid phase supports.[4] Liquid phase synthesis of oligonucleotides was established by using linear and branched polyethylene glycol (PEG) as the soluble support[5] Membrane enhanced synthesis of oligonucleotides and peptides using PEG as a soluble support have been shown to be a potential solution for the problems associated with solid phase synthesis.[6][7]The current work demonstrates proof of concept for a novel liquid phase manufacturing process utilizing cyclical flow synthesis combined with a novel membrane separation (Figure 1). This should enable fully scalable cyclical flow synthesis of oligonucleotides with increased productivity within a compact and scalable reactor footprint. This novel process intends to meet the growing demands of the industry and promises reduced capital costs and increased product quality control.Modifications to soluble supports were utilized to both optimize the nucleotide loading process but also to optimize membrane flux and rejection factor, and hence yield and purity with commercially available membranes. Both liquid batch and phase cyclical flow synthesis of short oligonucleotides were optimized for coupling yield, phosphoramidite consumption, solvent consumption and overall productivity. The ability to use optimum reaction/separation timescales which are inherent to coupling reactions in oligonucleotide synthesis enabled improved system performance and in turn the ability to rapidly supply oligonucleotides on demand.Acknowledgements:This publication is supported by Science Foundation Ireland (SFI) through SSPC, The SFI Research Centre for Pharmaceuticals (Grant number 12/RC/2275_P2).[1] Y. S. Sanghvi, Curr. Protoc. Nucleic Acid Chem. 2011, 1–22.[2] A. G. Molina, Y. S. Sanghvi, Curr. Protoc. Nucleic Acid Chem. 2019, 77, 1–17.[3] A. R. Lajmi, L. Schwartz, Y. S. Sanghvi, Org. Process Res. Dev. 2004, 8, 651–657.[4] P. R. J. Gaffney, J. F. Kim, I. B. Valtcheva, G. D. Williams, M. S. Anson, A. M. Buswell, A. G. Livingston, Chem. - A Eur. J. 2015, 21, 9535–9543.[5] B. Burcovich, F. M. Veronese, V. Zarytova, G. M. Bonora, in Nucleosides and Nucleotides, Marcel Dekker Inc., 1998, pp. 1567–1570.[6] M. Mutter, H. Hagenmaier, E. Bayer, Angew. Chemie 1971, 83, 883–884.[7] F. Brandstetter, H. Schott, E. Bayer, Tetrahedron Lett. 1973, 14, 2997–3000. ",39,5.0
"Within the pharmaceutical industry around 60% of recently developed drugs have solubility challenges while 80% of in-production line drugs are classified as being poorly soluble. Solubility is a critical property for drug performance (e.g. bioavailability) [1,2]. Cocrystal formation is one possible solution to overcome solubility problems. Pharmaceutical cocrystals, formed by combining an active pharmaceutical ingredient (API) with a cocrystal former (coformer), are often manufactured in a variety of methods that rely on the utilization of solvents. Pharmaceutical companies use between 25-100 kg solvent per kg of product, causing major water and air pollution. Hence, state-of-the-art solvent-based methods need to be replaced with more sustainable alternatives. Additionally, the formation of a cocrystal requires its components to be soluble in the same solvent. These limitations hinder the processability and development of novel pharmaceutical products.Mechanochemistry utilizes mechanical forces to induce reactions. The application of mechanochemistry to form cocrystals without solvents is a novel, promising and sustainable method [1,3]. Scientists have exploited the features of mechanochemistry, but only in a random, non-systematic manner for the production of new cocrystals. Indeed, with the enormous number of potential API-coformer combinations, it is currently impossible to predict if two crystalline materials will cocrystallize under mechanochemical forces.In this work, a high throughput screening approach combined with data analysis (machine learning) has been developed to predict if two crystals will cocrystallize under mechanochemical conditions. Strategically distinct pairs of APIs and crystalline coformers were screened by neat grinding (solventless) in a Retsch MM 400 vibration-mill. Powder X-ray diffraction has been applied to characterize all the samples and to determine the formation of new crystal structures. With the help of these results and molecular descriptors a machine learning algorithm (random forest model) has been successfully developed, effectively predicting the propensity of two molecules to cocrystallize under mechanochemical conditions. Additional cocrystallization events are predicted and experimentally validated. The technical feasibility of further API-coformer combinations can be estimated to create novel cocrystals without any solvent limitations, expanding the scope of mechanochemistry as a technique for drug discovery.[1] C. B. Aakeröy and A. S. Sinha. Co-crystals. Monographs in Supramolecular Chemistry. The Royal Society of Chemistry, 2018.[2] M. A. E. Yousef and V. R. Vangala. Pharmaceutical cocrystals: Molecules, crystals,formulations, medicines. Crystal Growth & Design, 19(12):7420–7438, 2019.[3] S. Lou, Y. Mao, D. Xu, J. He, Q. Chen, and Z. Xu . Fast and Selective Dehydrogenative C–H/C–H Arylation Using Mechanochemistry. ACS Catalysis, 6 (6) , 3890-3894, 2016.",39,6.0
"BackgroundParticle engineering of active pharmaceutical ingredients (API) is an increasingly studied area of research & development. During drug substance manufacturing, particle formation typically originates from crystallisation which is a critical multiphase unit operation for the purification and separation of APIs. It is imperative that product properties such as particle size, shape and polymorphic form are well defined and well controlled as they affect a drug’s bioavailability, shelf-life and to effectively formulate the API into a finished drug product. [1]However, achieving control over these properties is not necessarily met at the point of API crystallisation and isolation. Process intensified strategies through incorporation of ultrasound devices has shown wide interest in the last decade for manipulating particle properties during or immediately after the crystallisation step. For instance, the integration of ultrasound with crystallization allows for multipurpose functionality such as particle size & shape modification, [2] de-agglomeration, [3] scale-up, [4] accelerating the rate of crystallization kinetics and presents an overall more flexible approach to particle engineering. [5]Ultrasound technology is commonly applied across several industries through insertion of a probe horn in direct contact with a liquid medium. For pharmaceutical crystallisation, this can be advantageous in process development especially when small seed particles <15 µm are preferred for controlling the crystal growth and thus, final product properties. [6] Whilst still a popular method, direct contact of the probe horn with particles has well reported limitations such as surface erosion, [7] substantial temperature deviations, [8] excessive noise in the vicinity of health and safety, rapid dissipation of energy transferred per unit volume and continuous operability. Therefore, it is worthwhile exploring alternative particle engineering strategies which can avoid the aforementioned limitations whilst ensuring the manufacturability and quality of the drug substance.Case-StudyIn this work, indirect ultrasound is explored as a viable and robust means for particle engineering of APIs. A semi-continuous platform which applies indirect ultrasound for inducing nucleation, particle size reduction and shape modification is experimentally investigated via process intensification. The process consists of an inline flow-cell which generates intense ultrasonic vibrations via electrical stimulation (Figure 1). These mechanical vibrations are then transferred to a glass tube which in turn sonicate the incoming process stream without direct contact.Two APIs are selected for this study both of which have been found hard to nucleate with standard operating conditions under current industrial manufacturing processes. To establish key relationships between process parameters and product attributes, a series of isothermal desupersaturation experiments were conducted which monitored the impact of power amplitude (0, 20%, 40% and 80%) on inline count profiles, chord length distribution (CLD), particle size distributions (PSD) and solution concentrations. Nucleation rates affected from ultrasound impact were then extracted through population balance modelling as a proof-of-concept demonstration. To our knowledge, this is the first study to test and implement this indirect ultrasound flow-cell strategy on APIs for nucleation acceleration and control of particle attributes. The overall platform integration and ease of operation were additionally assessed for potential application in continuous manufacturing of pharmaceuticals.References: Variankaval, N., A.S. Cote, and M.F.J.A.J. Doherty, From form to function: Crystallization of active pharmaceutical ingredients. 2008. 54(7): p. 1682-1688. Zeiger, B.W. and K.S.J.J.o.t.A.C.S. Suslick, Sonofragmentation of molecular crystals. 2011. 133(37): p. 14530-14533. Cote, A. and E.J.A.P.R. Sirota, CRYSTALLIZATION: the pursuit of a robust approach for growing crystals directly to target size. 2010. 13(7): p. 46. Kim, S., et al., Crystallization process development of an active pharmaceutical ingredient and particle engineering via the use of ultrasonics and temperature cycling. 2003. 7(6): p. 997-1001. Ruecroft, G., et al., Sonocrystallization: the use of ultrasound for improved industrial crystallization. 2005. 9(6): p. 923-932. Jiang, M., et al., Indirect ultrasonication in continuous slug-flow crystallization. 2015. 15(5): p. 2486-2492. Price, C.J., Application of Ultrasound in Crystallization (Sonocrystallization), in Engineering Crystallography: From Molecule to Crystal to Functional Form. 2017, Springer. p. 301-313. Zhang, Z., et al., Enhancement of crystallization processes by power ultrasound: current state‐of‐the‐art and research advances. 2015. 14(4): p. 303-316.",40,0.0
"The control of the crystal properties has been in the focus of pharmaceutical industries and it represents a challenging aspect of the active pharmaceutical ingredient (API) production processes. To ensure that the desired critical quality attributes (CQAs) of the product are met, such as size, shape or polymorphic form, as well as to ease the downstream operations such as filtration or tableting, a careful design of the crystallization processes is critical. In this work we demonstrate the systematic design of a crystallization system for a commercial API, (compound A) from Takeda Pharmaceuticals International. The challenges related to the crystallization of the API is that the process is nucleation dominated and crystals are difficult to grow to the desired particle size (150-250 µm) by a simple cooling process. Moreover, the compound A tends to form high aspect ratio (AR) crystals, which can yield manufacturability problems. The aim of the crystallization design is to produce low AR and sufficiently large crystals with narrow distribution. Two methods were applied to reach these goals: (1) implementing temperature cycles to internally remove the fines, and (2) application of immersion milling to further control the shape and size of crystals.1,2,3 These approaches were suggested in the literature based on the results of comprehensive model-based optimization studies.4The work shows the implementation of a Quality-by-Control (QbC) guided wet-mill integrated crystallization design to produce large crystals with low AR. In this work, nine batch cooling crystallization experiments were performed. The first experiment was a seeded crystallization with very slow linear cooling rate. The remaining eight experiments were divided into two sets: (i) temperature cycling using simultaneous internal wet-mill; and (ii) crystallizations using temperature cycling but without milling. All experiments were performed with similar batch times but increasing the temperature cycle number from zero to four. Then, by using the same temperature profiles and operating procedure, simultaneous milling was applied for 75 % of the duration of the first temperature cycle as suggested in the literature, by using an immersion mill.3A batch seeded cooling crystallization with very slow cooling (-0.02 °C/min) was performed, which showed that even this very slow cooling was insufficient to obtain the desired large crystals (> 150 µm) because of strong secondary nucleation in the process. Instead, applying internal fines removal by temperature cycles led to considerable increase of the product crystal size. Increasing the number of temperature cycles had a significant effect on producing large crystals (with mean size of 150-250 µm) by eliminating the fines produced by nucleation. These conclusions were also evidenced by using FBRM and in-line particle images. The results suggested that at least 2 temperature cycles were required to reach the desired product size for compound A. In the second part of the study, experiments with the same temperature profiles with wet-milling applied during the first cycle were performed. In these experiments, large crystals with low AR (>150 µm with mean AR around 2.5) could be successfully produced. The reason for using the wet-mill only at the beginning of the process is that it helps create large amount of small, low AR particles, which are partly removed by the first heat-up stage. In addition, it was shown with the 4 cycle-experiment that the milling rate does not have a significant effect on the CSD within the range applied (4000 and 8000 RPM) due to the achievable smallest size limit that characterizes the milling equipment. Therefore, the milling intensity was set to 4,000 RPM for all experiments. The experiments demonstrated that wet-mill contributed significantly to reaching the desired product crystal size and shape. This was quantified by 2D size distribution measurements based on optical microscopy images. Additionally, unseeded experiments were also performed, in which in situ seed generation was achieved by (1) cooling or (2) application of the immersion-mill as a high share nucleator. Both of these experiments demonstrated that applying wet-milling during the first temperature cycle not only significantly improves the CSD and AR of the product, but also considerably decreases the uncertainty caused by variation in the seed quality, and in combination with internal seeding via cooling or using the immersion mill as a nucleator can increase the overall efficiency and robustness of the process.  References:[1] D. Ramkrishna and M. R. Singh, “Population Balance Modeling: Current Status and Future Prospects,” Annu. Rev. Chem. Biomol. Eng., vol. 5, no. 1, pp. 123–146, 2014.[2] D. Acevedo, V. K. Kamaraju, B. Glennon, and Z. K. Nagy, “Modeling and Characterization of an in Situ Wet Mill Operation,” Org. Process Res. Dev., vol. 21, no. 7, pp. 1069–1079, 2017.[3] Salvatori, F., Binel, P., & Mazzotti, M. (2019). Efficient assessment of combined crystallization, milling, and dissolution cycles for crystal size and shape manipulation. Chemical Engineering Science: X, 1, 100004.[4] B. Szilagyi and Z. K. Nagy, “Model-based analysis and quality-by-design framework for high aspect ratio crystals in crystallizer-wet mill systems using GPU acceleration enabled optimization,” Comput. Chem. Eng., vol. 126, pp. 421–433, 2019.",40,1.0
"The purification of high molecular weight proteins and peptides, for their use in biopharmaceutical drug therapeutics, can still account for a significant proportion of the drug development cost, which averages at between $1-6 billion. Crystallisation of these materials has mainly been used only in the field of structural determination, using small scale methods such as hanging drop vapour diffusion. However, advances in the optimisation of crystallisation conditions has seen significant increase in the scalability of protein crystallisation, including the development of continuous crystallisation approaches. This has seen crystallisation emerge as a more economic and greener alternative to the existing chromatographic methods typically utilised in peptide and protein purification.The Made Smarter review of 2017 identified that manufacturing industries should embrace transformative digital technologies to streamline their unit processes. Mechanistic process modelling, based on population balance equations to represent the underpinning physical phenomena, has proved a computationally efficient method of predicting optimal crystallisation conditions of the purification of small molecule active pharmaceutical ingredients. However, this has yet to be expanded into the crystallisation of biopharmaceutical proteins.Here, we utilise a combined process modelling and experimental approach to scale up the crystallisation of lysozyme and insulin from the µl to the tens of ml scale. We successfully optimise conditions, such as stirring rate and operation, pH, concentration and templating additives to not only scale up, but also increase the achieved crystallisation yield.Insulin, being more susceptible to denaturing, utilised shaking to induce mixing of the protein rich buffer and salt precipitant. Small scale hanging drop screening experiments found that the addition of amino acids could reproducibly encourage the crystallisation of insulin, offering the promise of control over the nucleation. Scale up experiments to 15ml allowed the investigation of the impact of shaker mixing from 50-150 rpm. The insulin case study also highlights how to circumvent common issues with protein crystallisation, such as working with extremely low solubility materials and identifying the nucleation and growth points during the crystallisation process.The lysozyme protein is more resistant to denature than insulin, whereby it can be subjected to mechanical stirring. Here we show successful batch and continuous crystallisations, utilising both shaking and mechanical stirring, up to the 30ml scale. Induction times were measured and classical nucleation kinetics parameters were derived from this data, with the pre-exponential factor and the surface energies being consistent with previous studies. Further optimisation of stirring methods and initial protein concentration was investigated to optimise the size and shape of the produced lysozyme crystals, whereby their purity was compared to the as purchased lipolyzed powder.Finally, process modelling techniques were applied to predict the crystallisation of lysozyme and insulin in the batch reactor conditions. Such models are well parameterised to describe the crystallisation of small molecule active pharmaceutical materials. However, the classical nucleation and growth equations, as well as the models to describe the liquid state, have been less commonly applied to biological materials. Kinetic parameters to describe the processes that were extracted from the literature were used as a starting point for the model, where careful optimisation of these parameters were used to identify problems and where discrepancies between modelled and experimental particle size distributions and concentration vs time data originated. The kinetic parameters that were optimised against reliable experimental data were confronted with further experimental data to examine the wider applicability of protein crystallisation process model.It was found that the classical Mullin nucleation and power law growth kinetic equations could be successfully regressed against experimental concentration vs time data, even for long nucleation times (Figure 1).Figure 1: Concentration vs time for a 30ml batch crystallisation of lysozyme, using a sodium acetate buffer and sodium chloride precipitant. Blue dots are experimental data with approximate 10% error from nanodrop UV vis measurements and the red line in the regressed fit using Mullin classical nucleation kinetics and power law growth kineticsFigure 1 shows that a good fit of the process model to the experimental concentrations vs time data was achieved. It is interesting to observe that the classical two step growth kinetics failed to find a fit, where we speculate that the diffusion term associated with the large lysozyme molecule limits the growth of this material in the model. Further investigation of the exact parameters in the equations will be presented.This study demonstrates a novel experimental and process modelling workflow for optimising the crystallisation of large biological molecules, paving a path for this to be a viable unit process in biopharmaceutical drug development. This study gives guidance and insights into the challenges of protein crystallisation and modelling of this process, along with what are the next steps to make both processes more robust for further scale-up.",40,2.0
"Anti-solvent and/or cooling crystallizations are commonly employed in purification of pharmaceutically relevant molecules. In addition, the crystallizations are also typically tasked with isolating solids with controlled physical properties. During development of multi-solvent crystallizations, it is not unusual to observe Liquid-Liquid Phase Separation (LLPS), commonly referred to as “oiling out”. LLPS is generally undesirable due to its detrimental impact on purity and physical properties as well as due to the challenges it brings in during scale-up. LLPS is impacted by multiple parameters such as solute concentration, solvent composition, and temperature. Understanding of such a dynamic phase diagram, in addition to the kinetic parameters, is paramount to achieve a predictable process scale-up.In this presentation, we detail a methodology to develop a hybrid anti-solvent / cooling crystallization process maneuvering around the LLPS boundaries. Use of automated Lab reactors (ALR) equipped with Process Analytical Technologies (PAT) is highlighted for performing data-rich experiments quickly. The generated data is further modelled with thermodynamic equations for enhanced process understanding. Finally, we will present a case study to showcase how the above-referenced methodology was utilized for enhanced process understanding and for development of a robust and scalable crystallization process.All authors are employees of AbbVie and may own AbbVie stock. AbbVie sponsored and funded the study; contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final publication.",40,3.0
"Introduction Digital design of manufacturing processes using mechanistic models is fast becoming an essential tool during Active Pharmaceutical Ingredient (API) early process development activities. It enables rapid and effective exploration of the decision space for Critical Process Parameters (CPP), helping to reduce risk and product time-to-market, and aiding in the effective and safe production of high quality pharmaceutical products. In this work, we demonstrate how mechanistic modelling can be utilised to support the early stage design of seeding strategy for a batch cooling crystallization process. We outline a step-wise workflow consisting of model validation from which primary nucleation and growth kinetics are estimated, as a minimum list of crystallization mechanisms. The data used during the bespoken parameter estimation was experimentally measured in an unseeded small-scale crystallization process.As part of this work, the model predictions for the seeded crystallization process was subsequently verified experimentally. This model-based approach along with rich experimental data yielded a more efficient workflow for seeding design, with reduced experimental effort compared to a more traditional purely empirical approach. A range of process data was utilized for the purposes of qualitative mechanism discrimination and quantitative model validation as follows:Qualitative mechanism & model discriminationFBRM data to inform active crystallization mechanisms to considerSEM and/or optical images of product crystals to inform crystallization mechanisms to consider within the mechanistic modelQuantitative model validationFBRM data to provide indication of onset of nucleation and access to the rate of nucleation in the unseeded crystallization runs.Solute concentration data over time both online (IR/Raman) &/or offline (HPLC)Particle size measurements of the product crystals from laser diffractionThe validated mechanistic model was subsequently used for model-based design of seeding policy, to assist in the seeding design (seed mass, seed addition point, and/or seed particle size distribution) required to achieve the target PSD specifications (defined by the volume-based quantiles d10, d50 and d90 of the product PSD). Uncertainty and sensitivity analysis were performed to understand the attainable region for the product PSD in terms of quantiles for the crystallization process and to identify the relative importance of process parameters, including seed mass and seed PSD.ConclusionsThe main conclusions of the work include the following:It has been shown that a model-based approach can be utilised to support the early stage development of crystallization processes to achieve the desired quality attributes of the product particles.A model-based approach has been shown to help to reduce the degree of experimental effort required compared to a purely empirical approach.",40,4.0
"Covalent Organic Frameworks (COFs) have gained significant attention in the field of material science over last two decades. 2D boronate-ester linked frameworks like COF-5, are porous, crystalline, organic polymers with low density, that can be used for numerous applications such as gas storage and as membranes for various industrial purification applications. COF-5 exhibits high thermal stability, high surface area and permanent porosity widening the scope of its applicability. To maximize its applicability, it is essential to optimize and control the morphology of the crystallites by tweaking appropriate processing conditions. COF-5 can be synthesized by evaporative crystallization using 1,4-diboronic acid (PBBA) and Hexahydroxy Triphenylene (HHTP), in an equimolar solvent mixture of Dioxane and Mesitylene. This process is heterogenous, uncatalyzed and slow, needing three days to reach a considerable and acceptable yield. Researchers have previously made efforts to reduce the reaction barrier and reaction time to 20 hrs. However, a systematic understanding of the effects of alterations in processing conditions on crystallite yield and morphology remain unanswered. Effective and accelerated crystallization of COF-5 while controlling the structure of these crystals is the prime focus of our research. This study has addressed the optimization of processing conditions, catalyst content, and solvent environment to synthesize and control the morphology of COF-5 at a higher rate. Additionally, these conditions were utilized to grow COF-5 films on substrates, while ensuring that their crystallite properties remained intact. Experimentally derived kinetic models using the optimized conditions, have been coupled with population balance equations to get a deeper insight on the crystal growth mechanisms of COF-5. ",40,5.0
"A new framework has been recently introduced to correlate in situ chord length distribution (CLD) measurements and offline particle size distribution (PSD) data (Irizarry et al, 2017) and applied to different particle morphologies (Schoell et al, 2019). Subsequently the model was expanded to consider bimodal distributions and inferences of modality and morphology changes (Irizarry et al, 2020). This work applies the proposed approach to estimate the kinetics parameters for seeded batch crystallization processes of different APIs in combination with population balance modelling. First, a data-driven CLD2PSD model was built for the studied pharmaceutical compounds since the differences in optical properties and particle morphology require a new model for every single substance. Then, time-resolved PSDs have been extracted from in situ CLD data of seeded batch processes and the CLD2PSD model. Finally, the online PSDs have been combined with online solute concentration data to allow for the estimation of crystal growth, secondary nucleation and agglomeration kinetics, depending on the supersaturation level encountered during the process. The validity of this approach has been confirmed by comparison to kinetic data determined using a traditional approach relying on offline data. The main advantage of the proposed approach is that all data is generated online and in situ, thus avoiding sampling errors. ReferencesIrizarry, R. et al., 2017. Data-driven model and model paradigm to predict 1D and 2D particle size distribution from measured chord-length distribution. Chemical Engineering Science, 164, pp. 202-218.Schoell, J. et al., 2019. Determining particle‐size distributions from chord length measurements for different particle morphologies. AIChE Journal, 65, e16560",40,6.0
"IntroductionAlthough preferential crystallization is a standard industrial process to obtain enantiopure crystals of conglomerate forming compounds, its maximum theoretical yield is 50%, unless one adds to it recycle and racemization steps [1]. On the contrary, one can produce an enantiopure powder from an initially racemic one with 100% yield through solid-state deracemization by exploiting several techniques [1,2]. Among such techniques, one based on temperature cycling is a promising candidate for industrial applications due its simplicity of operation [1,3,4]. However, the process itself is complex and hence it warrants the need for a mathematical model to understand the effect of initial conditions and of operating parameters on the process outcome, i.e. the production of the desired enantiomer, the process time, etc.Continuous processes have been shown to outperform batch operation. In addition, they can recover from process disturbances, because they run at stable steady state [5]. Continuous crystallization processes have been adapted for the separation of conglomerate forming enantiomers, in order to exploit the advantage of continuous crystallization techniques. In case racemization of enantiomers in the solution proves difficult, continuous preferential crystallization without a racemization reaction is a widely studied choice [6]. If the racemization reaction is available, continuous preferential crystallization with racemization reaction or continuous Viedma ripening can be performed. Alternatively, deracemization via temperature cycles in mixed suspension mixed product removal crystallizers (MSMPRC) can also be performed, but this is an unexplored method of choice.Mathematical modelA population balance equation (PBE) -based mathematical model has been developed to provide a clear explanation of the phenomena involved in the deracemization process. The model describes the interplay of several phenomena involved in a temperature cycling process (size-dependent solubility, crystal growth and dissolution, agglomeration, attrition/secondary nucleation and racemization) accounting for the dependence of their thermodynamic and kinetic parameters on the crystal size and on the temperature [5]. Results and discussion- batchUsing the PBE-model that we have introduced, first, we have simulated temperature cycles in batch crystallizers. A mathematical model of deracemization is a useful tool to perform process analysis: it provides information on all the relevant physical quantities during the whole process and enables decoupling of the individual phenomena, and investigating the effect of variations in e.g., the breakage rate intensity, the agglomeration intensity, etc. After identifying the necessary phenomena to deracemize using temperature cycles, process characterization was performed using the developed model, in order to identify an optimal process. Parametric studies varying operating parameters like the temperature and cooling rate were performed and based on the outcome it is shown that the simulation outcome qualitatively describes experimental results [1,4].Moreover, by investigating the effect of slight variation in the initial conditions, an explanation to the large variations in deracemization time and in process outcome — that are typically observed in experiments — is provided using the mathematical model.Results and discussion- continuousThe inherent disadvantage of the batch crystallization process is that it suffers from batch-to-batch product variability and requires control techniques to obtain the product with the specific quality of interest. As tuning the many degrees of freedom blindly while performing the actual experiments is ill-advised [7], this contribution aims at providing a mathematical tool that can be used to design robust continuous processes.Therefore, we have extended our batch non-isothermal population balance-based model of solid-state deracemization via temperature cycles to simulate the proposed continuous configuration. To avoid undesired nucleation of the counter enantiomer, different process variants are considered, e.g., utilization of recycle loop, wet mill, fines dissolution loop. We have investigated the steady state operation of temperature cycles carried out in MSMPRC. Periodic operation of the continuous crystallizer via temperature cycling affects the transient behaviour of the process and thereby the attainable steady state. We have run simulations to study the interplay of the cycle time and the residence time with respect to the physical properties of the system, i.e., how the process characteristic times compare to the characteristic times of growth, dissolution and racemization.The aim of this analysis was to verify whether one of the continuous configurations envisaged performs better than the batch alternative. The effect of the initial conditions (variations in the initial enantiomeric excess, in the suspension density, etc.) and of the operating parameters on the periodically perturbed process and its steady state in terms of productivity and attainable enantiomeric excess are identified. Together with this assessment, we aim to provide strategies for the optimal implementation of the process.AcknowledgmentsThis research has received funding as part of the CORE project (October 2016 – September 2020) from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 722456 CORE ITN.References[1] Breveglieri, G. M. Maggioni, M. Mazzotti, Cryst. Growth Des. 18 (2018) 1873-1881.[2] L. Noorduin, W. J. P van Enckevort, H. Meekes, B. Kaptein, R. M. Kellogg, J. C. Tully, J. M. McBride, E. Vlieg, Angew. Chem., Int. Ed. 49 (2010) 8435-8438.[3] Iggland and M. Mazzotti, Cryst. Growth Des. 18 (2011) 4611-4622.[4] Suwannasang, A. E. Flood, C. Rougeot, G. Coquerel, Org. Process Res. Dev. 21 (2017) 623-630.[5] Köllges and T. Vetter, Cryst. Growth Des. 17 (2017) 233-247.[6] Vetter, C. L. Burcham, M. F. Doherty, AICHE J. 61 (2015) 2810-2823.[7] Bodák, G.M. Maggioni, M. Mazzotti, Cryst. Growth Des. 18 (2018) 7122-7131.",40,7.0
"Projection to latent spaces (PLS) is a widely used statistical modeling approach and has found excellent applicability in modeling, monitoring and control [1-2] of complex batch processes. In batch processes, the quality of the final product is of utmost importance and predicting the quality of products well in advance is essential to move the process towards the desired objective. The simple PLS models built with data from previous batches have demonstrated great success in this regard. However, use of process knowledge in developing these models have not been explored in sufficient detail. Motivated by these considerations, in the present work, a hybrid PLS model built with recorded measurement data and data based on first principles model is proposed. A batch crystallization process is chosen as the motivating example where the particle size distribution at batch end is our desired quality variable.Measuring quality related variables, such as particle size, in a lab during batch runs is an expensive and time intensive task thus motivating the use of good models to predict time evolution of the process variable trajectories and the final quality attributes at the end of the batch. Detailed first principles models of these processes are often available which can predict such final quality attributes with reasonable precision. However, to do so they need to be calibrated to existing plant data by estimating key parameters in the model. Furthermore, to be used in on-line monitoring and control, where disturbances are present, these models need to be run in the form of an observer, Extended Kalman Filter (EKF) or incorporated into a Moving Horizon Estimator (MHE), all of which require considerable expertise and effort to build and maintain. As a result, fundamental models rarely find application in on-line monitoring, optimization and control.. However, these first principles models can provide much useful information that can augment on-line process data to potentially provide improved final quality attribute predictions.This work addresses the problem of integrating fundamental process knowledge with measurement data to build hybrid PLS models with better predictive ability. This is done by considering the process measurements of historical batches as inputs to the first principles model to predict batch trajectories of variables which are not directly measured. The X block matrix is then augmented with this valuable information once the data of these variables is generated. The first principles models do not have to be calibrated to plant data. Rather, they only need to help in predicting changes in the direction of the process during its duration. The hybrid batch PLS models will accommodate for any biases in the trajectory predictions of the incorporated first principles model, and, in an on-line monitoring mode, will provide the time varying adaptation of the predictions that might be afforded by an EKF or MHE implementation of a first principles only model.This hybrid PLS methodology approach is used to illustrate this straightforward but powerful approach to predict the final crystal size distribution for the batch crystallization process and is compared with the standard PLS approach based only on plant data, and to subspace based quality models. The simulation results show the improved predictive capability of the proposed approach over the other modeling techniques considered. [1] Paul Nomikos, John F. MacGregor, Multi-way partial least squares in monitoring batch processes, Chemometrics and Intelligent Laboratory Systems, Volume 30, Issue 1, 1995, Pages 97-108, ISSN 0169-7439[2] Latent variable MPC for trajectory tracking in batch processes, Journal of Process Control, Volume 15, Issue 6, 2005, Pages 651-663, ISSN 0959-1524[3] Dan Shi, Nael H. El-Farra, Mingheng Li, Prashant Mhaskar, Panagiotis D. Christofides, Predictive control of particle size distribution in particulate processes, Chemical Engineering Science, Volume 61, Issue 1, 2006, Pages 268-281, ISSN 0009-2509",41,0.0
"Continued improvement to data-driven algorithms has led to a growing list of data-driven-driven solutions to real-world problems, including applications for material characterization, image recognition and discovery of novel chemical compounds. [1, 2] Many of these applications can be characterized by ‘static’ conditions. However, in process systems engineering the applications are more often ‘dynamic’, requiring models to accurately predict process performance under time-varying conditions. For such applications, data-driven approaches have been less robust due to their limited interpretability and failure to observe known physical laws, especially for conditions not considered in model training. Thus, modeling efforts in these domains rely primarily on building mechanistic models based on first principles, which can be a time and labor-intensive process. A union of mechanistic and data-driven modeling techniques (hybrid modeling, HM), has been proposed that can enhance model interpretability while greatly accelerating the model-development process. [3] Unlike purely data-driven methods, HM has yet to become a standard tool for many who could benefit from its accelerated workflow. This is partially due to the lack of automated, computationally efficient tools, enabling practitioners to use hybrid methods at a high level. Moreover, several competing frameworks make identifying which framework is optimal for a given system difficult.In this presentation a framework is proposed for integrating data-driven models with mechanistic constraints for problems with the following characteristics: (1) nonlinear dynamic systems without steady state assumptions, (2) unknown mechanisms, and (3) incomplete and noisy data. We demonstrate how open-source software for automated parameter estimation, such as checkpointing, automated differentiation and adaptivity enable faster development of HMs. We weigh the merits of approaches that estimate parameters of the data-driven coupled with and without physical constraints. For methods coupling physical constraints, a formulation based on forward sensitivity equations is often used. [4] In contrast, in light of recent work [5], we investigate a backward adjoint-based formulation for enforcing physical constraints during model fitting. Through this comparison we demonstrate how the choice of numerical formulation plays an important role in the size and number of relationships that can be modeled via a hybrid framework. Advantages and limitations of hybrid frameworks will be identified with the aim of enabling practitioners to integrate hybrid workflows to answer domain-specific questions for applications common in (bio)chemical process engineering, such as reaction analysis, model-based optimization, and adaptive design of experiments.ReferencesHimanen, L., et al., Data-Driven Materials Science: Status, Challenges, and Perspectives. Advanced Science, 2019. 6(21): p. 1900808.Venkatasubramanian, V., The promise of artificial intelligence in chemical engineering: Is it here, finally? AIChE Journal, 2019. 65(2): p. 466-478.von Stosch, M., et al., Hybrid semi-parametric modeling in process systems engineering: Past, present and future. Computers & Chemical Engineering, 2014. 60: p. 86-101.Oliveira, R., Combining first principles modelling and artificial neural networks: a general framework. Computers & Chemical Engineering, 2004. 28(5): p. 755-766.Rackauckas, C., et al. Universal Differential Equations for Scientific Machine Learning. arXiv e-prints, 2020. arXiv:2001.04385.",41,1.0
"So-called glass (white)-box[1] models are built from scientific theories to capture expert knowledge and intuition for observed phenomena. Black-box (surrogate)[2] models on the other hand employ statistical and machine learning paradigms to reveal trends in (large) volumes of data. At the intersection of these paradigms, hybrid (grey-box) models fuse glass-box models with black-box statistical machine learning components to correct for phenomena not accounted for by the foundational scientific theories. In this talk, we explore efficient decision-making (optimization) under uncertainty with hybrid models. We advocate for a Bayesian approach and use joint posterior distribution from model training to generate scenarios for stochastic programming. We demonstrate how this workflow enables model-form uncertainty quantification and propagation using a ballistic firing example[3] and two additional case studies.Epistemic or model form uncertainty arises from model inadequacies due to simplifying assumptions or neglected control variables. Aleatoric uncertainty is a consequence of random phenomena such as experimental variability and noisy observations. The seminal work by Kennedy and O'Hagan[4] introduced a Bayesian framework for the simultaneous quantification of aleatoric and epistemic uncertainty inherent in all mathematical models and is shown in Eq (1) (see attached image).Experimental observations for experiment i is modeled using three components. The glass-box model η(.,.) contains physically meaningful global parameters θ and inputs xi. The stochastic discrepancy function δ(.) counteracts systematic bias in the glass-box model. Observation error εi is modeled as uncorrelated white noise, i.e., N(0,σ2I). Outputs of the Gaussian Process (GP) discrepancy function δi ~ GP(μ(.),k(.,.)) follow a conditional normal distribution with the mean and covariance fully specified by μ(.), and k(.,.), respectively. The parameters of the kernel function, along with those of the white noise model (i.e. σ) are denoted as hyperparameters φ.[5]The model parameters θ and hyperparameters φ are determined using Bayesian calibration, shown in Eq. (2) (see attached image). Bayesian model calibration interprets parameters ω as random variables. The standard workflow is to encode one's current belief in a prior probability distribution, observe data D, and apply Bayes rule, Eq. (2), to obtain the posterior probability distribution p(ω|D).[6] In our workflow, we use the posterior distribution from the calibration of the hybrid model as the uncertainty set in a stochastic program to minimize the uncertainty in the model predictions.Using ballistic firing as an illustration,[1] we demonstrate how Bayesian hybrid models overcome systematic bias from missing physics by simulating computer experiments to hit a target using a single-stage stochastic program. We highlight how the hybrid model outperforms a pure machine-learning (data-driven, GP) model when the amount of data available is low (5 to 6 experiments). We argue that Bayesian hybrid models are an emerging paradigm for data-informed decision-making under uncertainty by discussing two case studies from chemical engineering. In the first case study, we use our workflow to design a continuously stirred tank reactor (CSTR) by calibrating a reaction kinetics model with oversimplified assumptions using Bayesian hybrid models. In the second case study, we use Bayesian hybrid models to fit thermodynamic data to equations of state and propagate epistemic uncertainty in using our workflow in designing a flash process. We contrast the merits of our technique against a pure machine-learning (data-driven) GP based workflows and conclude by advocating for a fully Bayesian decision-making methodology which provides probability distributions necessary for stochastic programming and enables iterative (sequential) design of experiments.References:[1] Eason, J. P. & Biegler, L. T. Advanced trust region optimization strategies for glass box/blackbox models. AIChE Journal 64, 3934-3943 (2018). doi.org/10.1002/aic.16364.[2] Jones, M., Forero-Hernandez, H., Zubov, A., Sarup, B. & Sin, G. Superstructure optimization of oleochemical processes with surrogate models. In Computer Aided Chemical Engineering, 44, 277-282. 10.1016/B978-0-444-64241-7.50041-0.[3] Eugene, E. A., Gao, X. & Dowling, A. W. Learning and optimization with Bayesian hybrid models. In Proceedings of the 2020 American Controls Conference, Accepted (2020). arXiv:1912.06269.[4] Kennedy, M. C. & O'Hagan, A. Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63, 425-464 (2001). 10.1111/1467-9868.00294.[5] Bishop, C. M. Pattern Recognition and Machine Learning (Springer, 2006).[6] Higdon, D., Kennedy, M., Cavendish, J. C., Cafeo, J. A. & Ryne, R. D. Combining field data and computer simulations for calibration and prediction. SIAM Journal on Scientific Computing 26, 448-466 (2004). 10.1137/S1064827503426693.",41,2.0
"First-principles models can provide very good prediction even for cases when there are no data at all, or data are limited in certain range of operating conditions or for cases where data collection is infeasible. However, construction of these models can be time consuming, computationally expensive, and intractable for online adaptation. Furthermore, it can be difficult, if not impossible, to develop accurate models for some complex phenomena, that are poorly understood. On the other hand, artificial intelligence (AI) models can be simpler to develop even for complex and ill-defined dynamic systems. These models are typically computationally inexpensive, and therefore can be adapted online. However, development of AI models requires large amount of data1 so can be infeasible where data collection is expensive or collection of certain type of data is practically impossible given the current state of the measurement technology. Furthermore, AI models may not be predictive especially when they are extrapolated and/or if the data used for developing the AI models suffer from information gap. This work develops an approach where the first-principles models can be synergistically coupled with the AI models thus exploiting their key strengths.Various types of AI modeling techniques such as neural network (NN), deep learning, expert systems and fuzzy logic have been employed for process synthesis, design and modelling2. More recently, hybrid first-principle AI approaches have been proposed3 for reactive systems by using a linear dynamic model that is obtained through linearization of a non-linear dynamic model followed by a static neural network. That approach fails to account for the nonlinearity in the first-principles model and dynamics in the nonlinear AI model. Furthermore, neither the first-principles model nor the NN model is adaptive. If both the first-principles model and AI model are dynamic, nonlinear, and adaptive, it becomes considerably challenging to optimally synthesize such hybrid models with due consideration of complexity, computational expense, and accuracy.A hybrid first principles-AI modeling technique has been developed where not only both the first principles and AI models are dynamic and nonlinear but they also interact with each other leading to a time-varying model. Instead of a series structure, we propose a hybrid series-parallel structure where, instead of propagation of information from the first- principles model through the AI model in the series structure mentioned above, the AI model learns the residual error from the first-principles model stemming from the unmodeled and/or unknown phenomena. The AI model is desired to be dynamic, nonlinear, and adaptive. While existing adaptive NN models such as adaptive bidirectional associative memory (ABAM)4 and transversal/recursive filters5,6 have found applications in signal processing and communication, they are inadequate for many chemical engineering applications where learning must be accomplished with data that may be noisy, limited, and non-informative. In this work, we propose a hybrid static-dynamic NN structure that can quickly learn to equilibrate to a minimum energy state. The developed gradient descent algorithm for learning can efficiently deal with blowing and vanishing gradients by developing a modified batch normalization approach. The algorithm also quantifies optimality gaps at every iteration reflecting the tradeoff between the computational expense and accuracy. For making the learning process computationally efficient for highly parameterized system, the algorithm down selects weights using a sensitivity-based approach.The proposed algorithm is applied to the Van de Vusse reactor model as well as to a supercritical boiler system where complex dynamics associated with reactive-diffusive processes leading to oxide scale formation in the superheater tube banks coupled with mass and heat transfer makes it a challenging system. Our work shows the tradeoff between computational expense and accuracy. It is observed that the stability and speed of the learning algorithm are critical for success of the proposed algorithm for a real-life application.ReferencesZendehboudi, S., Rezaei, N. & Lohi, A. Applications of hybrid models in chemical, petroleum, and energy systems: A systematic review. Appl. Energy228, 2539–2566 (2018).Venkatasubramanian, V. The promise of artificial intelligence in chemical engineering: Is it here, finally? AIChE J.65, 466–478 (2019).Chen, L., Hontoir, Y., Huang, D., Zhang, J. & Morris, A. J. Combining first principles with black-box techniques for reaction systems. Control Eng. Pract.12, 819–826 (2004).Zupan, J. & Gasteiger, J. Neural networks: A new method for solving chemical problems or just a passing phase? Anal. Chim. Acta248, 1–30 (1991).Roussel-Ragot, P., Personnaz, L., Dreyfus, G. & Marcos, S. Communicated by Steven Nowlan Neural Networks and Nonlinear Adaptive Filtering: Unifying Concepts and New Algorithms 0. Nerrand. 199, 165–199 (1993).Schädler, K. & Wysotzki, F. Comparing structures using a Hopfield-style neural network. Appl. Intell.11, 15–30 (1999).",41,3.0
"This work presents an automated framework for optimization which integrates a modified trust region approach, generation of surrogate models, and a direct link with commercial simulators and algebraic modeling systems. The effectiveness of the proposed approach is demonstrated with two case studies. First, a simple flash model case study is proposed. Secondly, an MEA-based carbon capture system [1] where the reboiler duty was minimized while achieving a 90% capture rate. A comparison with available derivative free optimization methods shows improved solutions in both time and accuracy.Two main approaches for optimization have been used in carbon capture: 1) pure mathematical optimization and 2) simulation-based optimization. Pure mathematical optimization directly leverages the equations describing the physical system to be optimized. This approach encounters challenges, however, where large sets of PDE’s and highly complex, nonlinear representations are required to sufficiently characterize the process of interest. Simulation-based optimization, on the other hand, considers the system model to be a black box and is based on a heuristic algorithm that uses the results from process simulations to obtain the relationship between the relevant system input and output variables. Although this approach can be used to obtain satisfactory results for large scale, complex systems, it can often be computationally expensive [2]. Hybrid approaches have been proposed in literature [3]; however, their general implementation often requires a very large effort. Considering the advantages and disadvantages of these approaches, a streamlined hybrid framework is introduced to perform efficient optimization of process systems, in terms of solution accuracy, and time.This work is based on a surrogate model (SM)-based optimization algorithm that involves generating a simplified approximation of the rigorous process model (i.e. built using advanced commercial simulators like ASPEN, gPROMs, Python, etc.) that is more amenable to gradient-based optimization methods [4] and nonlinear programming (NLP) solvers. The approach can overcome the difficulties associated with complex process models (for example black box models or rigorous first principle models), without significantly compromising solution quality and speed, provided that the surrogate modeling method is accurate. The proposed work consists of a modified trust region method, which generates complex surrogate models in order to obtain the optimal solution [5]. The proposed framework has been implemented as a capability within NETL’s Framework for the Optimization and Quantification of Uncertainty and Surrogates tool (FOQUS) [6]. The algorithm for the SM-based optimization is highlighted in Figure 1. The algorithm is described in five steps.Step 1: The algorithm begins with the user providing a process simulation, input and output variables, an initial data set, upper and lower bounds for the optimization variables, and termination conditions. Then, an initial algebraic surrogate model is developed for each output variable for the full space problem (initial data set, set of candidate basis functions, and type of surrogate modeler are selected by the user).Step 2: Using the initial set of surrogate models and user input (bounds, objective function, ad hoc constraints, etc.), the software builds and solves the mathematical optimization model. The first optimization problem consists of the full-scale problem (original upper and lower bounds of decision variables). Using a multi-start approach, it proceeds to solve multiple problems at different initial conditions, reporting the optimal solution x* and y.Step 3: The original simulator model is solved at the optimal solution (x*) obtained by the optimization problem. This solution is input into the black box model to determine y* which corresponds to the solution obtained from the advanced process simulator model.Step 4: This is followed by a set of termination conditions to check whether the optimal solution of the surrogate model is in good agreement with the simulator model (y*). If this condition is satisfied, and the constraints are satisfied, the algorithm is terminated. However, if the termination conditions are not satisfied, the trust region is updated.Step 5: The trust region is updated by reducing the upper and lower bounds on the decision variables, generating a new data set using Latin Hypercube sampling method, re-running the advanced process simulation for the new data set, and generating a new set of surrogate models.The algorithm will repeat steps 2 to 5 until convergence. A summary of each algorithm iteration and the final optimal solution are provided, along with a file containing the surrogate models built at each iteration, and a python file containing the parity plots for surrogate model validation.The algorithm performance is compared against the derivative free optimizers available within FOQUS using two case studies. The first case study entails optimizing a simple flash model simulated in Aspen Plus [7]. The process model includes 59 variables and equations that get solved for simulation in Aspen Plus. Here, the revenue ($/hr) from selling two products is maximized, subject to a CO2 purity constraint on the vapor stream. In this example, the DFO (based on NLOpt) and SM algorithms find the same optimal solution, though the SM-algorithm converges far more quickly (128 vs. 358 CPU-seconds).The second case study entails optimizing an MEA carbon capture system model simulated in Aspen Plus. The process model includes 46071 variables and equations that get solved for simulation in Aspen Plus. In the second case study, an MEA carbon capture simulation is solved to optimality more quickly using the SM-algorithm compared with DFO solver (48 vs. 65 CPU-minutes). Additionally, the SM-algorithm finds a better optimal solution, which would translate in large savings during a year of operation.Compared with DFO’s, the algorithm presented is expected to show better performance (time and solution accuracy) when larger problems are studied. Finally, the proposed approach presents a generic framework for rigorous optimization of large-scale nonlinear systems.References[1] Morgan, J.C., Soares, C.A., Omell, B., Bhattacharyya, D., Tong, C., Miller, D.C. (2018). Development of a rigorous modeling framework for solvent-based CO2 capture. Part 2: Steady-state validation and uncertainty quantification with pilot plant data. Industrial & Engineering Chemistry Research 57: 10464-10481.[2] Larson, J., Menickelly, M., Wild S. M. Derivative-Free Optimization Methods. Optimization Online. http://www.optimization-online.org/DB_FILE/2019/04/7153.pdf[3] Lee, U., Burre, J., Caspari, A., Kleinekorte, J., Schweidtmann, A.M., Mitsos, A. (2016) Techno-economic Optimization of a Green Field Post-Combustion CO2 Capture Process Using Superstructure and Rate-Based Models. Industrial & Engineering Chemistry Research. 12014-12026.[4] Cozad, A., Sahinidis, N., Miller, D.C. (2014). Learning Surrogate Models for Simulation-Based Optimization. AIChE Journal 60(6): 2211-2227.[5] Eason, J. P., Biegler L. T. (2016). A Trust Region Filter Method for Glass Box/Black Box Optimization. AIChE Journal. 3124-3136.[6] Miller, D. C., Agarwal D. A., Bhattacharyya D., Boverhof J., Cheah Y-W., Chen Y., et al. (2016). Innovative computational tools and models for the design, optimization and control of carbon capture processes. 26th European Symposium on Computer Aided Process Engineering – ESCAPE 26. 2391-2396[7] Aspen Plus V8.0 – Optimization Tutorial, Merten Morales, Swiss Federal Institute of Technology Zurich (Aug 2013)",41,4.0
"Good predictive models are essential in modern chemical industry for control and optimization. Mechanistic models, incorporating physical and empirical knowledge, are dominant. However, the incorporated knowledge is inherently a simplification of reality, resulting in model structure uncertainty. Moreover, gathering the necessary knowledge is time-consuming and might be unfeasible for complex poorly-understood processes. With an ever-increasing amount of data available, data-driven methods are becoming more attractive. In these models, the structure is not explicitly specified, but rather determined by searching for relationships in the available data. Given sufficient and representative data, these models can make highly accurate predictions, unconstrained by any assumptions made. Therefore, they are particularly powerful for learning complex and poorly understood dynamics. The downside is their complete lack of interpretability. Moreover, as representative data is needed, they fail to extrapolate into regions not seen before.Combining both approaches into a hybrid model aims at taking the best of both worlds. The mechanistic component leverages the available knowledge to model the well-understood parts. The data-driven component models the poorly-understood dynamics to fill in the knowledge gaps. Combinations of both approaches are usually achieved through an uncoupled approach, whereby first a mechanistic model is set-up and calibrated and subsequently the data-driven model is calibrated on the residual error of the mechanistic model (i.e., the difference between the model prediction and data). The data-driven component thus corrects the errors of the mechanistic component. This so-called parallel architecture leverages knowledge and data, resulting in an increased predictive power at lower data requirements [1]. However, as the data-driven model operates on the states of the system, it does not learn its dynamics (i.e., the differentials of the states), but rather the effects of those dynamics. Therefore, an uncoupled hybrid model does not generalize well [2]. Moreover, as the data-driven model is calibrated on the residual error of the mechanistic model, the parameters of both models are uncoupled. Therefore, no synergies between the data-driven and mechanistic models are possible.Here, we propose a novel framework for coupled hybrid models based on neural differential equations [3]. Neural differential equations define the right-hand side of a differential equation as a neural network. The neural network is subsequently calibrated using backpropagation on the gradient obtained through automatic differentiation, directly learning the dynamics of the system. We combine these neural differential equations with mechanistic differential equations and calibrate the parameters simultaneously, resulting in a coupled hybrid model.We show the power of this coupled approach on two case studies with incomplete knowledge of the dynamics of the system. In a first study, a simple reactor is modeled whose kinetics are not completely known. A known forward reaction, converting chemical A into chemical B, is modeled mechanistically. However, unknown to the modeler, a reversible reaction occurs, converting chemical B into chemical A. This reaction is thus not incorporated into the mechanistic model, resulting in a model that does not fully capture the dynamics of the system. We show that the coupled hybrid model successfully retrieves the missing kinetics, resulting in a model with high predictive power. Although this result seems trivial on this toy problem, it highlights the possibilities of applying this approach to larger, more complex systems.Second, we model an enzyme microreactor with a pulsating feed (e.g., resulting from a positive displacement pump), inducing complex hydrodynamics. We generate synthetic data of this system using a transient CFD solver and log the outlet concentrations as a timeseries. The system is subsequently modelled using a simple mechanistic model that only includes kinetics. As the hydrodynamics are not incorporated, the model does not fully capture the dynamics of the system. Again, we show that the data-driven component of the coupled hybrid model successfully retrieves the hydrodynamics, resulting in a model that truly represents the system.In conclusion, we introduce an architecture for coupled hybrid models based on neural differential equations. The mechanistic component incorporates all available knowledge of the modeled system, while the data-driven component captures any unmodeled dynamics, including (but not limited to) unknown kinetics and complex hydrodynamics.",41,5.0
"Incorporating gradient information during the regression of surrogate models of various processes can greatly improve their representation of the process and help simulation and optimization of processes by providing more reliable gradient behavior [3]. Recently, Cozad and Sahinidis [2] proposed an MINLP formulation of symbolic regression that can be expanded to simultaneously calculate the first and second partial derivatives of key parameters. The purpose of the current paper is to enhance the surrogate model regression procedure with gradient information to improve the bounding and model discrimination by incorporating partial first and second derivatives into the regression framework. Symbolic regression learns both the model structure and parameters to model a data set, unlike traditional regression that limits the scope of the regression to a fixed functional form and has typically been performed with genetic programming [1]. The regression only requires the specification of a set of operators and operands (+, -, *, /, exp(.), log(.), sqr(.), cub(.), √(.), etc.) to flexibly develop new functional forms that accurately represent the data. Recent developments in applying a global deterministic approach have shown improved fitting metrics, such as sum of squared error and other information criterion [2,5]. Further expanding the symbolic regression capabilities by applying chain rule evaluations of the derivatives while simultaneously determining the model structure allows for gradient information to impact the regression process and improve the surrogate representation of the data. We applied this technique to benchmarked equations to demonstrate these new capabilities.Reference cited:[1] Koza, J.R.: Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT Press, Cambridge, MA (1992) 24. [2] Cozad, A. & Sahinidis, N.V. (2018). A global MINLP approach to symbolic regression. Mathematical Programming, 170, 97-119, 2018.[3] Matias, J. & Jaschke, J (2019) Using a neural network for estimating plant gradients in real-time optimization with modifier adaptation. IFAC-PapersOnLine, 15, 808-813, 2019.[4] J. McDermott, D.R. White, S. Luke, L. Manzoni, M. Castelli, L. Vanneschi, W. Jaśkowski, K. Krawiec, R. Harper, K.D. Jong, U.M. O’Reilly, Genetic programming needs better benchmarks, in Proceedings of the Genetic and Evolutionary Computation Conference (GECCO) (ACM, Philadelphia, 2012)[5] Tawarmalani, M.;Sahinidis, N. V. Global optimization of mixed-integer nonlinear programs: A theoretical and computational study, Mathematical Programming, 99, 563-591, 2004.",41,6.0
"Classification models are typically used to separate datasets based on a set of descriptors; however, such models can also be used to make decisions (e.g., make go/no-go decisions in a project). In this context, fairness becomes a major issue because such decisions tend to affect multiple stakeholders and because descriptors might inadvertently introduce biases [1, 2, 3]. A recent application of fair classification models include allocation of financial loans (e.g., descriptor is credit score). In this work, we show how fair classification models can also be used to tackle problems of interest to the chemical engineering community. For instance, in a manufacturing facility, access to experimental lab equipment is simultaneously requested by different stakeholders to test a variety of products. The decision here is whether a sample should be tested or not (or when) given the importance of the sample (captured by descriptors) and given that there is limited lab equipment and budgets. Such decisions are often based on minimizing a loss function, which is an aggregate of the cost associated with false positives and false negatives. The model output is a threshold value (based on a linear combination of sample features) that activates the binary decision. The inherent degeneracy of such models can lead to decisions that prioritize testing in an unfair manner [4].State of the art research in fairness in machine learning focuses on developing algorithms that do not violate state or federal anti-discrimination laws. This is achieved by adding either post-processing steps or additional constraints to the mathematical model to achieve properties such as demographic parity, equalized odds, and equal opportunity [1,2]. These conditions can be mutually exclusive and often lack a theoretical basis that connects them to the axiomatic view of fairness. In this work, we will present mixed-integer formulations that address the problem of fair classification from an axiomatic perspective [5]. In the field of game theory, the resource allocation problem has been viewed as a bargaining game between stakeholders. Nash [6] first provided an axiomatic approach to obtain solutions to the bargaining problem. These axioms include Pareto optimality, symmetry, affine invariance, and independence of irrelevant alternatives. Nash also proved that there exists an allocation scheme that satisfies these axioms (what is now known as the Nash solution). We observe that the ultimate goal of a fairness measure such as the Nash solution is to shape an allocation distribution in a desirable way. As such, the resource allocation problem can also be interpreted as a stochastic programming problem in which one seeks to find allocations that shape distribution of outcomes (in stochastic programming the outcome distribution is shaped by using a risk measure) [7, 8]. As in the case of fairness measures, axioms have been proposed in the stochastic programming literature to study the selection of suitable risk measures [9].We present a theoretical analysis of the statistical properties of different fairness criteria introduced in binary classification models. We will also provide a case study of a manufacturing facility where a binary classification model is used to drive the decision of which samples should be tested. We will demonstrate the impact of using Nash solution on the allocation distribution of tests between the equipment and the overall testing efficiency (measured by false positives and false negatives). We will showcase how the Nash solution inherently captures this tradeoff between efficiency and fairness, and selects the optimal solution based on the axiomatic properties.References:[1] Gölz, P., Kahng, A. and Procaccia, A.D., 2019. Paradoxes in Fair Machine Learning. In Advances in Neural Information Processing Systems (pp. 8340-8350).[2] Hardt, M., Price, E. and Srebro, N., 2016. Equality of opportunity in supervised learning. In Advances in neural information processing systems (pp. 3315-3323).[3] Wattenberg, M., Viégas, F. and Hardt, M., 2016. Attacking discrimination with smarter machine learning. Google Research, 17.[4] Sampat, A.M. and Zavala, V.M., 2019. Fairness measures for decision-making and conflict resolution. Optimization and Engineering, 20(4), pp.1249-1272.[5] Moulin, H., 1991. Axioms of cooperative decision making (No. 15). Cambridge university press.[6] Nash Jr, J.F., 1950. The bargaining problem. Econometrica: Journal of the Econometric Society, pp.155-162.[7] Dowling, A.W., Ruiz-Mercado, G. and Zavala, V.M., 2016. A framework for multi-stakeholder decision-making and conflict resolution. Computers & Chemical Engineering, 90, pp.136-150.[8] Hu, J. and Mehrotra, S., 2012. Robust and stochastically weighted multiobjective optimization models and reformulations. Operations research, 60(4), pp.936-953.[9] Artzner, P., Delbaen, F., Eber, J.M. and Heath, D., 1999. Coherent measures of risk. Mathematical finance, 9(3), pp.203-228.",41,7.0
"Solvents play a dominant role in the manufacturing of pharmaceutical products, as they are extensively used to facilitate synthetic reactions, enable separation/purification and take part in the final drug formulation. More than 80% of waste and byproducts generated in a typical active pharmaceutical ingredient (API) manufacturing process (i.e., 25-100 kg of byproducts/kg API [1]) is related to solvents used in purification processes [1,2]. Hence, selecting suitable solvents in different purification stages can affect the overall process efficiency and final product quality, as well as cost, environmental, health and safety metrics [3,4]. However, choosing optimal solvents (or solvent mixtures) and the best operating conditions for a given process can be challenging due to the discrete nature of the problem (that can lead to combinatorial explosion) and the trade-offs between competing objectives (e.g. enhance product quality attributes can lead to increased cost and/or environmental impact). In current practice, most pharmaceutical companies employ solvent selection guides to (i) consider the physicochemical properties and/or environmental profile of solvents, and (ii) focus on reducing solvent consumption in order to minimise cost and environmental footprint [4,5]. Despite these advances, most of the methods used are based on heuristic approaches or time-consuming experimental investigations. Such trial-and-error practices can often be expensive and are liable to lead to sub-optimal designs that fail to consider the integrated nature of all process decisions. Thus, the development of efficient methodologies and more sophisticated tools that explore all possible options and consider integrated process decisions would be an indispensable tool in the design of new pharmaceutical processes.Several solvent mixtures design approaches [6-9] are based on the computer-aided mixture/blend design (CAMbD) framework [10] and are applied to the crystallisation of active pharmaceutical ingredients. To date, most of the proposed methods consider optimising a single purification process unit (e.g., crystallisation) with fixed operating conditions (e.g., fixed temperature). Recently, this has been broadened to the consideration of temperature as a variable, to investigate hybrid anti-solvent and cooling crystallisation [9]. In this work, the scope of design is extended further and a comprehensive solvent mixture design methodology for the integrated crystallisation and isolation processes of pharmaceutical compounds is presented. The proposed integrated approach considers a combined cooling and antisolvent crystallisation process, where optimal solvent and antisolvent mixtures, their proportions in solution, and optimal process temperatures are determined simultaneously, while maximising crystal yield and reducing solvent consumption [9]. In addition, critical interlinked design decisions, such as API solubility across both crystallisation and isolation stages, as well as the miscibility of crystallisation and wash solvents, are taken into account. Furthermore, environmental, health and safety performance measures are considered in the comprehensive model, so that only safe and environmentally friendly solvents are identified.The design method is applied to identifying high-performance solvent blends for the crystallisation and isolation of mefenamic acid, while removing an impurity, chlorobenzoic acid, from the system. This work demonstrates that optimising simultaneously several design decisions and different performance criteria can lead to significant improvements in overall process efficiency over conventional designs.W. Cue, J. Zhang, 2009. Green Chemistry Letters and Reviews 2, 193-211.D. Curzons, C. Jiménez-González, A.L. Duncan, D.J.C. Constable, et al., 2007. The International Journal of Life Cycle Assessment 12, 272-280.J. Brown, T. McGlone, S. Yerdele, V. Srirambhatla, et al., 2018. Molecular Systems Design & Engineering 3, 518-549.Jiménez-González, P. Poechlauer, Q.B. Broxterman, B-S. Yang, et al., 2011. Organic Process Research & Development 15, 900-911.Prat, J. Hayler, A. Wells, 2014. Green Chemistry 16, 4546-4551.T. Karunanithi, L.E.K. Achenie, R. Gani, 2006. Chemical Engineering Science 61, 1247-60.Jonuzaj, P.T. Akula, P. Kleniati, C.S. Adjiman, 2016. AIChE Journal 62, 1616-33.Jonuzaj, A. Gupta, C.S. Adjiman, 2018. Computers & Chemical Engineering 116, 401-421.L. Watson, A. Galindo, G. Jackson, C.S. Adjiman, 2019. Computer Aided Chemical Engineering 46, 949-954.Gani, R., 2004. Computers & Chemical Engineering 28, 2441-2457.",42,0.0
"A novel sampler device for flowing powders was tested to quantify drug concentrations as low as 0.76% w/w in pharmaceutical powder blends. The sampler device was developed based on the powder flow behavior within a tablet press feed frame, following the principles laid down in the Theory of Sampling. Two Near-Infrared (NIR) spectroscopic calibration models were developed with powder blends that varied from 0.52 to 2.52% w/w and 1.51 to 4.52% w/w. The calibration models were able to determine caffeine concentration in test set blends with root mean square error of predictions and bias below 0.1% w/w. Samples were collected from the sampler device and analyzed by ultraviolet-visible (UV-Vis) to determine the caffeine concentration. A high agreement between the in-line NIR predictions and the sampled UV-Vis results was found. The paddle wheel speed in the sampler can be varied up to ±10% without affecting NIR predictions; however, the models did not respond adequately to a 25% increase in this speed. Variographic analysis showed that the sampler device may quantify low drug concentrations with nugget effects below 0.0050 (%w/w)2. This study demonstrate that the sampler device may handle throughputs up to 45 kg/h, without significantly affecting the physical properties of powder blends.",42,1.0
"For oral administration of hydrophobic drugs, a novel oral drug delivery system based on emulsion has been widely implemented, where hydrophobic drug is encapsulated in the hydrophobic core. However, emulsion in liquid state is lack of long-term stability under the action of Ostwald ripening, coagulation, creaming and cracking. This can be avoided by the transformation from liquid emulsion to solid particles and an effective technology is necessary. In this work, liquid emulsion was first prepared with ultrasound where ethylcellulose was as oil phase matrix and gum arabic (GA) as emulsifier and wall material. The oil-in-water emulsion formulation was then processed by an improved supercritical assisted atomization technique (SAA-HCM) and curcumin-loaded microparticles were successfully fabricated. Influences of operation conditions including mixer pressure, temperature, gas/liquid mass flow ratio on diameters and size distributions of microparticles were investigated in detail, and particle formation mechanism was discussed. It is verified that fast extraction of organic solvent from oil phase by supercritical CO2 in mixer is favorable to entrap curcumin in the matrix of oil phase preventing drug leaching at stage of atomization, and high drug loading efficiency (53.91±1.6%) was obtained when the GA concentration was up to 20 g/L. CLSM indicated that the shell of curcumin-loaded microparticles was composed of GA and nanoparticles dried from emulsion droplets were enclosed in the inner. The chemical stability of curcumin loaded in microparticles was identified by FI-IR, and the amorphous form of curcumin attributed to fast formation of drug-loaded particles was confirmed by DSC and X-ray. In vitro drug release behavior of microparticles exhibited controlled release and enzyme trigger properties. SAA-HCM is a green micronization technique, and this work will expand the application of SAA-HCM technique to emulsion, which will be of great progress in the design of drug-loaded microparticles with elaborate structures.",42,2.0
"Exosomes have great potential to deliver drugs due to its low antigenicity. However, low yield and lack of targeting capability have hindered its application. This study aimed at creating a biomanufacturing platform of cancer-targeted exosomes. First, we created a scalable, high-productivity, cell line-based monoclonal antibody tagged exosomes (mAb-Exo) production process, and characterized the product with transmission electron microscopy, NanoSight and Western blotting. Second, we confirmed the cancer-specific targeting using flow cytometry and confocal laser scanning microscopy and using live animal imaging system. Finally, we loaded Romidepsin, a histone deacetylase inhibitor, in mAb-Exo and validated the targeted drug delivery capability in tumor xenograft animal model by testing the cancer targeting specificity and anti-cancer efficacy. We developed new technique enabling targeted drug delivery.",42,3.0
"Microstructures play an increasingly important role in drug formulation design and product development. The physical properties of API domains, excipients, and pores, dictate both manufacturability and performance of a final drug product. Furthermore, to achieve specific drug release patterns and rates, functional coatings and performance-enabling excipients have become increasingly common.This poster will introduce a novel platform for evaluating the roles of microstructures from micro-images. The micro-scale nature of components and their interplay in a drug product demand a high-resolution technique to evaluate their role in performance. Three-dimensional micro-imaging, including X-Ray Microscopy (XRM) and Focused Ion Beam Scanning Electron Microscope (FIB-SEM), can be used separately and/or correlatively to qualitatively visualize these microstructures.What’s more, the spatial and chemical distributions, as well as image-based modeling of release behaviors, can be assessed quantitatively and semi-quantitatively. The performance impact of microstructures on a PLGA long-acting formulation and spray-dried particles, including performance relation to process and manufacturing conditions, will be used to illustrate this micro-image based platform.Image-based characterization can be utilized in drug product development for a fundamental understanding of the process-property-performance interplay, optimizing formulation process and design, and increasing manufacturability. The combination of micro-imaging, AI-based image analytics, and image-based modeling presents a new platform for advancing drug design and evaluation, with significantly reduced evaluation time, improved drug performance, and lowered costs of in vitro and in vivo experiments.",42,4.0
"Speed is critical in the research and development of drug products and processes. The challenge here is the existence of various process alternatives and product characteristics in tablet manufacturing processes, which makes the decision-makings complicated. As for the process alternatives, the choice of dry or wet granulation is known to affect product quality significantly, while this decision is usually conducted without rigorous comparison. As for the product characteristics, solubility of active pharmaceutical ingredients (APIs) depends on the product, and influences the dissolution behavior of tablets largely. In the literature, first-principle as well as statistical modeling approaches has been researched for tablet manufacturing (e.g., [1] and [2]) for assisting more efficient process design. The power of the first-principle modeling is recognized so well that commercial process simulators, e.g, gPROMS, are available for tablet manufacturing processes. However, there is still a hurdle of computational time to use first-principle models in the actual decision-makings. This work presents surrogate modeling of dissolution behavior for enhancing the use of first-principle modeling in the design of tablet manufacturing processes. Our research approach consists of the following three steps: (i) collect process input and output data using gPROMS, (ii) perform surrogate modeling, and (iii) test the developed model. In the first step, 200 data are collected per each manufacturing method by simulating flowsheet models. Secondly, random forests are used for developing surrogate models from collected data sets after fitting Weibull models for dissolution behaviors. Finally, in the third step, sensitivity analysis is performed using the developed surrogate model to generate a proposal for the efficient process design. Case studies of dry granulation and wet granulation methods were performed, where paracetamol was used as an API of the tablets. The simulation results showed the differences in tablet properties between manufacturing methods, e.g., hardness, porosity, and dissolution. After the execution of surrogate modeling, the sensitivity of input parameters was assessed using rank correlation coefficients, which determined critical parameters in the process, and product design phases. In ongoing work, the generality and the effectiveness of the developed models are discussed by performing case studies using a different API. [1] D. Van Hauwermeiren, M. Verstraeten, P. Doshi, M. T. am Ende, N. Turnbull, K. Lee, T. De Beer, I. Nopens, Powder Technol, 341, 116–125 (2019)[2] K. Matsunami, T. Nagato, K. Hasegawa, H. Sugiyama, Int J Pharm, 579, 119160 (2020)",42,5.0
"The growing recognition of the limitations of batch manufacturing and recent advances in Process Analytical Technology (PAT) have led to the shift towards continuous manufacturing (CM). However, the implementation of continuous manufacturing has its challenges and continues to be an area of intense research interest. Near-infrared (NIR) spectroscopy has been the choice of PAT tool in the pharmaceutical industry for online measurements of powder composition in drug product manufacturing, but it has its limitations in measuring low concentrations. Raman spectroscopy is an alternative vibrational spectroscopy method that has also received attention to monitor the drug concentration in powders and is studied in this work.Important components in implementing the CM strategy are: (1) real-time measurement and monitoring, (2) process control, (3) fault detection, (4) diagnosis of exceptional events, and (5) tracking and isolation of non-compliant product1. This research focuses on the first foundational component – real-time measurement and monitoring – using the spectroscopy-based PAT tools. The measurement of the active pharmaceutical ingredient (API) concentration in low drug load blends has been a known challenge2. Raman spectroscopy can be utilized to predict the API concentration in such low drug load products. Also, it is important to have the right spectroscopic sensor at the right sensor location to achieve the true benefits of the in-situ measurement. The feed frame of the tablet press has been selected as a promising location as it provides various benefits3 and has only been explored in to a limited extent in the literature. The blend composition measured multiple unit operations upstream of tableting may not be truly representative of the API concentration in the final tablets. Locations close to tablet compression provide better API concentration estimates in the final product. Also, real-time measurement of API at the tablet press location can aid in immediate rejection of the out of specification tablets.The present study aims to develop a reliable PAT method using Raman spectroscopy to determine the API concentration in real-time in continuous tablet manufacturing of Low Drug Load Product. This includes checking the model robustness against process conditions, understanding the difference in Raman spectra between static powder sample and moving powder stream inside feed frame, and understanding the issue of fluorescence with some of the ingredient excipients. Previous study has shown the use of Raman spectroscopy in measuring API concentration with 8% API (w/w)4. The second objective is to integrate the Raman sensor in the existing pilot plant facility at Purdue to send the data and results of concentrations to the Distributed Control System (DCS) using proper communication protocols to enable effective statistical process control of the critical quality attributes (CQA) and pave way to develop feedback control. This study investigates the application of Raman spectroscopy for real-time measurement of a continuous tableting process and the feasibility of Raman-based feedback control in the manufacturing of pharmaceutical solid dosage forms.Reference:Moreno M, Liu J, Su Q, et al. Steady-State Data Reconciliation Framework for a Direct Continuous Tableting Line. J Pharm Innov. 2018. doi:10.1007/s12247-018-9354-9Nagy B, Farkas A, Borbás E, Vass P, Nagy ZK, Marosi G. Raman Spectroscopy for Process Analytical Technologies of Pharmaceutical Secondary Manufacturing. AAPS PharmSciTech. 2019;20(1). doi:10.1208/s12249-018-1201-2De Beer T, Burggraeve A, Fonteyne M, Saerens L, Remon JP, Vervaet C. Near infrared and Raman spectroscopy for the in-process monitoring of pharmaceutical production processes. Int J Pharm. 2011;417(1-2):32-47. doi:10.1016/j.ijpharm.2010.12.012Li Y, Anderson CA, Drennen JK, Airiau C, Igne B. Method Development and Validation of an Inline Process Analytical Technology Method for Blend Monitoring in the Tablet Feed Frame Using Raman Spectroscopy. Anal Chem. 2018;90(14):8436-8444. doi:10.1021/acs.analchem.8b01009",43,0.0
"Continuous manufacturing of pharmaceuticals and fine chemicals is attractive due to its small footprint, consistent product quality, and demonstrated benefits from safety, economic, and environmental perspectives.1-4 However, handling solids in research-scale flow reactors creates hurdles, as the solids often lead to reactor channel clogging. To tackle this problem, we present a continuous stirred-tank reactor (CSTR) cascade that can handle slurries/solids during a chemical transformation in flow. Moreover, we implement a mixed-integer nonlinear program (MINLP) algorithm for multi-objective optimization by simultaneously modulating discrete variables (catalyst types) and continuous variables (residence time, temperature, and catalyst loading). The optimization strategy involves a sequential adaptive response surface methodology along with optimal design of experiments and the global search strategy branch and bound.5We demonstrated the autonomous optimization of a Suzuki-Miyaura cross-coupling reaction involving solid substrates and catalysts in an automated flow platform comprising a CSTR cascade, slurries feeding pumps, and an inline HPLC. The hardware control and automation were achieved with an integration of LabVIEWTM,6 MATLAB®,7 and online analysis. This research-scale fully automated flow platform for reaction self-optimization with solids/slurries feeding and handling while consuming a reduced amount of raw materials facilitate identification of optimal reaction conditions for manufacturing process development.References:1. Kevin P. Cole, Brandon J. Reizman et al. (2019) Org. Process Res. Dev. 23, 5, 858-8692. Kevin P. Cole, Brandon J. Reizman et al. (2019) Org. Process Res. Dev. 23, 5, 870-8813. Christopher L. Burcham, Alastair J. Florence, and Martin D. Johnson (2018) Annu. Rev. Chem. Biomol. Eng. 9:253–814. Kevin P. Cole and Martin D. Johnson (2018) Continuous flow technology vs. the batch-by-batch approach to produce pharmaceutical compounds, Expert Review of Clinical Pharmacology, 11:1, 5-135. Lorenz M. Baumgartner, Conner W. Coley, Brandon J. Reizman, Kevin W. Gao, and Klavs F. Jensen (2018) React. Chem. Eng. 3, 301-3011.6. LabVIEWTM is a trademark of National Instruments. This publication is independent of National Instruments, which is not affiliated with the publisher or the author, and does not authorize, sponsor, endorse or otherwise approve this publication.7. MATLAB is a registered trademark of The MathWorks, Inc. See mathworks.com/trademarks for a list of additional trademarks.",43,1.0
"Automation and process control are essential when operating a continuous (flow) process to minimize waste and start up time. Typical operating protocols involve implementing the steady state inputs such as feed concentration and reactor jacket temperature immediately at start-up, then divert the out-of-spec material until the process achieves steady-state. From that point the product is collected. In this study, we implement a model-predictive controller to start up a continuous crystallizer optimally, achieve and maintain steady-state conditions faster than open-loop recipe driven operations. Reactor temperature and solution concentration are the measured variables. The solution concentration is measured using a ReactIR probe calibrated using absorbance as a function of solution concentration. The feed concentration of the inlet stream and the reactor jacket temperature are the manipulated variables. This presentation will compare open-loop and closed-loop controlled concentration profiles in a continuous crystallization for a small molecule API and for a chemical intermediate at Biogen. The closed-loop controller results in steady-state achieved in approximately three residence times, whereas it takes five residence times for the open-loop case to reach steady state. Crystal habit and size are also monitored and show size trends as a function of supersaturation.",43,2.0
"The process model is in the heart of 'digital twin initiative' of the continuous pharmaceutical manufacturing process [1-3]. The process model can be used to optimize the process and design and tune the control system. The feeder is the first and one of the most crucial unit operations of the continuous manufacturing of pharmaceutical products. It directly influences the drug content of the tablets and thereby, patient safety. The feeder needs to be refilled very frequently for continuous manufacturing, and its performance depends significantly on the refill strategy followed. The feeder refill strategy involves three operational parameters that need to be identified before using the feeder for continuous manufacturing. These parameters are ‘feeder mass at which the refill needs to be started,’ ‘refill size,’ and ‘refill rate.’ If the refill strategy is not correctly optimized, then the overall quality of the product will be compromised. Presently, all the settings involved in refill strategies are being experimentally determined using a heuristic approach, which is time and resource-intensive and often leads to suboptimal solution. Therefore, a systematic dynamic optimization framework is needed through which the feeder refill strategy can be optimized.In this work, a systematic framework including the methods and tools has been developed for dynamic optimization of the feeder refill strategies. The feeder unit operation has been modeled using ‘gPROMS’ FormulatedProducts software (PSE),’ and the optimization method has been developed in MATLAB (Mathworks). Both tools are communicating through the gO:MATLAB toolbox (PSE). The deviation of the outlet mass flow of the feeder from the targeted flow rate has been minimized to obtain the optimum value of the feeder refill parameters. Once optimal refill strategy has been obtained then it has been transferred to distributed control system (DCS) to implement it into the continuous pharmaceutical manufacturing pilot-plant [4]. The material properties also affect the refill strategy meaning that the feeder refill strategy need to be frequently optimized if there are any changes in the materials and plant. Therefore, the developed feeder model and dynamic optimization tool can save the time and recourses significantly. The heuristic approach currently used in industry could also lead to suboptimal refill strategy and thereby poor feeder performance and poor product quality. The proposed method can save the time and resources needed for continuous manufacturing as well as can improve the product quality significantly.The objective of this presentation is two-fold; first to highlight the developed systematic framework for model-based dynamic optimization of the feeder refill strategy and then demonstrate its application for continuous pharmaceutical manufacturing process. ReferencesMetta, N., Ghijs, M., Schafer, E., Kumar, A., Cappuyns, P., Assche, I. V., Singh, R., Ramachandran, R. De Beer, T., Ierapetritou, M., Nopens, I. (2019). Dynamic ﬂowsheet model development and sensitivity analysis of a continuous pharmaceutical tablet manufacturing process using the wet granulation route. Processes Journal, 7(4), 234.Singh, R. (2018). System engineering for a novel continuous pharmaceutical manufacturing process. Pharma. Issue 30. https://www.pharmafocusasia.com/manufacturing/system-engineering-pharmaceutical-manufacturing-process.Barros, F. N., Bhaskar, A., Singh, R. (2017). A validated model for design and evaluation of control architectures for continuous tablet compaction process. Processes Journal, 5(4), 76. doi:10.3390/pr5040076Singh, R., Sahay, A., Fernando Muzzio, Ierapetritou, M., Ramachandran, R. (2014). A systematic framework for onsite design and implementation of the control system in continuous tablet manufacturing process. Computers & Chemical Engineering Journal, 66, 186-200. http://dx.doi.org/10.1016/j.compchemeng.2014.02.029",43,3.0
"The creation of a dynamic, real-time, mass balanced based process model for a continuous, direct blend encapsulation line will be discussed. The ability to control the response of a continuous process to deviations in feeder rate is a critical aspect of all oral solid dosage form continuous manufacturing systems. Real time modeling is a robust and flexible approach for this type of control strategy. This type of dynamic modeling can predict concentration changes in the system simultaneously with changes in throughput and hold up mass with as few as two fit parameters.",43,4.0
"Compared to traditional silicon-based solar cells, III-V materials possess several advantageous properties for photovoltaic applications. First, their large light-absorption coefficient allows for thin and lightweight solar cells, thereby enabling portable applications. Second, their conversion efficiency from light to electricity is superior to that of silicon-based materials. Metalorganic vapor-phase epitaxy (MOVPE) [1] is the preferred manufacturing process of III-V materials, as it has been successful at producing highly efficient solar cells [2]. However, this technique induces large manufacturing costs which make III-V materials viable only for specific applications. The large cost is due to the cost of the reactants used, and the rate at which the solar cells grow, which prevents industrial scaling of the technology.An alternative process is the hydride vapor phase epitaxy (HVPE) [3] which has been shown to provide a growth rate two orders of magnitude higher than that of MOVPE, while using cheaper reactants [4]. The process has been recently refined into a dynamic-HVPE (D-HVPE) and has been shown to provide satisfying efficiency [5, 6]. Although promising, the faster growth rate of the solar cells and the cheaper materials used come with new manufacturing challenges that need to be addressed. The overarching objective of this work is to propose a design that will address these challenges and make D-HVPE viable in an industrial context. The specific challenges are described below:􏵍 Thin film solar cells consist several layers, each one made of a different III-V compound. When the growth rate of the solar cell is low enough, the substrate can be kept idle in the same chamber while different reactants are injected over time. With a higher growth rate - such as in HVPE deposition reactors - this strategy can lead to a non-uniform composition in each layer, which is detrimental for the efficiency. To avoid this issue, here, the substrate is moved to a different chamber each time a new layer needs to be grown (see figure enclosed). Ideally, the chambers operate at the same time on a different substrate to maximize the throughput of the reactor. While efficient, this configuration poses a fluid dynamic problem: if the reactants of one chamber escape to a neighbor chamber, it could contaminate the layer grown in the neighbor chamber, thereby hindering the solar cell efficiency. Additionally, the deposition of reactants between chambers can create maintenance issues as the reactants are typically poisonous. The first challenge is to reduce the leak between chambers.􏵍 In HVPEs, the gaseous halides used for the deposition is obtained by mixing HCl with the group-III metal (here, Ga or In) at high temperature. Since the products of the reaction need to be maintained at a high temperature, it is advantageous to have the chemical reaction occur inside the deposition chamber (see figure enclosed), while the chamber is externally heated. This step creates a complex fluid flow in the chamber due to the buoyancy effects. At the same time, reactants should be deposited fast enough to avoid spurious reactions, and should mix fast enough to ensure uniformity of the layer grown on the substrate. The second challenge is to find an injection scheme that will satisfy both conditions.In this work, multiple large-eddy simulations are conducted to better understand the manufacturing challenges posed by D-HVPE deposition reactors, and improve their design. The simulations are run with a variable density low-Mach solver based on OpenFOAM [7]. The results are processed with different sensitivity analysis techniques including active subspaces [8] and Sobol indices [9] to identify the features that affect the most out-of-chamber leak and reactants uniformity on the substrate. The results of the sensitivity analysis are used to propose new designs and optimize the geometry of the deposition reactor. In the end, this work participates in the improvement of D-HVPE deposition reactors and will enable the emergence of portable photovoltaic applications in a variety of sectors.",44,0.0
"Process intensification (PI) aims to dramatically improve manufacturing processes through the application of novel process systems and equipment. The novel approaches can be used to overcome bottlenecks, such as those imposed by thermodynamics, or to combine processing phenomena into fewer processing units with a concurrent reduction of capital and operation and maintenance costs and energy, water and materials intensity. PI approach goes beyond the incremental improvements achieved through optimizing existing equipment and process systems and achieves step changes in energy and materials efficiency, total life-cycle cost reduction, and environmental impact by minimizing wastes at the sources via various hierarchy pollution prevention techniques. Our goal is to modify industrial processes so that services and manufactured goods can be produced without waste. But it is important to understand that some manufacturing processes inherently produce wastes, even after all reasonable efforts at pollution prevention. Thus in some cases the use of a conversion technology may be more appropriate than a program of pollution prevention: many industrial wastes can be processed to render them viable as material inputs to another industry or to part of an industrial cluster of several connected industries - as part of the movement of “industrial ecology”. Two separate case studies are presented that highlight a profitable industrial “by-product-to-new products (Das, 2020).Reference: Das, T.K. (2020). Industrial Environmental Management: Engineering, Science and Policy, Wiley, Hoboken, NJ.",44,1.0
"Yttria-stabilized cubic zirconia (YSZ) is a common electrolyte material for solid oxide fuel cells (SOFC) due to its moderate oxygen-ion conductivity and chemical stability at high temperatures. To maximize ionic conductivity, YSZ ceramics must be near theoretical density which conventionally requires placing the sample in a furnace and slowly increasing the furnace temperature to ~1450°C over a few hours. This requires significant energy use and can adversely affect other layers of the SOFC during manufacturing. Alternatively, flash sintering has been demonstrated to densify YSZ at furnace temperatures as low as 750°C in a few seconds, resulting in cost savings, higher ionic conductivities, and smaller grain sizes as compared to conventional processing. During flash sintering, an electric field is applied across the sample and at a minimum threshold furnace temperature, the sample conductivity and power dissipation rapidly increase causing densification. Typically, small quantities of aluminum oxide (Al2O3) are added to YSZ ceramics by mechanical mixing to alter grain growth behavior and increase the final density. In this work, particle atomic layer deposition (ALD) was used to uniformly coat YSZ particles in thin films of amorphous Al2O3, homogeneously dispersing the Al2O3 prior to flash sintering. Analysis of the flash sintering behavior revealed that the Al2O3 thin film reduced the electrical conductivity of the starting powder more significantly than an equivalent Al2O3 concentration added as particles, enabling adjustment of the onset flash temperature with minimal secondary phase addition. Small quantities of Al­2O­3 increased the sample relative density at a constant sample temperature, suggesting that the Al2O3 acts as a sintering aid and enhances the densification mechanism. This is supported by in situ XRD measurements, where the dissolution of Al2O3 into the YSZ lattice is observed and suggests that Al2O3 changes the chemical environment of the YSZ lattice and grain boundaries to enhance densification. The flash sintering of core/shell powders fabricated by particle ALD results in dense parts with fine grain sizes and has a significant cost/performance advantage over conventional processing.",44,2.0
"Microscale architectures – sought for their enhancement of heat and mass transfer in microscale-based technologies – can be difficult and/or expensive to manufacture with conventional means. Photochemical machining is a standard microscale fabrication method that is limited by low feature aspect ratios and low maximum etch depth. On the other hand, metal additive manufacturing provides a viable route to obtaining high aspect ratio features, but it is not an ideal manufacturing process for device prototyping due to post processing requirements, expensive materials and tools, and developmental infancy. Sinker Electric Discharge Machining (SEDM) is explored here as an inexpensive method for prototyping high aspect ratio features of lamina-plates that comprise a microscale-based device. Two SEDM lamina-plate prototypes are studied here. First, plates of an integrated reactor/heat-exchanger for the direct synthesis of dimethyl-ether (DME) are examined. Second, the flow plate of a lab scale multiphase microchannel separator (MMS) is discussed. In these designs, SEDM enables both scale-up and numbering-up, as well as consistent microscale-feature sizes with high aspect ratios. Physical characterization of the lamina-plates is presented. The limit in feature spacing attenable with SEDM is tested and found to be approximately 400μm.",44,3.0
"Each year, various low volume, high value medications are reported on drug shortage lists with no generic alternative. Here at Purdue University, a small-scale integrated, modular, reconfigurable platform was developed in order to meet the market demand for these drugs. The compound of interest selected for the case study was Lomustine; first produced in 1976, Lomustine is an anti-cancer drug for treating brain tumors and Hodgkin’s lymphoma[1]. In 2014, manufacturing rights changed ownership and Lomustine began to be produced under the brand name of Gleostine. Production of generics is not typically newsworthy; however, over the last few years, the price of the medication increased from $50 to $768 per capsule[2]. The current cost of the drug may prevent patient access to the medication even if it could positively impact their health, highlighting the need of an alternative process for production to lower overall costs.This presentation details the framework for the design and development of a small-scale end-to-end process for continuous manufacturing of Lomustine at a fraction of the current dosage costs. In order to achieve the successful design and development of the Lomustine process, researchers implemented a novel flow synthesis pathway for Lomustine, which was first developed by researchers Jaman et al. at Purdue University[3] within the process. The synthesis consists of two continuous reaction steps, which eliminates the need to handle the Lomustine intermediate, 1-(2-chloroethyl)-3-cyclohexylurea; inherently improving process safety by decreasing intermediate handling and exposure. Additionally, solvent screening, solubility studies, and impurity mapping for the process are reported along with individual unit operation design and construction. Finally, flow synthesis integration with continuous liquid-liquid extraction and crystallization modules showcase an end-to-end continuous synthesis and purification platform capable of producing the anti-cancer drug Lomustine. [1] Lee, F. Y.; Workman, P.; Roberts, J. T.; Bleehen, N. M., Clinical pharmacokinetics of oral CCNU (lomustine). Cancer chemotherapy and pharmacology. 1985, 14 (2), 125-131.[2] Loftus, P. Cancer Drug Price Rises 1,400% With No Generic to Challenge It. The Wall Street Journal [Online], 2017 https://www.wsj.com/articles/cancer-drug-price-rises-1400-with-no-generic-to-challenge-it-1514203201 (accessed Mar 15, 2019).[3] Jaman, Z.; Sobreira, T. J. P.; Mufti, A.; Ferreira, C. R.; Cooks, R. G.; Thompson, D. H., Rapid On-Demand Synthesis of Lomustine under Continuous Flow Conditions. Organic Process Research & Development. 2019, 23 (3), 334-341.",44,4.0
"We present a new microfluidic concept based on complex multiphase systems, which may be switched by temperature to a higher or lower number of fluidic phases. This would allow to fluidically open and close interim reaction compartments - close when reaction is required (= bringing all reaction components together) and open when separation is required (= giving each reaction component at best an own phase). We termed this ‘Green-Spaciant Solvent Factory’ (create spaces = ‘spaciant’) which provides ‘horizontal hierarchy’ – as opposed to the ‘vertical hierarchy’ of common multi-step flow syntheses (or batches) with their many consecutive reactors-separators. This is inspired by nature, as the cell with its internal organelles is a horizontally compartmentalized soft-matter reactor. The vision of the FET-OPEN project ‘ONE-FLOW’ (www.one-flow.org) is that such flow processing suits multi-step cascade reactions, since it ideally just needs one reactor passage for a complex synthesis task.The 2-step cascade from 3-chlorobenzaldehyde to (1R,3S)-1-(3-chloro-phenyl)butane-1,3-diol, well-established by Groeger’s group, [1] is studied as a model system. This is part of a chemoenzymatic cascade combining organo- and biocatalysis, which represents in general an attractive process concept for producing pharmaceutical medicines. In this work, we are exploring to achieve the functionalities of product separation and reactant/catalyst recovery solely by multi-phases with designer solvents. This would allow to reduce complex reaction and separation equipment as well as catalyst modification to one single tube reactor using a standard catalyst (one-flow). Firstly, a large database of conventional organic solvent and/or ionic liquids (ILs) is generated from the COSMOthermX software. Then, the reactivity potential of different families of solvent with the reactants is considered to narrow the screening space. Furthermore, relevant physical properties and the environmental effect of solvents (LCA assessment) is taken into account to screen out the suitable solvents. Afterwards, the top solvent candidates were carefully evaluated by experiments.[2, 3] The whole ‘One-Flow’ solvent screening methodology for aldol reaction is given in Scheme 1 [2].As key result, dodecane as best aldol reaction solvent and two 1-butyl-3-methylimidazolium based ionic liquids as candidate solvents in enzymatic reduction are identified. Up to 92mol% conversion and with (R)-aldol product yield (63 mol%) for aldol reaction is obtained, shown in Figure 1.Meanwhile, we propose the compatibility of nanoreactor into the micro-flow reactor to protect and separate catalysts. Our final goal is to achieve multi-step reactions in one run (ONE-FLOW) in multi-phasic systems using this methodology, i.e. to mimic the metabolic cascades of nature. In Scheme 2, [2] we have envisioned how such Spaciant Solvent Factory for a two-step catalytic reaction may look alike.AcknowledgementsThe research work is supported by FET-Open EU project ONE-FLOW (grant no. 737266). The authors acknowledge the ""Deutsche Forschungsgemeinschaft"" SFB/TRR 63 project InPROMPT and the grant (15120868/218) of the University of Adelaide.References[1] Rulli G, et al. Angewandte Chemie International Edition, 2011, 50(34): 7944-7947.[2] Zhang C, Song Z, Jin C, et al. Chemical Engineering Journal, 2020, 385: 123399.[3] Morales-Gonzalez O M, Zhang C, Li S, et al. Chemical Engineering Science: X, 2019, 3: 100024.",44,5.0
"Lipid-based excipients (LBE) are natural and biodegradable materials that have been extensively used in pharmaceutical dosage forms. The characteristic solid-state transitions of LBE, viz. polymorphism and phase separation, often make their application challenging. These transitions are strongly linked to the limited processability of LBE through certain pharmaceutical manufacturing processes such as spray drying. During spray drying, the crystallization of LBE into metastable polymorphs or phase separation into low-melting-point fractions impair the formation of solid particles. In a previous work of our group novel LBE, polyglycerol esters of fatty acids (PGFAs), were introduced (brand name: Witepsol PMF). The molecular structure and composition of PGFAs hinders polymorphism and provide crystallization into a monophasic system. In this work, the processability of PGFAs via spray drying, in order to produce inhalable lipid microparticles for inhalation, in the final dosage form of dry powder (DPI) is presented. Applications in organic solvent and aqueous-based spray drying processes were evaluated. Distinctive particle attributes of readily inhalable lipid-microparticles were envisaged to provide systemic or local effect after pulmonary administration.Lipid-microparticles loaded with ibuprofen intended for systemic delivery of analgesics were manufactured under an organic solvent-based spray drying process. For effective systemic delivery, large particles (VMD>3 µm) of low density (<0.4 g/cm3) are intended to provide median mass aerodynamic diameter in the range of 1 - 5 µm for reaching lung deposition and diminish lung clearance. PG3-C22 partial ester (WITEPSOL® PMF 123) was used as the particle matrix in a lipid:drug ratio of 70:30. A solution of 1.5% w/w solid content in tetrahydrofuran was sprayed through a 0.2 mm-nozzle at a feed rate of 3 g/min into a closed-loop system at 71°C. The process showed no impairments and resulted in high yield of solid particles (70.1%). Unimodal particle size distribution, volume mean diameter (VMD) of 6.6 ± 1.1 µm and density of 0.389 ±0.007 g/cm3 were achieved. Residual solvent < 50ppm was determined, which is below the limit for pharmaceutical products (<720ppm). The application of the particles as DPI was tested in Aerolizer dry powder inhaler coupled to a next generation impactor. The MMAD of 3.57 ± 0.11 µm evidenced the inhalability of the particles. In vitro lung deposition with high fine particle fraction of 45.2 ± 1.3% was reached. Crystalline dispersion of ibuprofen and PG3-C22 partial ester with eutectic composition was observed in the particles. Dissolution of the deposited particles in simulated lung fluid showed 38.9 ± 6.9% drug released in the first hour and 60.1 ± 0.1% after 6 hours.Lipid-microparticles for local delivery of rifampicin were produced through an aqueous-based spray drying process. Particle attributes were designed for a median particle size < 4 µm (optimal 2.1 µm) to facilitate phagocytosis from alveolar macrophages in the treatment of tuberculosis. An aqueous suspension of drug-loaded PG2-C22 full ester (WITEPSOL® PMF 222) was prepared by melt-emulsification and stabilized with the lipid-based emulsifier IMWITOR® 372P (glyceryl stearate citrate). Lipid-microparticles loaded with 2.5% w/w rifampicin, encapsulation efficiency of 95.2 ± 0.0% and median particle size of 2.11 ± 0.02 µm were achieved. The suspension with 0.5% w/w solid content was spray dried through a 0.4 mm-nozzle at a feed rate of 2 g/min into an open-loop system at 75°C. The process yielded solid particles (83%) without impairments. The produced particles showed a unimodal and sharp particle size distribution (span = 1.8) with a median particle size of 2.53 µm. Crystalline state of the lipids within the particles was determined. Neither polymorphism nor phase separation was observed.The use of PGFAs on organic solvent and aqueous-based spray drying processes led to high yield of drug-loaded solid particles. The systemic and local application of the produced particles is currently under investigation on in vitro cell culture studies. The processability of PGFAs is attributed to the absence of solid-state transitions, especially polymorphism. Conventional LBE crystallize in metastable polymorphs, which increases the degree of supercooling, namely the difference between melting and recrystallization temperature. Although crystallization via spray drying is mostly driven by solvent evaporation, melt-crystallization also occurs in LBE. The high degree of supercooling of conventional LBE delays the formation of solid particles. The absence of polymorphism in PGFAs led to a low degree of supercooling, thus achieving high processability via spray drying. In conclusion, the use of PGFAs is an attractive approach for developing lipid-based engineered particles for diverse applications in the pulmonary field.Acknowledgment: IOI Oleo GmbH, the Austrian Funding Agency (FFG) and the Doctoral Academy NanoGraz, University of Graz.",45,0.0
"Two-piece hard capsules continuously serve as widely used oral drug and dietary supplement dosage forms. Hard gelatin capsules are used in 20% of all oral dosage forms but have numerous shortcomings especially with potential crosslinking with the active ingredient, the growing consumer preference for non-animal products and concerns about bovine spongiform encephalopathy (BSE or mad cow disease). The most common polymer used to replace gelatin in hard capsules is hydroxypropyl methylcellulose (HPMC) or hypromellose. In this study, Propranolol HCl was used to evaluate dissolution behavior and bioequivalence of various capsules especially between gelatin and various types of HPMC capsules.DuPont’s FloVitro TM technology is a flow through approach utilizing a series of solid transfer cells which incorporates drug pharmacokinetic characteristics to simulate a biomimetic system. It is a biorelevant in vitro test that reflects physiological environment in the test conditions with a purpose of correlating in vitro drug release with in vivo drug absorption. FloVitro TM has the unique ability to generate dissolution curves that match plasma profiles directly without the need for mathematical modeling. This study will use Modified USPII and FloVitro TM technology to study dissolution behavior of Propranolol HCl in various commercial gelatin and HPMC capsules.Using USP II method, there was approximately 5 to 6 min delayed release of Propranolol HCl from HPMC capsules without gelling agent, and approximately 2 to 23 min delayed release of Propranolol HCl from HPMC capsules with gelling agent. Using FloVitro TM technology, it was found that the key pharmacokinetic parameters, Cmax and Tmax, of HPMC capsules without gelling agent were not statistically different to that of gelatin capsules. Additionally, HPMC capsules without gelling agent showed more consistent dissolution performance (less coefficient of variation) than that of HPMC capsules with gelling agents (see Table 1).In summary, dissolution of an active in the dosage form is critical for absorption and bioavailability, and for product quality and consistency assessments during drug product life cycle. It was found that the key pharmacokinetic parameters, Cmax and Tmax, of HPMC capsules without gelling agent were not statistically different to that of gelatin capsules using FloVitro TM technology. Using DuPont’s FloVitro TM technology, pharmaceutical scientists can relatively quickly assess biorelevant information on specific APIs and dosage forms. The prediction of in vivo performance of Propranolol HCl in various commercial gelatin and HPMC capsules will help formulation scientists gaining insights on key factors that impact the drug efficacy and safety in the patients.AcknowledgementWe would like to thank Alejandro Carbo, Jonathan Gilinski of CapsCanada for their discussion and support of this report.ReferencesAAPS 2008 Poster Presentation: Exploring Suitability and Feasibility of a Novel In Vitro Dissolution System A. Selen, L. F. Buhse, E. G. Chikhale, W. H. Doub, S. K. De, Z. Gao, A. S. Gehris, L. Hughes, R. Lu, H. Mahayni, P. K. Maturu, T. D. MehtaEddington ND, Rekhi GS, Lesko LJ, Augsburger LL. Scale-Up Effects on Dissolution and Bioavailability of Propranolol Hydrochloride and Metoprolol Tartrate Tablet Formulations. AAPS PharmSciTech. 2000; 1(2): articleAAPS 2010 Poster Presentation: Evaluation of Suitability, Transferability, and Bio-relevance of a Novel In-Vitro Dissolution Technique (Rohm & Haas Dissolution System, i.e. FloVitro™ Technology*) A. Selen, L. F. Buhse, E. G. Chikhale, W. H. Doub, S. K. De, Z. Gao, A. S. Gehris, L. Hughes, R. Lu, H. Mahayni, P. K. Maturu, T. D. MehtaAAPS 2011 Poster Presentation: Application of FloVitro™ Technology to Evaluate Dissolution of Furosemide and Danazol in Simulated Media at Fed and Fasted Conditions A. Selen, W.J. Rodriguez, W. H. Doub, L. F. Buhse, E. G. Chikhale, S. K. De, Z. Gao, A. S. Gehris, L. Hughes, R. Lu, H. Mahayni, P.K. Maturu, T. D. Mehta 13Hughes, Lyn, Dissolution Test Equipment and Method. US Patent No. 2004/US6799123B2, Sept 28th, 2004",45,1.0
"Pharmaceutical tablet coatings are commonly based on hydroxypropyl methyl cellulose (HPMC), poly(vinyl alcohol) (PVOH), and acrylic polymers. Currently available commercial pharmaceutical tablet coating technology includes spraying of a dilute aqueous polymer over the tablets, leading to a time-consuming and high cost drying process. Development of tablet coating technology that minimizes this drying step would be commercially advantageous since it would reduce coating manufacturing time and energy by up to 40%, leading to significant cost savings. The proposed amphoteric latex coating materials contain weak acid and strong base stabilizing moieties on the latex particle surfaces, and exist as low-viscosity dispersions at high solids at mildly acidic pH where the weak acid is not ionized. However, an increase in pH, triggered by the loss of CO2 or addition of base, deprotonates the carboxylic acid group and leads to its ionic linkage with the quaternary ammonium groups also present.1 This ionic coacervation between and among the latex particles enables the coating to set quickly (Figure 1(a)).This study2 focused on the evaluation of amphoteric polyacrylate latex-based film forming materials for tablet coatings. A series of amphoteric latexes was prepared from monomers already used in the synthesis of FDA-approved polyacrylate polymers. Polymer composition was systematically varied in order to understand the effects on polymer film formation temperature, setting efficiency as a function of pH, and film properties such as tack, puncture, gloss, optical clarity, water permeability, etc. Latex films were evaluated using pharmaceutical test methods, and showed equal or superior performance compared to commercially-used cellulosic and poly(vinyl alcohol) coatings. The quick-set efficiency of these latexes was evaluated based on their aggregation behaviors in a series of buffers with pH ranging from 5 to 10. The effect of composition variables on the setting efficiency of latexes was studied. Several performance characteristics for tablet coatings have been measured, including high solids (~40 wt. %), good optical properties (clarity > 90%, transmission > 93%, and haze < 6%), no tackiness, and low permeability (water vapor permeability < 2 x 10-7 g / Pa s m). Mechanical properties of films made from amphoteric latexes are similar to those of PVOH films, but they are softer and more elastic than HPMC films.We have demonstrated that latexes with a balance of quick-setting efficiency and tunable coating properties can be prepared. Amphoteric latex-based tablet coatings can be performance- and cost-competitive to HPMC- and PVOH-based coatings, easier to apply from a high solids formulation, and their composition can be easily adjusted to provide coatings with a desirable balance of properties. The polyacrylate latex films exhibited excellent mechanical and optical properties, high transmission, high clarity, and low haze. The water vapor permeability of latex films was lower than that of current commercial HPMC and PVOH films. These results suggest that amphoteric latexes can be used to prepare tablet coatings with good appearance and desirable performance. Typical coated tablets are shown in Figure 1(b).References: 1. (a) Schmidt, D.; Mussell, R.; Rose, G.  Coat. Technol. 2003, 75, 59-64. (b) Rose, G. et al. Langmuir 2005, 21, 1192-1200. 2. Ladika, M.; Kalantar, T.; Shao, H.; Dean, S.; Harris, K.; Sheskey, P.; Coppens, K.; Balwinski, K.; Holbrook, D.  Appl. Polym. Sci. 2014, 131(7), 539-550.",45,2.0
,45,3.0
"IntroductionHard gelatin capsules have been applied in single dose dry powder inhalers for several decades. Recently, hydroxyl-propyl methylcellulose (HPMC) capsules are increasingly being used in DPI devices due to their favourable properties, i.e. lower moisture content and mechanical characteristics less sensitive to variations in humidity. The capsules are externally lubricated during production to prevent sticking to each other in capsule filling operations and subsequent packaging [1], [2]. In order to emit the powder from the device, capsules are pierced by one or more needles equipped in inhaler device during dose administration manoeuvres [3], [4]. Different lubricants could alter adhesion to the capsule material surface and thus mechanical properties of capsules. This could in turn influence the puncturing properties and subsequently shape and size of the opening and the flap and subsequent powder release. This study attempts to evaluate the puncturing properties of the RS01 Plastiape® device of gelatin and HPMC capsules treated with different external lubricants. Moreover, the goal is to investigate the relationship between the mechanical properties of differently lubricated capsules and their puncture characteristics and to indirectly compare those to the aerodynamic performance.Material and MethodsSize 3 capsules of gelatin and HPMC were obtained from Qualicaps (Qualicaps Europe, S.A.U., Spain). These capsules were externally lubricated with magnesium stearate (MgSt), sodium lauryl sulphate (SLS) and carnauba wax (CW) and were compared to unlubricated capsules (w/o). After a storage period of one week in dry (22% RH) and humid (51% RH) conditions, the capsules (n=6) were punctured using a compression and tensile measuring device (Instron 5943, Instron® GmbH, Germany). The needle from an inhaler (RS01®, Plastitape, Italy) was disassembled from the device and attached to the force transducer. The applied forces (N) were recorded, as previously described by Torrisi et al. [3]. The needle moved with 1mm/sec towards the centre of the capsule lid and the maximum force applied was referred to as the breakthrough force of the capsule. Subsequently, macroscopic pictures of the created openings were taken using a DSLR camera with a reversed 18mm objective (Canon 60D, 0.6sec, ISO500) and the contours were traced using ImageJ software. The number of the pixels in the opening area was calculated and compared to an area of the external diameter of the needle (1mm, 100% of the opening area), resulting in a calculated opening value. Mechanical properties of the conditioned capsules were measured at ambient conditions (22°C ± 2°C, 35% RH ± 3%) using a MCR compact Rheometer (MCR 300, Anton Paar, Austria). The resulting normal force (N) and displacement (mm) values were recorded every 0.06 mm, resulting in the stress-strain curves. The Young modulus was determined by linear regression from the slope of the elastic region of the stress-strain curves. Subsequently, these results were indirectly compared to the release of a powder mixture from the capsules after a filling process. For this purpose, the unconditioned capsules were filled with 25mg of a model blend at the previously mentioned room conditions (22% and 51% RH) with a capsule filling machine (FlexaLab, MG2, Pianoro, Italy) [5]. The model blend consisted of 1% (w/w) Budesonide (Laboratorios Liconsa, Spain) and 99% (w/w) Inhalac 230 (MEGGLE, Germany). The machine settings were maintained for all capsule types and aerodynamic performance was measured via the fast screening impactor (FSI).Results and DiscussionPuncture profiles for gelatin and HPMC capsules were very different and were influenced largely by the storage conditions, which has also been previously described [3], [4]. Gelatin capsules required overall higher piercing forces (average of all capsules: 6.86N ± 1.14N at 22% RH to 5.90N ± 1.20N at 51% RH) than HPMC capsules (3.89N ± 0.65N at 22% RH to 2.99N ± 0.41N at 51% RH). The piercing forces decreased with increased humidity for both types of capsules. Even though there were slight differences in puncture properties between the differently lubricated capsules of the same material, this differences were not significant (data not shown). Our results showed that the small amount of lubricant does not significantly impact puncturing properties (shown in Table 1). This could be explained by the very low amount of lubricant used in the after-treatment of the production (2g lubricants on 10kg of capsules), which gives results in a very small layer in the nanometer scale.Further, taking the size and shape of the openings into account, visible differences between gelatin and HPMC could be noticed. While gelatin capsules have relatively large openings under all conditions (90.8% opening ± 6.2% (average of all capsules)), the position of the flap in HPMC capsules was closing the puncture to a greater extent, resulting in a lower value for the openings (39.5% ± 18.9% (average of all capsules)). Provided that the flap might return towards the capsule material between the time point of the puncturing measurements (Instron) and the time point when the pictures were taken, the size of the opening was calculated without the area covered by the returning flap. HPMC capsules still showed substantially lower opening values than gelatin capsules (77.3% ± 8.5% (average of all capsules/conditions). It is assumed that, when capsules are opened within an inhaler, the flap has substantially less time to return. Also, these experiments were performed in absence of airflow, which is actually the driver of in vitro or in vivo aerosolisation. Therefore, we assume that the opening of HPMC capsules in the actual use within inhaler could be larger as well. Observing further the shape of the opening, it was evident that the edges of the punctures become smoother with increasing storage humidity (Figure 1).The percentage values on Figure 1 show larger openings at lower humidity for all gelatin capsules and most of the HPMC capsules (exception were HPMC capsules lubricated with CW). However, looking into differently lubricated capsules, only few differences in opening sizes may be noticed for both materials. While at 22%RH, HPMC w/o (80.72% ± 8.98%), SLS (85.4% ± 10.41%) and MgSt (78.1% ± 15.38%) showed similarly large openings, HPMC capsules lubricated with CW (61.8% ± 11.75%) resulted in much smaller openings. Differences in the size and shape of the openings among the differently lubricated gelatin capsules were less pronounced. All gelatin capsules stored at 22% RH exhibited similarly large openings, (SLS 95.87% ± 5.09%, CW 95.76% ± 8.42%, w/o 92.26% ± 4.85% and MgSt 88.55% ± 2.99%). At 51% RH storage, opening area seemed to slightly decrease (w/o 91.76% ± 5.17%, SLS 90.17 ± 7.31%, CW 83.99% ± 7.16% and MgSt 87.94% ± 8.40%). The force needed for puncturing correlated well with the elasticity properties of the capsules as observed by Young modulus described previously by authors [6]. Elasticity of gelatin capsules increased with higher storage humidity and is probably the consequence of the increase of the amount of water molecules bound to the helical fragments of proteins in gelatin. The elasticity of HPMC capsules was found to increase at 51% RH, indicating the potential plasticization of the material [6]. Lubricants did not show large effect on the elasticity of both capsule types.The aerodynamic performance of all filled capsules was tested using FSI measurements and was related to the capsule openings Figure 2. It was found that the main difference in API release from capsules was arising from the capsule material, although, there were some differences observed as a consequence of storage and lubrication. In general, gelatin capsules resulted in smaller fine particle dose (FPD) compared to HPMC capsules, suggesting that complete opening of the capsules is not necessarily indicator of the higher FPD. It is possible that due to the modified opening structure on both materials, different air flows are created in the inhaler, resulting in different shear forces directly at the exit point, which influence the detachment of an API. On the other hand structural differences of the different capsule materials on the inner surface could potentially influence the release of finer particles. Interestingly, opening size for HPMC and gelatin capsules did not seem to have that much impact on the FPD (Figure 2). HPMC capsules without lubricants filled at 51% RH and capsules with MgSt and SLS filled at 22% RH achieved a FPD of almost 60µg, whereas the average for gelatin was only about 25µg.ConclusionIn summary, the use of different external lubricants on the surface of capsules could not be directly related to the force required to pierce capsules. However, it showed slight variations in the puncture sizes and shapes. The piercing forces were higher for gelatin than for HPMC capsules and have decreased with increased RH for both materials. The aerodynamic performance of both capsules showed that the larger size of the opening is not necessarily decisive for the higher dose delivery, however, both the capsule material and the storage humidity had an important role on the effective dose delivery from the device.",45,4.0
"Lumefantrine, an anti-malaria drug, suffers from low bioavailability due to its hydrophobic character. Via Flash Nanoprecipitation (FNP), we kinetically trap the drug as a nanoparticle in an amorphous state, which increases solubility and therefore bioavailability. 200-400 nm lumefantrine nanoparticles are produced via FNP using safe and inexpensive stabilizers: hydroxypropylmethylcellulose acetate succinate (HPMCAS), zein protein, and lecithin phospholipid. FNP and spray drying are combined to generate solid powders. These powders are stable under hot and humid conditions. Here we demonstrate the use of FNP and spray drying as a continuous and scalable platform to generate and recover nanoparticles without compromising the dissolution kinetics of the drug. Our process for continuous nanoparticle synthesis and recovery is also inexpensive, which makes it viable for low-income countries. Three scales of mixers were utilized, allowing nanoparticle production rates ranging from a few milligrams up to around 1 kg/day, all with similar nanoparticle size and polydispersity. We compare lyophilized and spray dried lumefantrine NP powders and confirm that using spray drying as our solidification method does not compromise the dissolution kinetics and can therefore be used as a cost effective and scalable drying method. The dissolution kinetics for the spray dried NP powders remain constant under fasted and fed conditions for over a month in accelerated stability testing conditions (50°C, 75% RH, open vial). Via Powder X-ray diffraction, differential scanning calorimetry, and solid-state nuclear magnetic resonance, we confirm that the lumefantrine in the core of the nanoparticle is amorphous. The combination of FNP and spray-drying offers a low-cost, scalable, and continuous nanofabrication platform to produce amorphous nanoparticles in a solid dosage form.",45,5.0
"Introduction/Purpose:Fluidized bed drying is a topic of interest for scientists and engineers because of its complicated physics and its presence in a wide range of industrial processes e.g. pharmaceutical product manufacturing, food processing, wood processing, reduction of iron ore, flue gas cleaning, the roasting of sulfide ores, drying of coal, catalyst industry and many more. Here, we propose a model based on coupled Computational Fluid Dynamics (CFD) and Discrete Element Method (DEM) approach to simulate the drying of granular particles in fluidized bed dryer associated to the drug manufacturing process. The drying process of wet granular particles in a fluidized bed dryer involves momentum, heat and mass transfer between the particles and the drying medium which is inherently a multiphase-multicomponent flow problem.Methods:The model was implemented using an opensource software CFDEM coupling where the fluid phase was solved by OpenFOAM codes and the motion of particles were calculated by LIGGGHTS codes. The information between the fluid phase and particles were exchanged at certain time intervals. The software CFDEM coupling was used because of its flexibility to include new models and its ability to handle large scale systems.Results:The model was validated with the results available in existing literatures. The CFDEM coupling was capable of capturing the physical fluidization phenomena and predicting the fluidization velocity correctly for a packed particle bed. Our model was capable of capturing the heat and momentum transfer between the particles and the drying medium. Present work is going on the modification of the model to include moisture transfer between the particles and drying medium. It was observed that the fluidization of particles significantly improved the performance of the fluidized bed dryer.Conclusions:This validated coupled CFD-DEM model provides a better qualitative and quantitative understanding of the effects of different process parameters on the drying process which can be beneficial for the industry. This model can further help set up reliable scale up, troubleshooting or optimization schemes by greatly replacing the burden of the cost of design of experiments.",46,0.0
"The rheology and hydrodynamics of wet granular mixtures are important in many industrial processes such as pharmaceutical and agricultural production, and petrochemical refining. Heat and mass transfer considerations are also important in key industrial processes involving solvent removal by thermal drying of powders and the product of wet granulation. Continuum simulations involve solving the averaged equations for conservation of mass, momentum, and energy by treating the granular medium as a continuum. In this study, we develop an analytical model for verifying Computational Fluid Dynamics (CFD) simulation results in a cylinder filled with stationary wet granular material. The analytical model solves for the unsteady axisymmetric temperature field during transient heating of the wet granular material to the saturation temperature and the steady axisymmetric temperature field during drying of the wet granular mixture. In order to obtain tractable analytical solutions, it is assumed in the analytical model that there is no mass transfer during the transient heating phase and that all mass transfer occurs at the steady temperature distribution corresponding to the drying stage through a specified sink term in the heat equation. Continuum simulations of heat transfer in the stationary granular bed are performed and the results are verified with those of the analytical model. Further realities of particle drying due to solvent boiling and wet bulb evaporation mechanisms are incorporated into a coupled heat and mass transfer CFD simulation to obtain more realistic results. The assumptions made in the analytical model are also quantitatively examined using the continuum simulations.",46,1.0
"MotivationHot Melt Extrusion (HME) is a continuous manufacturing process used in the pharmaceutical industry to produce drug products with a controlled release profile or with increased bioavailability. Due to the complex flow in Twin Screw Elements (TSE), some parts of the polymer melt reside longer in it then others. This effect is commonly described with Residence Time Distributions (RTD). The RTD can be investigated experimentally for entire Hot Melt Extrusion (HME) processes. However, when kinetic effects (e.g., product formation or degradation) play a role local RTDs are required and these are difficult to obtain experimentally.MethodHere we show that the numerical simulation method Smoothed Particle Hydrodynamics (SPH) is suitable for determining local RTDs in silico. SPH is a lagrangian meshless fluid dynamics method and offers computational advantages for flow simulations with complex moving boundaries.[1–3]ResultsWe compared our computed local RTDs with experimental results. Furthermore, we investigated a variety of geometrically different screw elements and how their local RTDs varied with screw speed, throughput and screw length. Computational tracer experiments were used to visualize the underlying flow mechanism leading to differences in the local RTD.References[1] Eitzlmayr A, Khinast J. Co-rotating twin-screw extruders: Detailed analysis of conveying elements based on smoothed particle hydrodynamics. Part 1: Hydrodynamics. Chem Eng Sci 2015. https://doi.org/10.1016/j.ces.2015.04.055.[2] Eitzlmayr A, Khinast J. Co-rotating twin-screw extruders: Detailed analysis of conveying elements based on smoothed particle hydrodynamics. Part 2: Mixing. Chem Eng Sci 2015;134:880–6. https://doi.org/10.1016/j.ces.2015.05.035.[3] Eitzlmayr A, Matic J, Khinast J. Analysis of Flow and Mixing in Screw Elements of Co-Rotating Twin-Screw Extruders via SPH. vol. 00. 2016. https://doi.org/10.1002/aic.",46,2.0
"Recently the pharmaceutical industry has undergone changes in the way of producing oral solid dosages (tablets) from traditional inefficient and expensive batch production to continuous. Recent advances in the pharmaceutical industry include increased use of twin-screw wet granulation (TSWG) in the manufacturing of solid dosage and application of advanced modeling tools such as Population Balance Models (PBM). The twin-screw wet granulation unit is a unit operation of the ConsiGmaTM-25 continuous powder-to-tablet process line from GEA Pharma Systems. However, improved understanding of the physical process properties within the TSWG, expanding knowledge of the effect of the active pharmaceutical ingredient (API)/formulation on the granulation mechanisms, and the improvement of current PBM models are necessary to successfully operate this continuous production process.As an initial effort to look into process mechanisms present in the twin-screw wet granulator, a unique dataset was collected at different locations inside the granulator and reported in (Verstraeten et al., 2017). Here, the main factor that affects the granule size distribution (GSD) is the liquid-to-solid ratio (L/S). At low L/S ratio, the GSD exhibits bimodality, whereas at high L/S ratio the GSD shows unimodal behavior. From this work, a 1D-PBM compartmental model was developed for predicting the GSD at the outlet of the granulator starting from the pre-blend using aggregation and breakage as the main phenomena that take place in the granulation process. The full model is composed of three PBM models in series (Van Hauwermeiren et al., 2018). One for each zone for which data was gathered: the wetting zone (pre-blend to after the addition of the liquid binder), kneading zone 1 (from after the wetting zone to after the first kneading elements), kneading zone 2 (just before the second kneading elements up until the end of the granulator barrel).Since none of the traditional kernels from the literature could model the bimodal behavior observed from the measurements in the wetting zone, a new aggregation kernel was proposed that was capable to capture different types of behavior at different L/S ratio conditions with one single kernel. The breakage kernel (only used in the kneading zones) is a combination of attrition and uniform breakage (Van Hauwermeiren et al., 2018). This work was conducted for a single formulation, therefore, for general applicability, this needs to be extended by adding the effect of formulation properties. In that way, a true generic PBM can be constructed.In the present work the 1D-PBM compartmental model was calibrated with a new dataset collected for two hydrophobic and two hydrophilic formulations, with different process conditions, at 4 different locations: the wetting zone, kneading zone 1, kneading zone 2, and at the end of the granulator.To attain the desired behavior in the wetting zone, some mathematical modifications were made to the aggregation kernel to accurately predict the location and the size of the two peaks for the bimodal behavior and simultaneously incorporate the influence of the L/S ratio. Further, the model parameters that affect the bimodal behavior were linked to specific formulation properties.The aforementioned changes in the aggregation kernel, constitute improvements to the model reducing the number of parameters to be calibrated from five to two. This parameter reduction makes the model more clear and interpretable. Further, it removes any parameter identifiability issues so that a clear link between model parameters and both process settings as well as formulation properties can be determined.The PBM was calibrated and validated for multiple formulations (with different APIs content) and process settings. These developments allow us to assess the effect of the formulation properties on granulation behavior. For a new formulation, its properties can now be used to make a simulation of the possible GSDs so that with only a limited number of experiments, the optimal range can be determined. This general PBM for TSWG will greatly reduce the amount of material needed for experimentation and it can reduce the time-to-market for a new product.",46,3.0
"Tablet coating is a common unit operation in pharmaceutical manufacturing. Process engineers in the pharmaceutical industry are interested in optimizing the perforated pan coater operation to get low inter-tablet coating variability. In a tablet coater, tablet mixing, spraying and drying occur simultaneously. Tablets are mixed as a result of drum rotation and the action of helical baffles mounted inside the drum. The coating solution is sprayed on the tablet bed using spray nozzles. Hot air is directly supplied to ensure efficient drying of the coated tablets. Performing physical experiments with coaters to determine the optimal set of process parameters is, more often than not, impractical. Computer simulations incorporating the detailed physics of tablet coating offer a viable alternative to experiments.In the current study, a coupled CFD-DEM model is developed to understand the impacts of coater hardware and process parameters on the inter-tablet coating variability in an L.B. Bohle tablet coater (Figure-1). Impact of operational parameters like drum rotation speed (Figure-2), fill level, tablet shape & size, spray rate, spray area (determined by spray gun-to-tablet distance and spray pattern air flow rates), is studied. The model was also used to compare the performance of the coaters from laboratory to industrial scales. The results from the simulation were used to understand which parameter plays a crucial role in determining the coating variability. The model results were also used to evolve guidelines for scale-up.",46,4.0
"Freeze-drying is widely used by pharmaceutical companies for manufacturing materials that are otherwise sensitive to moisture or high temperatures. A freeze-drying cycle is energy intensive since it happens over several days with the primary drying step being the longest. The energy and time required to maintain the desired shelf temperature and support the vacuum for several days at a time contributes towards a significant cost factor. In an optimal freeze-drying cycle, the drying time must be designed to be energy efficient as well as meet the product critical quality attributes (CQAs).This work illustrates process optimization of a previously designed sub-optimal cycle, following a model-based design of experiment approach to minimize the cycle time while meeting the product CQAs. This cycle optimization was unique in that the formulation contained acetic acid and no bulking agent. These atypical formulation components led to a rare freeze-drying cycle optimization opportunity.A mechanistic, non-steady state process model with a moving boundary (sublimation interface) was used in this work [1]. The cycle time was reduced by 50% (~2.5 days). The process map obtained from the model was used to establish the proven acceptable ranges (PARs) on the critical process parameters and identify the design space. The process was then scaled from lab to pilot plant based on the identified design space. This entire process optimization workflow required less than ~20 grams of the active pharmaceutical ingredient (API) in experimentation.References[1] Liapis and Bruttini, 1994, A theory for the primary and secondary drying stages of the freeze-drying of pharmaceutical crystalline and amorphous solutes: comparison between experimental data and theory. Sep. Technol., 4, 144-155",46,5.0
"BackgroundManufacturing particulate products with optimal performance requires tightly controlled processes across several industries such as pharmaceuticals. A key unit operation at the drug substance to drug product interface is crystallisation which is a critical separation and purification technique for the recovery of active pharmaceutical ingredients. It is therefore essential to understand and control crystal properties as they have shown to strongly impact downstream processes and the manufacture of pharmaceutical tablets [1].Spherical agglomeration (post-crystallisation) is an emerging particle formation method which has the capability to improve problematic particle shapes (needles, rods, flakes) and increase particle size [2]. This involves the controlled addition of a partially-miscible solvent termed the bridging liquid to pre-suspended crystals through a single intensified unit operation. The bridging liquid should have a high affinity with the drug to agglomerate the crystals and under optimised conditions, compact and dense spherical agglomerates are produced. As a result, the produced agglomerates display high bulk densities, better flow properties, easier filtration and downstream processing in addition to suitability for direct compression of active pharmaceutical ingredients [3].Process ModellingAnalogous to wet granulation mechanisms, spherical agglomeration exhibits different rate processes: wetting/nucleation of primary crystals by the bridging liquid, consolidation/growth of agglomerate nuclei, and attrition/breakage of the agglomerates through an immersion and distribution mechanism (Figure 1). A recent novel mathematical model was developed to predict kinetics and formation of agglomerate nuclei occurring in an immersion spherical agglomeration process [4]. Importantly, three different regimes for agglomeration nucleation were found; immersion rate limited regime, collision rate limited regime and intermediate regime. The mechanistic model was then successfully integrated along with key material properties and process parameters into a gPROMS FormulatedProducts® (PSE) flowsheet to develop a population balance model framework.The aim of this work is to identify and assess the impact of material attributes, equipment and process parameters on product attributes through population balance modelling. The influence of key input model parameters such as the bridging liquid mean particle, liquid flow rate and addition time on the particle size, size distribution and porosity is investigated. Furthermore, characteristic process trends such as initiation and duration of nucleation are studied. As such, a comprehensive sensitivity analysis will be evaluated for risk mitigation approaches and greater process understanding of the predictive model. For kinetic estimation and model validation, carefully designed small-scale decoupled experiments will be performed to account for the different rate processes in spherical agglomeration.References:Variankaval, N., A.S. Cote, and M.F.J.A.J. Doherty, From form to function: Crystallization of active pharmaceutical ingredients. 2008. 54(7): p. 1682-1688.Pitt, K., et al., Particle design via spherical agglomeration: A critical review of controlling parameters, rate processes and modelling. 2018. 326: p. 327-343.Amaro-González, D. and B.J.P.t. Biscans, Spherical agglomeration during crystallization of an active pharmaceutical ingredient. 2002. 128(2-3): p. 188-194.Arjmandi-Tash, O., et al., A new mathematical model for nucleation of spherical agglomerates by the immersion mechanism. 2019. 4: p. 100048.",47,0.0
"Introduction: Dry Active Pharmaceutical Ingredient (API) auto-agglomeration is a type of unwanted powder transformation that occurs without any additives. If uncontrolled, formation of hard, i.e. capable to withstand drug manufacturing process, API agglomerates may lead to an inadequate API distribution within the final formulation exhibiting the real risk to safety, efficacy and processability of drug products. Both, the fact that API auto-agglomeration may occur at, almost, any stage of drug manufacturing and/or handling processes as well as that a variety of general powder transformation mechanisms have been previously reported (for instance, mechanical, chemical, and triboelectric modes of bulk powder caking) make API auto-agglomeration phenomenon highly complex and challenging to anticipate and to prevent. However, despite the key role it has in the drug product performance and manufacturing, the number of studies addressing auto-agglomeration is currently limited and its underlying root-causes remain largely unknown. As a result, pharmaceutical industry relies mainly on the cost-ineffective trial and error approach when facing issues related to the unwanted API agglomeration.The aim of this paper is to explore the correlation between particle properties, its processing/handling conditions and auto-agglomeration phenomenon as well as to establish a method/tool that would allow anticipation and, hence, better control of API auto-agglomeration leading to enhanced product quality, sustainability and cost-effectiveness. The experimental work was carried out to test the following hypothesis: API surface chemistry impacts its auto-agglomeration tendency. Ibuprofen recrystallised in different solvents was used as the main model API. Its auto-agglomeration tendency was tested using mechanical vibration that corresponds to drug handling conditions. Size distribution and particle shape of ibuprofen were analysed using SEM and G3 morphology, FTIR was used to confirm the identification of ibuprofen structure, and Instron to study the strength of the ibuprofen particles before and after the mechanical vibration.Results and Discussion:Four batches of ibuprofen of distinct morphologies were recrystallised in order to test the effect of crystal surface chemistry on its auto-agglomeration tendency. Collected FTIR data confirmed the ibuprofen structure in each case. SEM and morphology G3 analyses revealed that the shape regularity of recrystallised ibuprofen decreases with decreasing solvent polarity as follows: ibuprofen recrystalised from methanol (IbuMeth) > ibuprofen recrystalised from ethanol (IbuEth) > ibuprofen recrystalised from acetonitrile (IbuAce) > ibuprofen recrystalised from hexane (IbuHex), in line with previously reported data in the literature. Mechanical vibration experiments resulted in formation of visually detectable, relatively soft agglomerates that differ in size and shape across the batches. In addition, morphology G3 results indicated formation of agglomerates at microscale in the case of IbuEth and IbuHex being vibrated: d10, d50, and d90 of recrystallised IbuEth approximately doubled after mechanical vibration, whereas in the case of IbuHex this shift was less considerable. The strength of both IbuEth d90 as recrystallised and after mechanical vibration was tested using Instron revealing that the latter is approximately ¾ of the former. Conclusion:Detection of ibuprofen agglomerates upon mechanical vibration suggests there is a correlation between ibuprofen particle morphology, hence, particle surface chemistry and ibuprofen auto-agglomeration tendency. Further investigations of ibuprofen agglomerates as well as other model APIs agglomerates will include analysis of their strength (Instron, Nanoidenter), internal structure (X-ray micro computed tomography), and their surface chemistry (X-ray photoelectron spectroscopy).",47,1.0
"Pharmaceutical roller compaction (RC) is a particle size enlargement process in which granules are produced by making ribbons from an active or placebo compound and milling them into smaller agglomerates. Determination of the granule size distribution (GSD) as the major product attribute requires extensive experimental efforts which are time consuming and material demanding. Modeling the impacts of process parameters and ribbon properties on granule attributes provides a quantitative insight into the RC granulation process. In this talk, we present a computational approach to simulate the size distribution of granules produced in the milling step of pharmaceutical roller compactors. RC process was emulated by making slugs and milling them with a lab hand-mill apparatus. The model was structured based on the GSD data obtained from different active compounds of Aspirin and Ibuprofen with various drug loads, including two commonly used excipients of Microcrystalline Cellulose PH102 and Lactose FastFlo 316. Experimental data were simulated using a Guassian distribution function, and the determined model parameters were correlated to ribbon Young’s Modulus. Model was validated by data which were excluded from the training set. Results demonstrated the success of the model in predicting the GSD obtained from experimental measurements. This model holds only two fitting parameters which can be determined from regular material profiling techniques. This approach can be integrated with an RC model that can predict the ribbon attributes, and be utilized as a powerful computational tool for RC process space design.",47,2.0
"The role of binders in the formulation of tablets widely varies based on the intended application. Binders, or excipients, account for most drug formulations and provide enhanced properties that aid in drug delivery and bioavailability. Excipients are distinguished by their functionality, efficacy, safety, and qualify. Currently, scientists are seeking more naturally occurring sources (plant-based) to replace common synthetic binders such as polysorbates, povidone, polyvinylpyrrolidone (PVP) and polyethylene glycol (PEG). Natural excipients are considered non-toxic and more biocompatible. Overall, the tablet or granule cohesiveness, stability, manufacturability, and release characteristics will provide an indicator of the feasibility for using a given excipient in industrial-scale manufacturing applications. The purpose of this research is to investigate the influence various natural and semi-synthetic excipients on granule properties.Paracetamol granules were prepared using the previously proven method of binder dropping. All excipients were dissolved into deionized water prior to placement on the powder bed. Analysis of the granules included weight and size analysis using Image-Pro Premier software, friability testing, compression testing, disintegration testing, and dissolution testing. Size measurements included diameter, aspect ratio, and projected area. Dissolution and disintegration tests were performed at a dip rate of 10 DPM in phosphate buffer solution at a pH of 7.0. The concentration of paracetamol in the solution was determined using UV-Vis spectrometry. Friability testing occurred for 100, 150, and 200 rotations. With the exception of PEG, the concentration of the excipient influenced the behavior of the paracetamol granules during dissolution and friability testing. However, the extent of the influence varied between the excipients. For most excipients, the increase in concentration produced larger granules that were more susceptible to breakage during friability testing. The bonding of the excipients determined whether the formed paracetamol granules immediately disintegrated or dissolved over time.",47,3.0
"Twin screw wet granulation (TSWG) is gaining increasing interest in the pharmaceutical industry as an effective continuous manufacturing process for solid oral dosage forms. Although multiple variables have been shown to impact various characteristics of granules produced by this technique, liquid-to-solid ratio is known to be one of the primary factors affecting granule attributes. Hence, identifying an optimum range of liquid-to-solid ratio can help streamline formulation development and minimize material requirements for TSWG process development. Here we present the results of TSWG studies conducted with microcrystalline cellulose-based (MCC) formulations of varying drug loads, using micronized acetaminophen as a model active compound. The blends were granulated using a GEA ConSigma-1 25 mm twin screw wet granulator with a standard kneading element screw configuration at a wide range of liquid-to-solid ratios until pasting was visually determined. It was observed that the paste points for different granulations were dependent on the level of MCC in the formulation. We couple this process-focused analysis with a lab-scale approach to establish viable ranges of liquid-to-solid ratios in advance of actual process development. Comparisons of lab-scale mechanical properties of wet granular material, various process parameters, and material and granule characteristics will be discussed, with guidelines for optimizing liquid addition levels proposed.",47,4.0
"A typical workflow in developing a new product via high shear wet granulation (HSWG) involves experimentally testing every combination of input variables, across 3 to 5 scale of operation. When done using traditional DoE approaches, the experimental effort increases as 2n  where n is the number of process and formulation parameters. This makes HSWG a costly process to formulate a design of experiments in time, money, and materials. Therefore, this work aims to apply a model-driven design workflow to identify the most critical process parameters and thus reduce and better target experiments to be performed.The model-driven design focuses on developing a predictive and well-calibrated process model. The process model is based on a population balance modelling framework. Mechanistic understanding of the rate processes is incorporated through appropriate kernels. The most impactful modelling parameters of these kernels must be identified in addition to the operating parameters for the process. To do this, we adopt a Gaussian Process (GP) surrogate modelling approach to directly interrogate the wet granulation computational model. The GP is trained from an input space of the wet granulation computer model produced from randomly sampling the model parameters, normally distributed about the mean. Predicting the model output using a GP enables a reduction in the considerable computational effort required to analytically calculate the Sobol’ indices for a Global Sensitivity Analysis (GSA).Cross-validation ensures the GP surrogate model is capable of predicting the model outputs given all twenty modelling parameters. Subsequently, calculations of the Sobol’ indices demonstrate that only four of the twenty parameters have sufficient impact to influence the DoE. The most critical process parameter is the liquid spray rate, dominating all four models outputs. The collision coefficient and the critical pore saturation are together the most important modelling parameters with respect to the fine fraction and D50, with each variable ascribing to 20% of the output variance alone and above 60% when including cross-effects. Whereas, for fine fraction the breakage coefficient is the most significant modelling parameter, resulting in a first-order Sobol’ indices of 25% and negligible interactions. Interestingly, the total Sobol’ index of the nuclei-to-drop diameter ratio is 50% making it the most dominant variable for granule porosity but it has little relevance towards the other outputs. Overall, the GSA has identified the critical process parameter and the impactful modelling parameters, and has enabled a proposal of an appropriate experimental design and model calibration workflow. By applying this workflow within model-driven design, industrial development is improved by determining more beneficial production conditions and reducing experimental effort by 30-60% compared to conventional approaches.",47,5.0
"Continuous wet granulation (high shear granulation) process is widely used in the chemical and pharmaceutical industries. This work aims to investigate the product properties and production efficiency of the continuous high shear granulation with experimental studies and simulation using gPROMS. In the continuous high shear granulation process, powder and liquid binder contact in a short resident time, where particles experience nucleation, growth and breakage. This work is also the first time to examine the effects of varying process parameters in a Schugi mixer (continuous high shear granulator). At the meantime, gPROMS is used to simulate the processing efficiency (yield) and the product properties (PSD, porosity, etc.) with the different formulation and process parameters in the granulation process.",47,6.0
"The transport and deposition of fine particulates in turbulent flows play important roles in many engineering and medical systems. Examples include dry powder inhalers for drug delivery and fluidized bed reactors. Micron-sized particles tend to form aggregates due to inter-particle cohesion. The dynamical evolution and morphology of these aggregates involve a complex interplay between turbulent stresses and inter-particle cohesive forces. As a result, particle clumping can arise under various circumstances, which is known to compromise the performance of the aforementioned systems. Of particular interest to the present study is turbulence-induced breakup of fine particulate aggregates. Solid particles with diameters smaller than the Kolmogorov length scale (dp < η) are initially aggregated into a spherical ‘clump’ of diameter D > η and placed in homogeneous isotropic turbulence. Parameters are chosen relevant to powder suspended in air such that cohesion due to van der Waals is important. Simulations are performed using a CFD-DEM framework that models two-way coupling between the fluid and solid phases and resolves particle-particle interactions. Aggregate breakup is investigated for different Adhesion numbers Ad, Taylor micro-scale Reynolds numbers Reλ and nondimensional clump sizes D/dp. The intermittency of turbulence is found to play a key role on the early-stage breakup process, which can be characterized by a turbulent Adhesion number Adη that relates the potential energy of the van der Waals force to turbulent shear stresses. A scaling analysis shows that the time rate of breakup for each case collapses when scaled by Adη and an aggregate Reynolds number proportional to D. A phenomenological model of the breakup process is proposed that acts as a granular counterpart to the Taylor Analogy Breakup (TAB) model commonly used for droplet breakup. Such a model is useful for predicting particle breakup in coarse-grained simulation frameworks, such as Reynolds-averaged Navier–Stokes, where relevant spatial- and temporal-scales are not resolved.",48,0.0
"Particle breakage is relevant to many applications, from pharmaceutical processing to jet printing to defense technologies. Fundamental understanding of particle behavior during and after impact as a function of particle strength and impact velocity is needed for process optimization. In addition, detailed information about the number, shape, size and velocity of resulting fragments is difficult to obtain, particularly at conditions of high impact velocity. Discrete element method (DEM) simulations provides the capability to simulate the physics of particle impact and breakup at the micro-scale. This work explores varying particle breakage behavior upon impact with a flat surface. Impacting particles are modeled as spherical agglomerates consisted of a number of smaller constituent particles held together via a constant adhesive force characterized by a surface energy. Under the influence of a wide range of impact velocities and particle surface energies, five distinct behavioral regimes – rebounding, resting, fragmentation, pancaking, and shattering – are identified. In the rebounding regime, the coefficient of restitution decreases linearly as impact velocity increases. In the fragmentation regime, the rebound velocity generally decreases with increasing fragment size. These trends are consistent with the experimental results obtained in the study of Hassani-Gangaraj et al. (2018). Some regimes are only present at certain velocities while the extent of these regimes varies with particle strength. The type of regime depends largely on how the impact force is transmitted and dissipated through particle-particle contacts within the agglomerate.Hassani-Gangaraj, M.; Veysset, D.; Nelson, K.A.; Schuh, C.A. “Melt-Driven Erosion in Microparticle Impact,” Nature Communications 2018, 9, 1-7",48,1.0
"The breakage of structured particles is important in the food, automotive, electronic, and ceramic industries. Many of the structured particles created consist of relatively uniform or well-graded primary powder particles designed to have minimal segregation that are then made into larger particles that have intricate structures and designs. The tendency for structured particles to break depends on the strength of the bonds formed between the primary particles as well as on the structure of the larger particles. Breakage also depends on the set of forces that act on a structured particle during a breakage event. A population balance model is a reasonable means to describe breakage events and can be used to examine how breakage might occur. However, current formulations of the population balance model do not include the effects of global structure on breakage. They also do not include the effect of particle scale bond strengths, nor do they include any description of the influence of the different types of stress-strain or impact-induced stresses that are present with the material during processing. There is a difference between the breakage mode when the primary influence of breakage events consists of impact events as compared to stress-strain events.The goal of this paper is to look at particle scale strength as a function of particle size distribution and then examine the differences in breakage of structured particles during stress-strain events versus impact events. A population balance model is used to determine the breakage selectivity coefficients. These coefficients are then correlated with the mode of breakage (impact versus stress-strain) using FEM models of stress induced by each type of breakage. However, the breakage rate constants are correlated to the strength as a function of the particle size distribution of the primary particles. Combining the breakage pattern computed by FEM analysis of particles with particle scale strength models and the population balance model provides a rich toolset to understand and predict breakage. A set of structured particles (balls, rods, crosses, and grid-shaped) were created from a consistent set of cohesive particles. These particles were then subjected to stress-strain events and impact events. The breakage studied by the population balance model and FEM analysis of structured particles were used to determine the breakage selectivity behavior. A polydisperse particle scale strength law was used to determine strength as a function of the primary particle size which was correlated to the breakage rate constants.",48,2.0
"Abrasion is a particular form of particle attrition in which a particle changes in shape by becoming smoother over time through the preferential removal of protrusions on the particle’s surface. Fines are produced by abrasion which are much smaller than the particle’s size. Abrasion plays a significant role in industrial applications, as does attrition more broadly. Fines can reduce flowability which can impair processing operations such as conveying, blending or tableting [1]. Attrition affects the bulk density, specific surface area, segregation behaviour and dissolution rate which has major implications for the quality of pharmaceutical products [2, 3]. The rehydration characteristics of infant formula are significantly affected by attrition [4]. Abrasion of fluid cracking catalysts is a significant cost factor in these processes as it necessitates the periodic replacement of the catalysts [5]. Because of its importance to a wide range of industrial particulate processes, quantifying abrasion, and ultimately understanding abrasion in order to better control it, are of fundamental importance. DEM simulations have contributed greatly to our understanding of granular materials behaviour in recent years [6], as they give access to particle-scale information such as interparticle contact forces which are difficult, if not impossible, to acquire directly from experiments. Unfortunately, most DEM simulations are based on spherical particles, while our everyday life experience shows that abrasion depends heavily on the particle shape: angular particles are more susceptible to surface breakage than rounded ones. Additionally, simulations of abrasion in DEM are very few and are mostly application-based [7].In view of these considerations, a novel approach is presented for modelling non-spherical, abradable particles in DEM. The particle’s shape is described by an expansion in spherical harmonic functions [8], with the high-degree terms mainly responsible for microscopic details of the shape, e.g., the surface texture. Mathematical considerations [9], together with recent experimental results [10], show that higher harmonics, corresponding to the surface texture, are the first to be eroded, while ellipsoidal shapes, found in abundance in nature, take much longer to become spherical, i.e., the sphere is the equilibrium shape [9]. Therefore the abrasion process can be represented through the sequential removal of the highest spherical harmonics (see Fig. 1). During abrasion, mass is lost in the form of fines, and an additional set of scaling factors ensures that the high-degree spherical harmonic expansion, representing the original shape, bounds the expansion at any lower degree: the abraded shape. The sequence of spherical harmonic expansions with decreasing order allows prediction of a particle’s shape evolution during the abrasion process. Finally, this abrasion is related to microscopic wearing laws, based on energy considerations. A comparison between the abraded shapes obtained through the microscopic wearing laws and the predicted shapes is discussed. The implementation of this novel method in the open-source LAMMPS code is presented along with possible sources of performance improvement.[1] A. Lekhal, K. P. Girard, M. A. Brown, S. Kiang, B. J. Glasser, J. G. Khinast (2003), Impact of agitated drying on crystal morphology: KCl-water system, Powder Technol. 132(2-3): 119–130.[2] C. Hare, M. Ghadiri, R. Dennehy (2011), Prediction of attrition in agitated particle beds, Chem. Eng. Sci. 66(20): 4757–4770.[3] X. Hua, J. Curtis, Y. Guo, B. Hancock, W. Ketterhagen, C. Wassgren (2015), The internal loads, moments, and stresses in rod-like particles in a low speed, vertical axis mixer, Chem. Eng. Sci. 134: 581–598.[4] K. J. Hanley, E. P. Byrne, K. Cronin, J. C. Oliveira, J. A. O'Mahony, M. A. Fenelon (2011), Effect of pneumatic conveying parameters on physical quality characteristics of infant formula, J. Food Eng. 106(3): 566 236–244.[5] J. Reppenhagen, J. Werther (2000), Catalyst attrition in cyclones, Powder Technol. 113(1-2): 55–69.[6] G. Lu, J. R. Third, C. R. Müller (2015), Discrete element models for non-spherical particle systems: From theoretical developments to applications, Chem. Eng. Sci. 127: 425–465.[7] P. W. Cleary, R. D. Morrison (2016), Comminution mechanisms, particle shape evolution and collision energy partitioning in tumbling mills, Minerals Eng. 86: 75–95.[8] E. J. Garboczi (2002), Three-dimensional mathematical analysis of particle shape using X-ray tomography and spherical harmonics: Application to aggregates used in concrete, Cem. Concr. Res. 32(10): 1621–1638.[9] J. F. Bloore (1977), The shape of pebbles, Math. Geol. 9: 113–122.[10] I. Deiros Quintanilla, G. Combe, F. Emeriault, C. Voivret, J. Ferellec (2019), X-ray CT analysis of the evolution of ballast grain morphology along a Micro-Deval test: key role of the asperity scale, Granul. Matter 21: 30-1–30-12.",48,3.0
"Both breakage and classification occur within the main chamber of a spiral jet mill. The result is a complex fluid flow field that is difficult to analyse using traditional experimental techniques, whilst underlying mechanisms of particle size reduction remain poorly understood [1, 2]. Using Discrete Element Method (DEM) modelling, coupled with Computational Fluid Dynamics (CFD) the effect of hold-up has on both the fluid and particle phase within the mill. The design of the mill domain is based on the Hosokawa Micron AS-50 spiral jet mill (Runcorn, UK) and five mass loadings were used: 0.4 g, 0.8 g, 1.2 g, 1.6 g & 2.0 g. It was found that as the mass of material held within the bed increases, the average particle velocity decreased, along with the span of particle velocity distribution. As a result, the energy associated with an average collision decreased. It was also found that the bed has its own velocity gradient, and the solid particles travel with a greater velocity towards the surface. The result is that greatest amount of energy transfer takes place where the largest presence of shearing is found. This is located along the bed surface and on the back-face of each jet, as shown in Figure (1). These results are consistent with the experimental work of Kurten and Rumpf [3] and Luczak et al. [4]. Finally, it was found that the fluid velocity surrounding the classifier decreases with increasing particle load. The decrease in fluid velocity is due to damping of the fluid field, as momentum is transferred to stabilise the particle bed. MacDonald, R., et al., The spiral jet mill cut size equation. Powder Technology, 2016. 299: p. 26-40. Bnà, S., et al., Investigation of particle dynamics and classification mechanism in a spiral jet mill through computational fluid dynamics and discrete element methods. Powder Technology, 2020. 364: p. 746-773. Kürten, H. and H. Rumpf, Strömungsverlauf und Zerkleinerungsbedingungen in der Spiralstrahlmuhle. Chemie Ingenieur Technik, 1966. 38(11): p. 1187-1192. Luczak, B., et al., Visualization of flow conditions inside spiral jet mills with different nozzle numbers– Analysis of unloaded and loaded mills and correlation with grinding performance. Powder Technology, 2019. 342: p. 108-117.",48,4.0
"Filtration of particles is influenced by the size-shape distribution of the particles. Therefore, it is necessary to determine how upstream processes affect the size-shape distributions in upstream processes before entering the filter. One such upstream process is particle breakage in stirred vessels such as crystallizers . Experiments on high aspect ratio particles such as paracetamol and urea demonstrate how the size-shape distribution changes with time. This work presents recent developments in modeling fragmentation of high aspect ratio crystals based on the experimental results in stirred suspensions.",48,5.0
"To enhance the current understanding of biomass fracturing mechanism so as to optimize the mechanical preprocessing techniques for biomass size reduction, a discrete element method (DEM) model has been proposed and developed with customized bonding laws designed specifically for biomass materials. The customized bonding laws are able to capture the elasto-plastic deformation within a single biomass particle under compression, bending, and twisting. To better match the experimental observations, two additional nonlinear force-displacement form for normal force contact freedom are proposed as well, i.e., quadratic form and square root form. The fracturing of biomass particles is simulated by allowing the bond to break based on the normal and shear strength criteria. Simulation cases of fracturing tests of loblolly pine cube have been developed and further validated by comparing to the physical experiments. It is shown the elasto-plastic DEM model with a square root force-displacement form can replicate the mechanical failure of pine cubes under shearing with a higher fidelity comparing to the linear elastic bonded-sphere model. By assigning different bonding strengths across the growth ring, the DEM model is able to simulate the anisotropic behavior of pine cubes with different fiber orientations, which is a key factor that should be considered in the biomass processing modeling. Future research will focus on understanding the grinding process of biomass particles for equipment optimization and energy consumption minimization.",48,6.0
"Particles with a high specific surface area usually exhibit desirable functional responses, such as high reactivity, fast dissolution, and good content uniformity. For this reason, various industries usually increase the specific surface area of particles, like pharmaceuticals, pigments, and minerals, using milling processes that decrease the size of particles for downstream processing or end use. However, current milling technologies are both energy-intensive and energy-inefficient [1]. Therefore, designing a milling process that can operate at or near an optimal condition would benefit the overall performance, but finding optimal milling operational conditions by trial and error is costly. For this reason, computer simulations of the process could provide significant operational insights while enhancing understanding of the milling process, thereby aiding in rational process design and optimization at a minimum cost.Several modeling approaches, such as the discrete element method and the population balance model (PBM), have attempted to develop a fundamental understanding of the milling processes. Among these approaches, only the PBM provides a quantitative analysis of the spatio-temporal evolution of the particle size distribution (PSD) during milling. In this theoretical study, we developed a cell-based PBM that takes into account the complex nonlinear breakage kinetics and the degree of mixing in an open-circuit continuous dry mill with a discharge screen, unlike previous studies [2–6]. In the context of dry milling, the proposed mathematical model is the first in literature that accounts for (i) the elusive nonlinear breakage kinetics that originate from the multi-particle interactions, (ii) a non-ideal degree of mixedness, and (iii) particle separation via an ideal discharge screen. Various back-mixing ratios (R) and number of cells (n) were used to emulate different extents of particle mixing in the mill including the idealized cases of perfect mixing and no back-mixing (plug flow). Other simulations were performed to examine the impact of cushioning action of fine particles and the screen size. The simulation results were reported in terms of cumulative PSD at steady-state and during the transient period. Apart from the PSD of the product at the mill outlet, spatial variations of the PSD inside the mill were also examined.For continuous mills without a screen, the cell-based PBM with n = 1 exactly emulated the PBM solution of an idealized perfect mixing scenario, whereas the cell model with n = 60 nearly represented the ideal plug-flow (no back-mixing) scenario, with an increase in n converging to the PBM solution for the plug-flow obtained previously in the literature. These findings demonstrated the consistency of the cell-based PBM. The simulation results also suggest that an increase in the number of cells and/or a decrease in the back-mixing ratio, emulating lower extent of back-mixing, led to finer product. While the cushioning action of finer particles led to coarser product PSD in the absence of a screen, this nonlinear impact becomes less important when a screen is present and especially when the screen opening size is small. This study has demonstrated for the first time in literature that the use of a discharge screen with a fine opening not only allows for passage of the desired fine particles and retention of the relatively coarse particles, but also it reduces the retardation impact of the fines on breakage kinetics, thus ultimately resulting in a finer product PSD. References[1] B.A. Wills, An introduction to the Practical Aspects of Ore Treatment and Mineral, Butterworth-Heinemann, Oxford, 2016.[2] L.G. Austin, P.T. Luckie, D. Wightman, Steady-state simulation of a cement-milling circuit, Int. J. Miner. Process. 2 (1975) 127–150.[3] H. Benzer, L. Ergun, M. Oner, A.J. Lynch, Simulation of open circuit clinker grinding, Miner. Eng. 14 (2001) 701–710.[4] E. Bilgili, B. Scarlett, Numerical simulation of open-circuit continuous mills using a non-linear population balance framework: incorporation of non-first-order effects, Chem. Eng. Technol. 28 (2005) 153–159.[5] O. Genc, S.L. Ergun, A.H. Benzer, The dependence of specific discharge and breakage rate functions on feed size distributions, operational and design parameters of industrial scale multi-compartment cement ball mills, Powder Technol. 239 (2013) 137–146.[6] C. Mihalyko, T. Blickle, B.G. Lakatos, A simulation model for analysis and design of continuous grinding mills, Powder Technol. 97 (1998) 51–58.",48,7.0
"In-situ monitoring of solid-phase composition during crystallization is often accomplished by Raman spectroscopy, but not all compounds in the system of interest are strongly Raman active. We are developing an image-based analysis to detect a second undesired solid phase with in-situ microscopy before other PAT tools detect the appearance of the solid. Our system involves the reactive crystallization of beta-lactam antibiotics, wherein a byproduct of the enzyme-catalyzed reaction can crystallize within the antibiotic slurry.The current target of our research is cephalexin, a critical antibiotic, with a byproduct of phenylglycine; however, several other beta-lactam antibiotics exhibit the same behavior, including ampicillin, cefaclor, and cefazolin. The byproduct phenylglycine is present in solution at concentrations on the order of tens of millimoles per liter with a supersaturation less than 1.5; detecting a concentration decrease of about 10 millimolar (indicating initiation of phenylglycine crystallization) within a mixture of several components at >100 millimolar concentration proved difficult with in-situ liquid-phase tools like ATR-FTIR. Phenylglycine shares many functional groups with the desired product and reactants, making its spectroscopic fingerprint similar to the product and reactants. With in-situ Raman spectroscopy, we were unable to detect low loadings of solid phenylglycine in a cephalexin slurry, either because of overlap with the complex solution of similar functional groups or because of low Raman activity compared to cephalexin. Changes in focused beam reflectance measurement (FBRM) counts and chord length distributions did not show up at low phenylglycine solids loadings, nor did changes in turbidity. The solution pH exhibited the strongest change upon formation of phenylglycine solids, but pH is not a robust measure of the appearance of phenylglycine as many (often benign) system perturbations give similar pH responses. As crystals of beta-lactam antibiotics are (predominately) needle-shaped and phenylglycine crystals are plate- or column-shaped, the difference in shape, as well as backscatter intensity, can be used to differentiate the two classes of particles. Additionally, phenylglycine crystals are known to partition to air-water interfaces, a fact that can be exploited as the crystals tend to clump on bubble surfaces making them easier to identify in images (Hoeben et al. 2009).Image analysis enables features and statistics of in-situ images are extracted and recorded in real time, and we have shown that phenylglycine can be detected from in-situ images. First, objects within the image are identified by edge detection. Then properties of the objects such as size, aspect ratio, pixel intensity, and more, as well as distributions of these properties for every object in each image are used to detect phenylglycine. By tracking these features and statistics, a classifier can be constructed to identify phenylglycine from image data and can be implemented in real-time for monitoring and detection. The statistics can also be recorded as a historical record, which is more feasible than storing the experiment as complete images.Hoeben, M. A., et al. (2009). ""Design of a Counter-Current Interfacial Partitioning Process for the Separation of Ampicillin and Phenylglycine."" Industrial & Engineering Chemistry Research 48(16): 7753-7766.",49,0.0
"The presentation will discuss a case study, where multiple in-silico solubility prediction tools and high throughput automation were utilized to accelerate solubility and morphology screening in binary and ternary solvent systems. Regression-based prediction tools using single solvent solubility data and first-principle models were used to identify potential solvent systems to improve crystallization performance. Over 85 solvent combinations were prepared using an automated platform for solubility measurement and morphology screening, and several synergistic solubility behaviors were observed. Experimental solubility trends conformed well with in-silico predictions, indicating these tools can be a beneficial first screen at identifying synergistic solvents to explore. The presentation will delve into the utility of an integrated approach comprising of the solubility modeling and automated experimental screening platform in the efficient data generation to enable timely crystallization process development.All authors are employees of AbbVie and may own AbbVie stock. AbbVie sponsored and funded the study; contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final publication.",49,1.0
"Crystallization is a key separation process that is used in the agrochemical industry to separate the agrochemical actives from impure solutions. A poorly designed crystallization step can lead to poor yield, low purity, and long filtration time. Traditionally, a series of design of experiments led by the Quality-by-Design (QbD) approach is used to optimize operating profile of the crystallization process (i.e. temperature profile, solvent/antisolvent addition profile, pH profile).1 An exhaustive list of experiments covering all factors is needed to explore the whole design space experimentally. To minimize the number of experiments and personnel exposure to toxic chemicals, Quality-by-Control (QbC) utilizes control strategies implemented for the target critical quality attributes (CQAs) to determine the operating profile of the process.2 For rapid process design, direct design or model free approaches can be used to quickly determine an operating profile of a process leading the system to the desired CQAs. Two direct design approaches, direct nucleation control (DNC) and supersaturation control (SSC), are used in this work to control the crystallization of a model agrochemical compound. Both DNC and SSC utilize process analytical technology (PAT) tools to acquire data and control the process. DNC utilizes closed-loop feedback control approach with particle measurements from focused beam reflectance measurement (FBRM) to generate temperature cycles.3 On the other hand, SSC uses concentration measurement via UV/Vis detector in a closed-loop feedback control approach to control the concentration by manipulating temperature.4In this work, both direct design approaches were implemented to improve the particle shape, length, and filtration time of needle shaped particles. Preliminary results have indicated significant improvement in the overall crystallization-filtration process performance. Using a DNC-based direct design approach, the filtration time was reduced by a factor of four compared to the standard recipe. The improved procedure not only reduces the unit operation cycle times, but also improves the particle shape for better downstream operations (i.e. drying, transport). The analysis of the product crystals using Ultra Performance Liquid Chromatography (UPLC) indicates that the impurity profile of the agrochemical compound was not compromised with the thermocycles. With SSC-based direct design approach, the concentration of the system was controlled via a set point of the absolute supersaturation. The system was able to perform a cooling profile and generate uniform needle particles. The QbC-based direct design approaches were also evaluated in larger scale (5 L) crystallization systems indicating that the feedback control based rapid design approach can lead to fast and robust scale-up of agrochemical crystallization processes.References:(1) Bondi, R. W.; Drennen, J. K. Quality by Design and the Importance of PAT in QbD; Academic Press, 2011; Vol. 10.(2) Su, Q.; Ganesh, S.; Moreno, M.; Bommireddy, Y.; Gonzalez, M.; Reklaitis, G. V.; Nagy, Z. K. A Perspective on Quality-by-Control (QbC) in Pharmaceutical Continuous Manufacturing. Comput. Chem. Eng. 2019, 125, 216–231.(3) Bakar, M. R. A.; Nagy, Z. K.; Saleemi, A. N.; Rielly, C. D. The Impact of Direct Nucleation Control on Crystal Size Distribution in Pharmaceutical Crystallization Processes. Cryst. Growth Des. 2009, 9 (3), 1378–1384.(4) Saleemi, A. N.; Rielly, C. D.; Nagy, Z. K. Comparative Investigation of Supersaturation and Automated Direct Nucleation Control of Crystal Size Distributions Using ATR-UV/Vis Spectroscopy and FBRM. Cryst. Growth Des. 2012, 12 (4), 1792–1807.",49,2.0
"Metal Organic Frameworks (MOFs) are a class of three-dimensional materials comprised of organic and inorganic building unit cells. The self-assembly of metal ions and organic linker through coordinate bonds results in synthesis of infinite and diverse micro-porous structures. High porosity and ultra-high surface to volume ratio of MOFs along with their fascinating chemical and design flexibility have prompt to numerous applications. Despite of the ubiquitous applications of MOFs, great challenges still exist for developing a relatively cheap, safe and readily scalable process that is sought after by industries. Here we have developed a continuous-flow synthesis of MOFs by modifying the design of the previously developed microfluidic mixer to accommodate for the separated nucleation and growth chambers to induce synthesis of the uniform nanosized particles at constant supersaturation. Both chambers have identical designs, yet the growth chamber has a membrane that separates inlets and outlets. Nucleation chamber is initially filled with premixed solutions of the aqueous solutions of the metal ligand and organic ligand while keeping its outlet open. After running the system for few minutes to ensure all the air has been moved out and system is filled with the solution, the outlet is closed, and circulation is started. Using an optical system, the nucleation of the crystals can be observed within the nucleation chamber. By that time, outlet is opened, and nucleated crystals will move to the growth chamber. Crystals growing in the growth chamber are kept inside and extracted for the further analysis. Using this microfluidic platform, parametric screening is performed and the effect of the ratio of metal ligand to the organic linker on the morphology, growth rate and yield of the synthesized MOFs are measured.",49,3.0
"We discovered that the polymorph of a pharmaceutical intermediate transformed in situ during crystallization from solution. Initially, we noted an anomaly in the signal recorded during a crystallization experiment using a Mettler-Toledo Particle Track G400 instrument, an unexplained decrease followed by an increase in the average chord length of the particles. At roughly the same time, we discovered that there is a clear change in the rheology of the suspension. Rapid crystallization early in the process produces a suspension with a yield-stress that flows and mixes poorly. This gradually transforms during subsequent antisolvent addition to a free-flowing suspension without yield stress. We determined that the phenomenon causing both the observed change in particle size and rheology is crystal polymorph transformation accompanied by a dramatic change in habit. Our first indication of the transformation was obtained observing suspension sampled early in the crystallization under an optical microscope. We observed a clear habit change with time. We then further explored this transformation using simultaneous in situ Raman spectroscopy (Kaiser RXN1-785) and microscopy (Mettler-Toledo PVM and EasyViewer). We verified that a polymorph transformation occurs during the antisolvent addition phase of crystallization, accompanied by a change in crystal habit. The initial form grows rapidly as acicular crystals, followed by a gradual transformation to a second, more stable form characterized by roughly equant crystal aggregates. This transformation is responsible for the change in suspension rheology and the observed change in particle size. The dependence of the transformation on process conditions and seeding, as well as the kinetics of the transformation, have been determined. Knowledge of this transformation allowed us to understand the experimental observations and enabled confident scale-up.",49,4.0
"A new framework has been recently introduced to correlate in situ chord length distribution (CLD) measurements and offline particle size distribution (PSD) data (Irizarry et al, 2017) and applied to different particle morphologies (Schoell et al, 2019). In this work we are expanding this modelling scheme to consider multimodal distribution measurements (Irizarry et al, 2020). This work also applies the proposed approach to predict if the particle morphology or distribution of different APIs can be predicted from a CLD signal. To accomplish this a data-driven inference system and a more robust CLD2PSD model was built for the studied pharmaceutical compounds since the differences in optical properties and particle morphology require a unique model for each individual substance. The inference system is trained on PSDs annotated by modality and morphology, and the individual models have been combined with online solute concentration data to allow for the estimation of crystal growth, secondary nucleation and agglomeration kinetics, depending on the supersaturation level encountered during the process. The validity of this approach is studied in terms of classification and prediction error rates on test sets. The main advantage of the proposed approach is that all data is generated online and in situ, thus avoiding large sampling errors. Also, it can operate in areas of high concentrations and on small particle sizes where PVM pictures are more difficult to resolve. ReferencesIrizarry, R. et al., 2017. Data-driven model and model paradigm to predict 1D and 2D particle size distribution from measured chord-length distribution. Chemical Engineering Science, 164, pp. 202-218.Schoell, J. et al., 2019. Determining particle‐size distributions from chord length measurements for different particle morphologies. AIChE Journal, 65, e16560Irizarry, R. et al., 2020. CLD-to-PSD model to predict bimodal distributions and changes in modality and particle morphology. Submitted to Chemical Engineering Science.",49,5.0
"This work utilizes PAT to develop a downstream purification process on an industrial scale for a low yielding synthetic amino acid from a dilute chemical reaction product. Cooling crystallization optimized by use of inline IR measurement was used to remove 90% of the highest impurity paving the way for efficient use of continuous chromatography to remove lower concentration impurities. The steps leading up to the final design of the Downstream Process (DSP) process are outlined in this work.The food grade product, an amino acid (AA) is used globally for animal feed as well as human consumption. Production is targeted to be on several MT/year scale. The process of synthesis of the AA involves several reactions involving organic and inorganic compounds.The resulting reaction product is dilute, over 50% water and contains a higher molar ratio of impurities in the form of organic and inorganics compounds than the product AA. The salts in the reaction product are metal sulfate and sulfite, both at concentrations higher than the AA. Ion exclusion chromatography pulse test results were promising with AA eluting after all impurities. However, scale up to continuous chromatographic separation using Simulated Moving Bed (SMB) chromatography failed due to pressure build up in the columns with the sulfates precipitating out and high ionic load causing osmotic pressure to build up, shattering the resins.To allow the use of SMB chromatography, precipitation of sulfates prior to chromatography was imperative. Various methods of sulfate salt crystallization including evaporative, cooling and antisolvent crystallization were studied. Evaporative crystallization lead to formation of anhydrous sulfates while cooling crystallization forms decahydrate sulftates, removing considerable amounts of solvent in the process. In line ReactIR® was used to determine the end point for cooling crystallization when the sulfate concentration in the ML plateaued. Given the translucent nature of decahydrate crystals Inline particle track FBRM® was not capable of accurate crystal count measurement. Nucleation and growth kinetics were estimated by off line microscopy as well as real time solution concentration measurement from ReactIR. Use of PAT allowed for faster optimization of the salt precipitation step with 90% of sulfates removed prior to SMB. Salt precipitation lead to some product loss which required an additional cake wash step to recover AA and recycle back into the process with the feed. Permeability of decahydrates crystals were considerably better than anhydrous crystals allowing for better wash and product loss reduction in salt precipitation step. A second round of crystallization process is required for purification of the AA in the SMB extract from 95% purity to commercial grade product. Inline particle track FBRM(R) was utilized for kinetic measurements in that process.",49,6.0
"Motivated by market pressure to reduce product costs, regulatory incentive to modernise manufacturing and a growing environmental concern to reduce waste has led the pharmaceutical industry to improve its manufacturing process. The impetus to adapt and modernise the manufacturing process can largely be attributed to the promotion of the Quality by Design paradigm (QbD) and guidance on Process Analytical Technology (PAT) by the FDA in 2004. The promotion of these methods coincided with a time where the cost of computing and data storage had declined, but also with developments in non-invasive online sensor technology, such as Raman and Near-Infrared (NIR) spectroscopy. This culminated into an increase intro the application of multivariate analysis (MVA) and specifically Multivariate Statistical Process Control (MSPC).MSPC exploits the Big Data generated from a complex chemical process such as pharmaceutical production and can be used for process understanding, troubleshooting and on-line fault detection. MSPC tools include multivariant process charts and soft sensors and can be combined with APC methods to improve overall plant efficiency. MSPC aims to use big data to better understand a process, to develop a definition of normal operation and then use that to detect faults on-line. There are three main categories to classify as SPC methods (1) model-based (2) knowledge-based (3) data-based. Data-based methods are often the preferred method for pharmaceutical manufacturing. The advantages of data-driven methods are that they do not require first principals understanding of the process or a unit as they construct a model using input and output data. The most popular data-driven methods are multivariate latent/MVA methods (which include PCA, PLS and their adapted versions). These methods project data into the latent space and work as a feature extraction method and reduce the dataset into a smaller or manageable set that can be used in further applications of fault detection.This paper presents an industrial case study which focused on fault detection towards operational improvements for a key API production, pursued in collaboration with a major multinational UK pharmaceutical company (GSK). The aim of this project is to exploit data availability towards constructing and validating a data-driven model of a specific API production unit of the GSK plant (Montrose, Scotland, UK). The project involves using large data sets of process data (temperatures, partial pressures, flow rates, and stream compositions, from an on-site GCMS analysis station), and implementing advanced MVA methods in MATLAB, in order to diagnose operational patterns and recommend feasible improvements towards reliable API production intensification.LITERATURE REFERENCES[1] E. Tomba et al., “General procedure to aid the development of continuous pharmaceutical processes using multivariate statistical modeling-An industrial case study,” Int. J. Pharm., vol. 444, no. 1–2, pp. 25–39, 2013.[2] T. Kourti, “Application of latent variable methods to process control and multivariate statistical process control in industry,” Int. J. Adapt. Control Signal Process., vol. 19, no. 4, pp. 213–246, 2005.[3] J. F. MacGregor and T. Kourti, “Statistical process control of multivariate processes,” Control Eng. Pract., vol. 3, no. 3, pp. 403–414, 1995.[4] T. Kourti, “Multivariate dynamic data modeling for analysis and statistical process control of batch processes, start-ups and grade transitions,” J. Chemom., vol. 17, no. 1, pp. 93–109, 2003.[5] J. F. MacGregor, P. Nomikos, and T. Kourti, “Multivariate Statistical Process Control of Batch Processes Using PCA and PLS,” IFAC Proc. Vol., vol. 27, no. 2, pp. 523–528, 1994.[6] S. Garcia-Munoz and D. Settell, “Application of multivariate latent variable modeling to pilot-scale spray drying monitoring and fault detection: Monitoring with fundamental knowledge,” Comput. Chem. Eng., vol. 33, no. 12, pp. 2106–2110, 2009.[7] M. Boiret, D. N. Rutledge, N. Gorretta, Y. M. Ginot, and J. M. Roger, “Application of independent component analysis on Raman images of a pharmaceutical drug product: Pure spectra determination and spatial distribution of constituents,” J. Pharm. Biomed. Anal., vol. 90, pp. 78–84, 2014.",50,0.0
"Developed over the past four years, the Kinetic Parameter Estimation Toolkit (KIPET), is an open-source toolbox for the determination of kinetic parameters from a variety of experimental datasets including spectra and concentrations. KIPET seeks to overcome limitations of standard parameter estimation packages by applying a unified optimization framework based on maximum likelihood principles and large-scale nonlinear programming strategies for solving estimation problems with nonlinear differential algebraic equations (DAEs). The software package includes tools for data preprocessing, determination of parameter confidence levels for a variety of problem types, and informative wavelength selection to improve the lack of fit (Schenk et al., 2020). All these features have been implemented in Python with the algebraic modeling package Pyomo. KIPET exploits the flexibility of Pyomo to formulate and discretize the dynamic optimization problems that arise in the parameter estimation algorithms. Solutions of these optimization problems are obtained with the nonlinear solver IPOPT and confidence intervals are obtained through the use of either sIPOPT or a newly developed tool, k_aug.In this talk, we discuss a recently enhanced KIPET package (Short et al., 2020) that considers multiple experiments with potentially different reactants and kinetic models, different dataset sizes with shared or unshared individual species' spectra, leading to fast parameter estimation and confidence intervals based on the NLP sensitivities. In addition, we present a new variance estimation technique based on maximum likelihood derivations for unknown covariances from two sample populations. This approach leads to a straightforward deconvolution of variances between noise in model variables and in spectral measurements. Moreover, we discuss a new estimability analysis approach that systematically determines a ranked subset of kinetic parameters with well-defined confidence intervals (Chen and Biegler, 2020). With nonlinear kinetic models and limited measurements, it is often difficult to correctly estimate all the parameters, due to linear dependence and low correlation among the parameters. A common approach is to estimate a subset of the parameters by fixing the others at reasonable (often literature) values. However, it is often challenging to determine which parameters can be properly estimated. In this talk we present an efficient approach that ranks the estimable parameters, and discards those that cannot be estimated accurately. Based on reduced the reduced Hessian information with simple Gauss-Jordan elimination, the proposed approach leads to fast parameter selection and estimation within a simultaneous collocation framework. This approach becomes much more efficient for large problems than competing approaches based on multiple eigenvalue decompositions (Quaiser and Mönnigmann, 2009). Several case studies with increasing complexity are presented to demonstrate the performance of this proposed approach.W. Chen, L. Biegler, “Reduced Hessian Based Parameter Selection and Estimation with Simultaneous Collocation Approach,” submitted for publication (2020) Schenk, M. Short, J. S. Rodriguez, D. Thierry, L. T. Biegler, S. Garcia-Munoz, W. Chen, “Introducing KIPET: A novel open-source software package for kinetic parameter estimation from experimental datasets including spectra,"" Computers and Chemical Engineering, 134(4), 106716 (2020)Short, L. T. Biegler, S. Garcia-Munoz, W. Chen, ""Estimating Variances and Kinetic Parameters from Spectra Across Multiple Datasets Using KIPET,"" Chemometrics and Intelligent Laboratory Systems, to appear (2020)Quaiser, T., Mönnigmann, M. “Systematic identifiability testing for unambiguous mechanistic modeling-application to JAK-STAT, MAP kinase, and NF-κB signaling pathway models,” BMC Systems Biology, 3,50 (2009).",50,1.0
"We present a novel algorithm for the automatic data analysis of modulated Differential Scanning Calorimetry of thermograms designed to minimize subject matter expert input reduce the man reliability. Differential Scanning Calorimetry (DSC) and mainly modulated DSC (mDSC) analysis are widely used to support the production of amorphous solid dispersions for the identification of kinetic and thermodynamic events such as glass transition, crystallization and melting. However, interpretation of thermograms require careful analysis of subject matter experts (SMEs) which can be time-consuming and thus can be a bottleneck to, e.g., formulation development where large numbers of samples are generated are require prompt analysis for reiteration. Furthermore, often there are elusive phenomena that can sometimes go unnoticed or cause dubious interpretation from different SMEs. The presented Python-based software tool’s computation of the derivatives and the maximums, minimums and inflections of each curve, is the base of the software. It then uses this information to search for events, based on their magnitude. One essential advantage of using an automated software is the ability to use the derivatives of the heat flow curves to improve the precision of the results. The derivatives are essential not only to the detect the exact temperature at which events occur but also to help define their onsets. In order to tackle the inherent noise of the DSC analysis and data acquisition, the proposed tool employs a low pass, Fourier transform-based filter, which enables the use of the thermogram derivatives. We show case studies where the software analysis increases the detection rate of events that can easily be unnoticed by visual assessment due to their magnitude or shape, as they only cause very subtle variations. We also present case studies where smaller events are distorted or confounded with a nearby dominant events. However, following the digital detection of these events a more detailed visual assessment confirms their presence and often indicative of the onset of undesirable events such as phase separation or crystallization. Furthermore, the presented digital analysis tool promotes a faster and objective analysis of generated DSC thermogram data.",50,2.0
"The ability to predict the mechanical properties of compacted powder blends of Active Pharmaceutical Ingredients (API) and excipients solely from those of the individual components can reduce the amount of “trial-and-error” involved in formulation design. Machine Learning (ML) models can reduce model development time and effort with the imperative of adequate historical data. This work evaluates linear and non-linear ML for predicting the Youngs Modulus of directly compressed arbitrary blends of known excipients and API from readily available information about the API. The training data obtained from three BCS Class I APIs and four excipients demonstrate “data-smart” strategies to train ML models efficiently. The results indicate that even simple linear ML model provide reasonable prediction accuracy. The practical benefits of this method and how it compares with other mechanistic models are discussed. Finally, we demonstrate an application of the model to enable Quality-by-Design in drug product development.",50,3.0
"The EIOT method[1] was proposed as an extension to the IOT[2] method as an alternative approach to extracting information from spectroscopic data into interpretable mass fractions of the resolved chemical species in the powder mixture.The method in essence seeks to use Beer-Lambert’s law as the deterministic functional relationship between spectra and mass fractions. The main difference with IOT is that EIOT does not assume the spectrum of the pure components is a measurable quantity; but a quantity that can only be estimated from measurements of known composition. The EIOT method proposes to decompose the measured spectra as the summation of the chemical signal plus the effect of the non-chemical interferences. The chemical signal is given by the summation of the apparent pure spectrum weighted by the mass fraction of each specie; the non-chemical interferences as the summation of the non-chemical interference signals weighed by their respective strength. The method relies on optimization techniques to estimate the apparent pure spectra and the signal of the non-chemical interferences; and when the method is used in real-time, to estimate the mass fractions and the strengths of the non-chemical interferences in each new sample. Since the method was published, a number of enhancements have been done to the method, this talk presents such enhancements that i) the consideration of supervised signatures and ii) the consideration of exclusivity in a set of signatures (i.e. only one in a set is allowed to contribute). Details around the maintenance and long-term ownership of the EIOT method are discussed for its potential inclusion as a central component for a control strategy in drug product manufacture. EIOT (as well as its predecessor IOT) bring a disruptive change in the practice of chemometrics; retiring the use of fixed regression models, to the use of real-time optimization and computer decision-making instead. As such, the requirements and expectations for use and long-term maintenance might have to be revised and adapted. [1] Z. Shi, J. Hermiller, S.G.J.A.J. Muñoz, Estimation of mass‐based composition in powder mixtures using Extended Iterative Optimization Technology (EIOT), (2018).[2] K. Muteki, D.O. Blackwood, B. Maranzano, Y. Zhou, Y.A. Liu, K.R. Leeman, G.L. Reid, Mixture component prediction using iterative optimization technology (calibration-free/minimum approach), Industrial & Engineering Chemistry Research, 52 (2013) 12258-12268.",50,4.0
"Common cross-currently illuminated photochemical reactors, i.e. radially irradiated reactors, lack the energy efficiency to be competitive in industry. The use of co- and counter-current illumination was previously proven to increase reactor performance, but using a less efficient collimated LED.1,2 By studying the use of non-collimated LEDs (illuminates the reactor along its axial axis) for the use in co- and counter-currently illuminated reactors efficiency can potentially be improved (Figure-1).For this study, 4 types of LEDs were used ranging in total viewing angle from 20° to 120°, the reflectivity of the reactor material was varied, the absorbance (specific absorbance and path length) was altered and the tube diameter was investigated. The model results indicate that the optimal light source for achieving efficient light absorption in the reactor was the most collimated LED possible, in this case with a total viewing angle of 20°. The optimal reactor set-up uses the most reflective material preferably aluminum or silver to recuperate diverging light rays for the used wavelength. Furthermore, the wall thickness of the glass reactor should not be excessively large. In this case we saw no performance increase below 1.5 mm reactor thickness. Regarding reagents and absorbance, it is shown to best use a higher concentration and reduce the reactor length as this increases reactor performance under the condition that quantum yield is stable as function of concentration. Via the use of non-collimated light, it was determined that the entrance efficiency can be increased compared to a fully collimated light source, at the cost of reflection losses that increase with path length. Thus, the optimal parameters were determined to utilize co- and countercurrent illumination in the industrial context while keeping efficiency high.ReferencesMeir G, et al.,. J Adv Manuf Process. doi:10.1002/amp2.10044Meir G, et al., J Adv Manuf Process. 2020;(February):1-16. doi:10.1002/amp2.10045",51,0.0
"Photocatalysis is a promising technology to perform oxidation reactions without the use of harmful chemicals based on heavy metals. Despite the interest in the field, no practical example of a photocatalytic reactor in the industry exists. This is mainly due to the lack of synergy between photon and mass transfer in photocatalytic reactor design. A widely investigated design in the past decade is the microreactor. Despite the high volume productivity, microreactors are difficult to illuminate and are not scalable. The illumination and the scalability problem is solved by incorporating multiple microreactors in translucent structures. These structures contain multiple channels which can be coated with catalyst. The large surface area of structured reactors is easy to illuminate and allows to apply thin catalyst coatings while maintaining a sufficiently high catalyst loading.The design flexibility of translucent structured reactors causes a more extensive design procedure than single channel reactors. The number of incorporated channels and the catalyst layer thickness are the important design parameters. Plenty of work in this field is either of experimental nature or based on numerical simulations. Researchers interested in designing their reactors to test particular catalysts or specific chemical systems want design methods or tools which can be easily used without the need for multiple experiments or complex numerical simulations. In this work, a graphical tool and an analytical design equation is proposed based on the diffusion equation in the catalyst layer. The presented procedure allows to predict the optimal catalyst layer thickness and number of structural layers based on initial calibration experiments. This reduces the number of experiments researchers have to perform to obtain an optimal design.",51,1.0
"Chemists are beginning to turn their attention to the development of catalysts whose activity in a given chemical processes can be switched by an external stimulus. To achieve a mild, selectivity-controllable and general C-O, C-C and C-S bond formation, the facile selectivity-switchable functionalization of 1,3-dicarbonyl compounds represents an ideal protocol, yet a significant challenge. We thus focused on developing a bioinspired strategy. In this work, a visible light-mediated a-functionalization of 1,3- dicarbonyl compounds with switchable selectivity induced by disulfide is disclosed for the first time. Upon irradiation with visible light, the metal- and base-free α-hydroxylation and α-hydroxymethylation reactions proceeded smoothly through a disulfide-catalyzed oxidation with air under mild conditions and generated the desired products. The highly tunable selectivity between the hydroxylation (21 examples, up to 98% yield) and hydroxymethylation (20 examples, up to 97% yield) is controlled by simple changes in the stoichiometry of the styrene additive. In contrast, conducting the reaction in darkness prevented the S-S bond from being photoactivated, and the disulfides served as a sulfurization reagent and promoted the α-sulfenylation (15 examples, up to 95% yield). The reaction efficiencies of the hydroxylation and hydroxymethylation could be further improved by using a continuous-flow reactor. The combination of a continuous-flow strategy and enzyme-like switchable catalysts allowed the facile preparation of synthetically useful intermediates and products meanwhile providing a digital control and manipulation of chemical reaction paths. ",51,2.0
"Automated systems for chemical synthesis integrated with computer-aided synthesis planning and algorithm-guided optimization of reaction conditions have helped relieve chemists from routine tasks and accelerated process development [1,2,3]. The synthesis of organic compounds during process development often involves multiple steps with both discrete (e.g., catalyst, solvent) and continuous (e.g., temperature, time) process variables. In continuous flow synthesis where multiple steps can be telescoped, an important question is whether for a given optimization objective (e.g., yield, productivity), the combination of optimal conditions for each step is the same as the globally optimal conditions for the entire sequence. The ability to analyze the process inline between two steps in addition to at the end of a multistep synthesis can provide valuable insight into the effect of process variables on individual steps.Here, we present a robotically reconfigurable flow chemistry platform capable of executing, analyzing, and optimizing multistep reactions. The platform contains a library of process modules that can be placed in any order onto a process stack by the robot for performing reactions, separations, and inline analysis (liquid chromatography–mass spectrometry (LC-MS) and Fourier transform infrared (FT-IR) spectroscopy). Reagents are delivered to the process stack via reconfigurable fluidic connections which the robot can place onto a reagent “switchboard”, enabling switching between multiple reagent candidates. The hardware was coupled to an optimization algorithm that employs optimal design of experiments (DoE) to intelligently navigate the design space with as few experiments as possible, and branch and bound (B&B) to handle discrete variables in addition to continuous variables [4]. The LC-MS and FT-IR modules were utilized simultaneously in multistep syntheses to analyze the process at multiple locations, providing reaction-specific information. Several case studies highlight the platform’s capabilities for chemical synthesis and explore the question of how to find globally optimal reaction conditions for multistep flow synthesis.",52,0.0
"Scalable manufacturing of chiral amines is of major importance in the pharmaceutical industry. Biocatalytic production of these compounds offers key advantages including high enantioselectivity, mild operating conditions, and sustainable catalyst production and disposal. However, enzyme stability and retention pose challenges which must be addressed prior to scale-up, a task frequently accomplished by enzyme immobilization. The authors present a continuous flow packed bed reaction platform with co-immobilized amine dehydrogenase1 (AmDH) and formate dehydrogenase (FDH) on a commercial immobilized metal affinity chromatography resin. AmDHs convert prochiral ketones into chiral amines with the incorporation of ammonia and hydride transfer from NADH. FDH consumes formate to regenerate NADH from NAD+, and generates carbon dioxide in the process. Key reactor parameters including residence time, enzyme loading, and reaction temperature were explored to determine their effects on space time yield and total turnover number. The authors report the first stable implementation of AmDH and FDH in a continuous flow reactor, with an apparent half-life exceeding six days.2 Productivity values in the reactor ranged between 150 g/L/day and 400 g/L/day, depending on the inlet flow rate. The results underscore the importance of the trade-offs between conversion, productivity, and stability and the engineering decisions in which those trade-offs result. As the biocatalytic production of pharmaceutical ingredients matures, more work must be focused on bridging the gap between protein science and process development.REFERENCESBommarius, B. R.; Schurmann, M.; Bommarius, A. S., A novel chimeric amine dehydrogenase shows altered substrate specificity compared to its parent enzymes. Chem Commun (Camb) 2014, 50 (95), 14953-5.Franklin, R. D.; Whitley, J. A.; Caparco, A. A.; Bommarius, B. R.; Champion, J. A.; Bommarius, A., Characterization and optimization of a packed bed reactor with co-immobilized amine dehydrogenase and formate dehydrogenase for the continuous production of chiral amines. Chemical Engineering Journal 2020 (submitted).",52,1.0
"Due to increased concerns over sustainability of chemical processes, significant research has been conducted to minimize the amount of process waste generated. Despite advances in efficiency, selectivity, and conversion, waste management is still a concern for all processes, especially biological. The most effective method for waste disposal would be its conversion into a value-added product through profitable means. Many industries are attempting to achieve this goal, but it is challenging to design a novel catalyst, enzyme, or process. In particular, recent advances in enzymatic process modeling have created an opportunity for small-scale cheese manufactures to convert their waste into value-added products. The manufacturing of cheese results in the formation of lactose rich whey permeate that cannot be easily utilized and thus is often disposed of at cost to manufacturers [1].A recent advance in enzymatic modeling corresponds to a new kinetic model to convert lactose waste into value-added galactooligosaccharide (GOS) products using β-galactosidase [2]. This specific reaction has been studied by several researchers in the past, but each proposed model presented challenges such as mass balance issues and overfitting due to the excessive number of parameters [3]. The proposed kinetic model contains no mass balance errors, while minimizing the number of parameters to accurately depict the process characteristics. Furthermore, this model uniquely distinguishes between purely galactose derived disaccharides, trisaccharides, and tetrasaccharides and those containing a glucose saccharide group. Due to the similarity of glucose and galactose, there are no online measuring techniques that can discern between the specific di, tri, or tetrasaccharides stereoisomers. A High Performance Liquid Chromatography (HPLC) method coupled with an ion-exchange column and refractive index detector can distinguish between glucose and galactose, but has no resolution for other isomers such as disaccharide, trisaccharide, and tetrasaccharide stereoisomers. Without the ability to measure specific component concentrations, state estimation must be used to predict the individual states using the online measurements and the proposed kinetic model.This work addresses the state estimation and advanced control employing the developed enzymatic model for the first time. Unique estimation challenges are posed and addressed, with the specific novelty of using nonlinear state estimation to find stereoisomer concentrations from available online measurements and the nonlinear kinetic model. Both an Extended Kalman Filter (EKF) and a Moving Horizon Estimator (MHE) are analyzed for process implementation. The EKF is an unconstrained estimator that allows for rapid estimation but does not guarantee feasible and optimal results. The MHE is a constrained estimator that guarantees feasibility at the cost of increased computational time [4] [5]. Using these estimation techniques, a Biologically-Inspired Optimal Control Strategy (BIO-CS) algorithm is applied to optimize the feeding of lactose and enzyme into a semi-batch reactor [6][7]. The proposed estimation and advanced control framework allows the feasibility of this process to be examined and provides a guide for other bioprocesses to follow.References:[1] Illanes, A. (2011). Whey Upgrading by Enzyme Biocatalysis. Electronic Journal of Biotechnology.[2] Schultz, G., Alexander, R., Lima, F.V., Giordano, R., & Ribeiro, M. (2019). Kinetic Model for the Enzymatic Synthesis of Galacto-Oligosaccharides: Describing Galactobiose Formation. AIChE Annual Meeting.[3] Vera, C., Guerrero, C., Illanes, A., & Conejeros, R. (2011). A Pseudo Steady-State Model for Galacto-Oligosaccharides Synthesis With ß-Galactosidase From Aspergillus oryzae. Biotechnology and Bioengineering.2270-2279.[4] Campani, G., Ribeiro, M., Zangirolami, T., Lima, F.V. (2019). A Hierarchical State Estimation and Control Framework for Monitoring and Dissolved Oxygen Regulation in Bioprocesses. Bioprocess and Biosystems Engineering.1467-1481.[5] Lima, F.V., Rawlings, J. (2011). Nonlinear Stochastic Modeling to Improve State Estimation in Process Monitoring and Control. Process Systems Engineering.996-1007.[6] Mirlekar, G., Gebreslassie, B., Diwekar, U., Lima, F.V. (2018). Biomimetic Model-Based Advanced Control Strategy Integrated with Multi-Agent Optimization for Nonlinear Chemical Processes. Chemical Engineering Research and Design.229-240.[7] Mirlekar, G., Al-Sinbol, G., Perhinschi, M., Lima, F.V. (2018). A Biologically-Inspired Approach for Adaptive Control of Advanced Energy Systems. Computers and Chemical Engineering.378-390.",53,0.0
"A wide variety of industrial processes utilize monitoring methods in order to ensure that safety and quality is maintained. Conventional model-based methods include dimensionality reduction techniques such as Principal Component Analysis (PCA) are often employed as they are computationally simple, and easy to implement in practice [1]. Unfortunately, these techniques assume that the process data is Gaussian, uncorrelated, and only contain a moderate level of noise. This work will demonstrate how wavelet-based representation of data can be utilized in order to efficiently handle data that violate these assumptions. Statistical hypothesis testing methods such as the generalized likelihood ratio chart have shown promise with respect to fault detection, and thus will be used to enhance monitoring capabilities [2], [3].Additionally, this work will demonstrate the necessity to track deviations in the process models themselves in order to identify process drifts or equipment degradation. When a process operates under control, a controller continuously adjusts the level of manipulated variables in order to ensure that the controlled variables are being maintained within specific limits. Unfortunately, this may be at the expense of increased operating and additional costs. Therefore, this work will utilize a dynamic contour-based algorithm in order to illustrate how equipment degradation can be efficiently tracked in multiple operating regimes [4].For both algorithms, illustrative examples using simulated synthetic data, and real data from different applications will be utilized in order to highlight the effectiveness of the developed algorithms.References[1] I. T. Joliffe, Principal Component Analysis, 2nd ed. New York, NY: Springer-Verlag, 2002.[2] M. R. Reynolds and J. Y. Lou, “An Evaluation of a GLR Control Chart for Monitoring the Process Mean,” J. Qual. Technol., vol. 42, no. 3, pp. 287–310, 2010.[3] M. Z. Sheriff, M. Mansouri, M. N. Karim, H. Nounou, and M. Nounou, “Fault detection using multiscale PCA-based moving window GLRT,” J. Process Control, vol. 54, 2017, doi: 10.1016/j.jprocont.2017.03.004.[4] M. Z. Sheriff, H. Nounou, M. Nounou, and M. N. Karim, “Monitoring process degradation through operating regime based process monitoring,” in AIChE Spring Meeting and Global Congress on Process Safety: Process Control Monitoring and Analytics, 2019.",53,1.0
"Existing coal fired power plants are increasingly being subjected to significant load-following due to the increased penetration of intermittent renewables to the grid. During load following operations maintaining the boiler environment at the desired conditions is significantly difficult. In particular, higher flue gas temperature than expected and higher concentration of corrosive sulfur species in the flue gas are causing external sulfidic corrosion especially in the waterwall (WW) tubes, which is one of the primary causes for the WW failure [1]. Experimental studies on corrosion rates of Ni-Cr alloys suggest that corrosion rate is a function of alloy composition, temperature, SO2 and O2 concentrations in the flue gas [2-3]. The effect of metal temperature on the corrosion rate is highly nonlinear with a “bell” shaped response between 600oC and 1000oC for most of the alloys used in the WW section. Unfortunately, as of now, there is no online commercial sensor for real-time measurement of corrosion rate due to the high operating temperature and challenging environment in the WW section. Recently a novel type of electrochemical sensor is being developed in-house for in-situ measurement of the corrosion rate [4]. As the operating temperature and concentration of sulfur species greatly vary spatially and temporally in the WW, the corrosion rate can vary significantly with space and time. However, it is infeasible to install such sensors in the entire WW. Motivated by these challenges, an optimal sensor network is designed, and a nonlinear state estimation technique is developed for real-time monitoring of corrosion rate at all desired locations inside the WW.A first-principles dynamic process model of the WW is developed for calculating the spatial and temporal variation in the temperature and concentration of sulfur species. Models are also developed for the corrosion rate. Electrochemical sensor model has been previously developed at West Virginia University [4]. This is a high nonlinear differential algebraic equation (DAE) system. Therefore, an unscented Kalman filter algorithm for DAE system is developed for state estimation, particularly for estimating the corrosion rate given the data from the electrochemical sensor.Typically, process information which can be obtained from a given sensor network and the reliability of sensor network improves as the number and/or the accuracy of the installed sensor increases. However, it is neither feasible nor necessary to install sensors at all desired locations. An optimal sensor network is designed by minimizing the posterior error covariance of the optimal estimator. The algorithm provides the optimal number, location and type of sensors.The optimal sensor network is used to estimate the corrosion rate for a coal-fired boiler. A number of operating scenarios are simulated by changing the temperature in the WW and concentration of the sulfur species. Our work shows that even in the presence of large mismatch between the process and the model and high measurement uncertainty, the sensor network provides satisfactory spatial and temporal resolution of the corrosion rate under aggressive load-following operation.ReferencesNorth American Electric Reliability Council (NERC), “State of Reliability,” 2017.L. Luthra and D. A. Shores, “Mechanism of Na2SO4 Induced Corrosion at 600-900oC,” J. Electrochem Soc., no. 1971, pp. 2202–2210, 1978.Viswanathan and C. J. Spengler, “Corrosion of 85 Ni-15 Cr Alloy at 1600 F in Controlled Atmospheres Containing O2, SO2, SO3, H2S, and N2,” Corrosion, vol. 26, no. 1, pp. 29–41, 1970.N. Aung and X. Liu, “High temperature electrochemical sensor for in situ monitoring of hot corrosion,” Corros. Sci., vol. 65, pp. 1–4, 2012.Kumari, S. K. Das, and P. K. Srivastava, “Modeling fireside corrosion rate in a coal fired boiler using adaptive neural network formalism,” Port. Electrochim. Acta, vol. 34, no. 1, pp. 23–38, 2016.",53,2.0
"Successful applications of the 3D surface land seismic method to monitor CO2 injection into a reservoir have been found in the literature. Reducing the operational effort to acquire 3D data and the turnaround time for the processing and interpretation of field data are highly desirable for more efficient seismic monitoring. The scalable, automated sparse seismic array (SASSA) method has been ideated as a cost-effective alternative or complementary to conventional 3D seismic methods. Based on a sparse array of receivers deployed strategically on Earth’s surface and one or more fixed seismic sources, the SASSA method can be used to study specific portions of the subsurface to detect the movement of CO2 in a reservoir with lower environmental footprint and shorter turnaround time to deliver results than conventional 3D seismic methods.  The Energy & Environmental Research Center (EERC) carried out seismic monitoring of an oilfield undergoing CO2 enhanced oil recovery in Montana, USA, using the SASSA method. Initial efforts in collaboration with the field operator were invested in designing an optimum distribution of two sources and 96 receivers not only to honor the subsurface geologic conditions and CO2 injection patterns but also to minimize the impact of cultural noise sources. The signal-to-noise ratio was further improved in data acquisition by use of two fixed surface orbital vibrators that were remotely controlled by the EERC team in North Dakota. Weekly data acquisition was conducted on weekends from May 2019 through March 2020. Data-processing workflows were developed to preserve the true seismic amplitudes representing the time-lapse changes of the reservoir due to CO2 injection and maximize the representation of the reservoir in the seismic sections from the distribution of sources and receivers on the surface. Encouraging results were obtained using the SASSA method in areas adjacent to the CO2 injection wells. Two validation methods were considered: a time-lapse 2D seismic survey acquired at the beginning and the end of the monitoring activities and reservoir simulation.",53,3.0
"Coronavirus has demonstrated the life and death circumstances related to critical shortages of essential medical items. The U.S. dependence on foreign drug supply has intensified already serious drug shortages, and questions remain over the safety of the drugs we are able to procure. Pandemic-driven drug shortages due to supply chain disruption are ongoing and will only intensify as we struggle with burgeoning hospitalizations and critical care patients.A sophisticated distributed manufacturing platform could be an answer to rectify supply chain disruption. This platform, coined ‘Pharmacy on Demand’ (POD) is a miniaturized manufacturing unit the size of a household refrigerator that features proprietary micro-reactors and continuous flow synthetic chemical processes to make active pharmaceutical ingredients (API) and final formulated medicines. POD can make medicine from American-made chemical inputs to a finished product of a pill or liquid, resulting in complete medicine self-reliance. The flexible and distributed manufacturing capability will mitigate drug shortages during both peacetime and crisis, reducing U.S. dependence on foreign manufacturers.",54,0.0
"Over the last few decades, optimization for cost reductions in production across industry segments has been prioritized over maintaining flexible and resilient supply chains. Off-shoring of pharmaceutical production is an important example of this evolution, and the COVID-19 pandemic has exposed the U.S. population and economy to vulnerabilities created by limited domestic control over strategically important supply chains. The Manufacturing USA institutes are working collectively to craft a different future for critical U.S. supply chains by (1) mapping existing supply chains to understand critical gaps in U.S. manufacturing capabilities and infrastructure, (2) identifying critical feed stock limitations, materials and process development needs, and opportunities to refine current and design new technologies for the optimized system design, and (3) coordinating and managing the work to build a more resilient and flexible manufacturing infrastructure in the U.S. In particular, the RAPID Manufacturing Institute is partnering with its members to understand the current supply chains for several critical medicines and develop flexible, modular process options for cost competitive domestic production of these therapeutics.",54,1.0
"The novel coronavirus has brought significant change to our lives, and our workplaces. While some changes are temporary, the need to work remotely has significantly accelerated the adoption of new technology and business practices that will remain in place long after vaccines are available. Travel restrictions have forced tech-transfer teams to collaborate in new ways, requiring remote support of manufacturing suites, and democratizing decision-making through virtual teams. The need to work from home as put a spotlight on the importance of solid data-management infrastructure and enabled a culture change toward “default-to-open” data security practices. Crises serve to highlight the most critical work, and allow a fresh-eyed re-evaluation of organizational priorities. Change creates opportunity, and major change has created disruptive organizational innovation.",54,2.0
"Dow Life Sciences is a strategic market segment in Dow. The goal of this market segment is to deliver high quality product and technical solutions to Human Health and Human Comfort. There are 5 sub-platforms that we are participating in: API and Manufacturing, Topical Drugs, Oral Drugs, Biopharmaceutical, and medical devices. Dow products sold in this area include Carbowax Sentry PEGs, Isopropanol, defoamers etc. Under this challenging COVID-19 scenario, our Life Sciences global team, quickly responded by identifying unmet needs from the market and customers. Due to the high demand of isopropanol (IPA), the first response was to address the supply issue. This resulted in a capacity expansion of IPA. Meanwhile, we also coordinated the IPA supply among various pharmaceutical customers and continued supporting our strategic customers to launch drug products. In addition to IPA, our global team also captured opportunities related to other Dow products such as amines, chelants, and PEGs to support our global pharma customers in formulated hand sanitizers. Meanwhile, some disruptive opportunities were also identified and investigated due to a great effort and collaboration across Dow multiple functions.",54,3.0
"Product Development a Bristol Myers Squibb has an unwavering dedication to developing innovative medicines that transform patients’ lives. Transferring oral and parenteral drug product manufacturing processes for clinical or commercial manufacturing is an essential deliverable from Product Development to maintain delivery of innovative medicines to patients. In the time of SARS-CoV-2/Covid-19 and our fight to combat the pandemic through social distancing and working remotely, hands on manufacturing processes - particularly when transferring the process between organizations - can introduce risk of viral transmission. Our team has employed augmented reality, remote monitoring protocols, and other digital technology to maximize the ability to collaborate remotely, collect data, and transfer information and know-how while mitigating the risk of viral transmission. Several projects at different stages have been de-risked, troubleshot, and otherwise enabled through application of these innovative digital technologies.",54,4.0
"Innovation is uniquely dependent on supply chains as new vendors and new components need to be designed, built, prototyped, sourced. Impacts can come at us in unexpected ways ranging from government mandated staff reductions that affect productivity, communication hurdles, and challenges when multiple disciplines need to work together on-site but staff limitations preclude collaborative work environments.Flight reductions can result in limited cargo capacity leading to schedule delays and cost escalation. Vendors may no-longer be allowed on site, or if they do require extensive, complex planning to get approval to be on-site to accomplish a previously simple 4 hr service call. Internally the team responsible for implementation innovations can become fractured in their communications, their perception of priorities and goals.Overcoming these challenges become drivers for yet other innovations in how we accomplish work.",54,5.0
"Since its inception in early 2014, the Process Development Intensification Laboratory (iLab) within Small Molecule Process Research & Development at Merck has been focused on the development of tools and techniques to enable acquisition of deeper, more fundamental process understanding in a resource sparing manner. This is predominantly centered around the concept of data intensification, specifically, around enabling more efficient data capture, data reduction and data analysis. Since 2014, we have grown to call these activities Data-Rich Experimentation, or DRE, and these investments have spawned a significantly broader strategic initiative, across large and small molecule Process R&D, which we eloquently call “the DRE initiative”.In this submission, we will look back on the development, demonstration, and perhaps more importantly, the deployment of novel tools designed to increase the information density of the experiments we conduct, and the efficiency of the chemists and engineers who conduct them. We will highlight the installation of novel, enabling technologies such as process modeling, bench-top automation, medium and high-throughput experimentation, process analytical technologies and statistically designed experimentation. Furthermore, this submission will emphasize the importance, as well as demonstrate the utility, of application of such tools across scales typically encountered in a process R&D setting. We will also provide perspective on future directions the team is moving.",55,0.0
"Automation and High Throughput Screening are helping to drive innovation in drug development. Advances in technology and collaboration have opened the door to a wide range of physical and digital automation solutions to produce higher quality data at a much faster pace. Modular Solid and Liquid handlers along with a range of integrated analytical equipment offer both flexibility in experiment design as well as coordination among different users. Solubility and stability screens create robust synthesis and isolation processes on an accelerated timeline. Automated parallel reactor systems allow for controlled pressurized reactions and timed sampling, enabling kinetic reaction screens and Design of Experiment (DoE). Data tools such as Spotfire give options to better organize, analyze, and visualize data, enabling faster recall of information and better decision making. How these technologies have been implemented and the benefits they have on research timelines, data quality, and productivity will be presented.",55,1.0
"Development a Workup for Removing Triphenylphosphine Oxide from a Mitsunobu CouplingEric G. Moschetta, Benoit Cardinal-David, and Moiz DiwanAbbVie, Inc. Process R&D 1401 Sheridan Road North Chicago, IL 60064, USATriphenylphosphine is a common and useful reagent in many organic reactions, such as the Wittig reduction, the Mitsunobu coupling, and the Staudinger reaction, among others. Additionally, triphenylphosphine is used often as a stoichiometric reagent in these reactions, resulting in a stoichiometric amount of triphenylphosphine oxide, the main byproduct of such reactions. Triphenylphosphine oxide (TPPO) is notoriously difficult to remove from reaction mixtures, especially at such large concentrations. Chromatography works to purify reactions in the laboratory, but is impractical for the larger scales required to manufacture drug substance. Extraction is not a viable option, either. TPPO is poorly water soluble and would lead to a solvent inefficient process (nonpolar solvents are common media for Mitsunobu couplings). Process chemists in pharmaceutical development often avoid implementing reactions where TPPO will be a byproduct because of the challenges associated with removing it from the reaction mixture. Bypassing such reactions limits the synthetic tools available for API process development and may lead to the implementation of longer synthetic routes, thereby creating more wasteful processes and requiring more time and effort to develop the process. As such, developing a scalable workup to remove TPPO from such reactions presents a valuable opportunity for pharmaceutical process development to enable reactions where triphenylphosphine is used as a stoichiometric reagent.This presentation describes the development of a procedure to remove TPPO by reacting it with solid magnesium chloride. This protocol was developed on a Mitsunobu coupling in toluene. Magnesium chloride and the resulting magnesium chloride-TPPO complex are virtually insoluble in toluene, creating an intrinsic biphasic system. This presentation will detail the key features of the solid-liquid reaction and the factors that control the rate of reaction as observed with on-line IR spectroscopy. The rate of reaction was observed to be very fast upon addition of solid magnesium chloride, with a sharp decrease in the rate of reaction seconds after addition. Based on this observation, it was hypothesized that employing a surface regenerating technique to expose fresh magnesium chloride for reaction with TPPO would increase the rate of reaction. Including surface regeneration significantly increased the rate of reaction compared to conditions without. The protocol was then implemented on a model Mitsunobu coupling to demonstrate that the workup had no impact on the impurity profile and could effectively remove TPPO from the reaction mixture.Eric Moschetta, Benoit Cardinal-David, and Moiz Diwan are employees of AbbVie and may own AbbVie stock. AbbVie sponsored and funded the study; contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final publication.",55,2.0
"In the last years continuous technologies have emerged in the pharmaceutical industry, but still some steps are not sufficiently developed. One of those technologies is the last step in the active pharmaceutical ingredient (API) production, drying. Suitable, continuous equipment is not available, especially for the case of cohesive, poorly flowable and thermosensitive materials, as many APIs are. Such a technology would be promising for a truly continuous primary manufacturing route and with attractive particle properties. Additionally, the time to market can be reduced significantly, and scale-up and scale-down considerations can be neglected.Drying is an especially challenging process, as it involves simultaneous mass and heat transfer and can have challenging consequences for the particle structure and morphology which eventually control powder behavior. In addition, agglomeration and attrition compete. This is an undesired effect, as during crystallization the particle properties are usually fixed in an optimal way for the intended use of the API. Often, the drying process changes the tailored particle properties, as particle size distribution (PSD) and morphology.This presentation will show the concept of a continuous drying process, using a newly developed dryer design which intends to overcome these challenges by a unique approach of balancing forced feed, powder bed motion and residence time. The results indicate that the particle properties are maintained to a large extent. The influence of several process parameters (mass flow, air flow, rotational speed, temperature, inlet moisture) is initially evaluated, and the product morphology and PSD is investigated. As further improvement to the dryer, a vacuum-tight design was implemented, to enable the dryer to dry off organic solvents and thermosensitive materials. This enables the dryer to continuously operate at the intended scale, in an industrially relevant way. The governing drying kinetics are investigated under vacuum and the drying efficiency and limitations of the systems are discussed.As main test substance Ibuprofen was used, with varying inlet moisture levels (10 to 50 wt. %). It was assured that the technology is able to dry cohesive powders with a small particle size (<100 µm) continuously, in the range of 0.5 to 2.0 kg/h, dry basis. In this range, the PSD was maintained and the residual moistures of the product for some process configurations could be reduced to below 1 wt. %. Additionally, the residence time distribution (RTD) was investigated throughout the process and scanning electron microscopy (SEM) was used to classify the morphology prior and after the drying step.In summary, the presented continuous technology gives a robust drying process, suitable to dry temperature-sensitive cohesive particles at low mass flows under vacuum.",55,3.0
"The pharmaceutical industry is facing a constantly rising demand for drugs of growing complexity and more efficient manufacturing processes. The solubility of pharmaceuticals is a key property during the drug formulation and the subsequent design of the processes involved in the manufacturing of the drug. Common challenges in these manufacturing processes include: the large number of experiments required and the extremely low solubilities of active pharmaceutical ingredients (APIs) in water for which experiments are difficult to perform. Computer-aided approaches provide an attractive alternative to performing time-consuming and costly experiments. In this context, molecular modelling approaches can deliver physical properties predictively are key enabling tools.The SAFT-γ Mie group contribution (GC) equation of state (EoS) [1, 2] is such a predictive thermodynamic modelling technique. In the SAFT-γ Mie framework, molecules are modelled as heteronuclear chains formed from fused spherical segments, which represent the distinct functional groups comprising the molecule. In this framework, it is assumed that the properties of a molecule or a mixture can be determined from the weighted contributions of the functional groups present in the system of interest, with the assumption that the parameters characterizing the functional groups are fully transferable across molecules.We demonstrate the validity of the SAFT-γ Mie EoS in the prediction of thermodynamic properties of ionizable APIs. The pH-solubility profiles and the complete phase diagrams of the APIs in water, outlining the solid-liquid, liquid-liquid and vapour-liquid equilibria for these mixtures, are successfully predicted. It is well known that the bioavailability of a drug is improved by salt formulation, which is the preferred method to enhance the solubility of ionizable drugs. The SAFT-γ Mie EoS accounts for the complex speciation phenomena that take place under pH changes including partially ionised (weak electrolytes) systems. We demonstrate the effect on the pH-solubility profile of ibuprofen of multiple inorganic counter ions including sodium, potassium, lithium, calcium and magnesium. This is followed by studying the impact of organic counter ions, particularly the amines, on the pH-solubility profile of ibuprofen and the selection of its optimal salt form. [1] Papaioannou, V. et al. J. Chem. Phys. 140, (2014).[2] Dufal, S. et al. J. Chem. Eng. Data 59, (2014).",55,4.0
"Design space (DS) is a very important concept in the pharmaceutical industry. The importance of DS definition lies in the assurance of quality, a key goal of Quality by Design (QbD), and in the broadening from an acceptable operating setpoint to a collection of tolerable operating regions [1]. Defining proper limits for the design space is vital to provide manufacturing flexibility and quality assurance. The design space is determined by the uncertainty in the model parameters along with the allowable set of conditions in the process parameter space [2]. Flexibility index is an important measure to evaluate the operability of a design model with uncertainties [3], and it also provides a way to describe the design space; for example, the design space can be approximately regarded as a largest inscribed hyperrectangle or hyperelliptic. In addition, if the nominal conditions are not given, the flexibility index can be extended to the design centering problem [4], which is another important problem of DS definition, where it might be desirable to obtain optimal nominal conditions that maximize the feasible region of operation. The ultimate goal of flexibility analysis is to accurately and explicitly describe the design space, regardless of the convex and nonconvex nature of the design space.Design space description, flexibility index, and design centering are viewed as three different issues in the DS field, and they are commonly formulated as multi-level optimization models. In general, these issues can be handled by numerical computation methods; for example, through the KKT conditions and complementarity conditions, the multi-level optimization models can be transformed into MILP/MINLP models [5]. However, it is well known that it is difficult to find the global solutions of an MINLP model. Moreover, the numerical methods can only estimate the outer envelope of the design space, rather than providing an accurate description, especially for the nonconvex cases. Mathematically, the expressions with variables can be obtained through the symbolic computation methods, which is very different from the numerical methods.As we will show in this presentation, the original design model can be obtained directly in the space of the uncertain parameters through the proposed symbolic computation method; thus, it is unnecessary to solve numerical optimization problems. In order to provide an accurate description of the design space, we propose to reformulate the traditional flexibility analysis problem as an existential quantifier model. Then, we introduce a symbolic computation method, cylindrical algebraic decomposition (CAD) [6, 7] to eliminate the quantifiers and transform the flexibility analysis formulation into a series of explicitly triangular formulas. Each of these formulas represents a subspace of the design space, and the logical combination of them constitutes the complete symbolic representation of the design space projected in the space of the uncertain parameters. We show that with this symbolic computation method, the nonconvex design space can also be derived effectively.Symbolic computation provides a novel way to deal with the characterization of a design space. Moreover, the proposed unified symbolic framework can integrate the three approaches described above for design space definition. These three problems have a similar symbolic model, and the only difference is that the dimension is increased successively, and the corresponding computational complexity is also increased successively. For the design space description problem, through the CAD method, the original high-dimensional feasible space can be projected on the space of the uncertain parameters. For the flexibility index problem, the feasible space can be projected onto the one-dimensional space of flexibility index, and all of the candidate values of flexibility index can be generated at the same time. For the design centering problem, the feasible space can be projected onto the space of the flexibility index and the uncertain parameters, and a rule to locate the final nominal point is developed, which only requires a series of simple numerical operations. In summary, the proposed method for design space has four properties, (1) it is a unified symbolic framework; (2) it can avoid solving numerical optimization problems; (3) it can guarantee finding the optimal flexibility index or nominal points; (4) it can provide explicit algebraic expressions of the design space. A design space may be constructed for single unit operation, multiple unit operations, or for the entire process. However, as the scale of the design model increases, the complexity is greatly increased. For reducing the complexity of symbolic computation, we propose to use the adaptive sampling method to construct the surrogate model and eliminate the equalities of the model. Then, the CAD method can be adapted to obtain the explicit expressions of the design space, and the boundary of the design space can be validated and revised iteratively to satisfy a specified accuracy. Using several examples from the pharmaceutical industry, we illustrate the application of the proposed symbolic computation method to show its novelty and value.Keywords: Design space, flexibility analysis, design centering, cylindrical algebraic decomposition, symbolic computation.Reference:Pramod K, Tahir MA, Charoo NA, et al. Pharmaceutical product development: A quality by design approach. Int J Pharm Investig.2016;6(3):129.Rooney WC, Biegler LT. Optimal process design with model parameter uncertainty and process variability. AIChE J. 2003;49(2):438-449.Swaney RE, Grossmann IE. An index for operational flexibility in chemical process design. Part I: Formulation and theory. AIChE J. 1985;31(4):621-630.Harwood SM, Barton PI. How to solve a design centering problem. Math Methods Oper Res. 2017;86(1):215-254.Grossmann IE, Floudas CA. Active constraint strategy for flexibility analysis in chemical processes. Comput Chem Eng. 1987;11(6):675-693.Collins GE. Quantifier elimination for real closed fields by cylindrical algebraic decomposition. Lec Notes Comp Sci. 1975;33:134-183.Arnon DS, Collins GE, McCallum S. Cylindrical algebraic decomposition I: the basic algorithm. SIAM J Comput. 1984;13:865-877.",56,0.0
"A prominent research focus for systematic process development in the continuous pharmaceutical industry is the implementation of Quality-By-Design [1] (QbD) approach. This approach focuses on detailed system understanding from a mechanistic perspective, using high-fidelity multi-physics simulation tools like computational fluid dynamics (CFD) [2] and discrete element modeling (DEM) [3]. These tools can accurately replicate the process dynamics within a unit operation, providing insight into complex processes, such as powder mixing and flow. However, due to huge computational costs associated with such simulations, direct implementation of these tools is prohibitive for quick process evaluations. On the other hand, flowsheet modeling focuses on empirical, low-order models using simplified representations of process dynamics. Though it provides quick assessment, it fails to capture important process complexities, such as powder mechanics. These underlying differences create a need for a synergistic integration of high-fidelity simulation models for capturing complex process dynamics with predictive flowsheet models for fast evaluations. In this study, the proposed integration strategy is demonstrated for continuous blending unit operation using a sequential hybridization approach [4]. Under the proposed approach, particle-scale information [5,6] from DEM simulations (using Simcenter STAR_CCM+ software) is obtained to develop a data-driven model. This data-driven model is then combined with a flowsheet model (developed in PSE gFORMULATE modeling platform [7]) to provide fast predictions for the entire production line. The combination of DEM simulation with flowsheet modeling via a data-driven approach is performed such that any process changes occurring in the process flowsheet are recorded and replicated in the DEM simulation. This allows DEM simulation to capture dynamic effects of process changes on particle mechanics, which is then transferred to update the data-driven model linked to the flowsheet. This approach allows the flowsheet to incorporate effects of real-time process dynamics on powder behavior. The proposed study focuses on the application of this approach for evaluation and prediction of cross-sectional blend uniformity within the blender. Given the difficulties encountered in experimental measurement of cross-sectional blend uniformity for continuous blenders [8,9], the proposed approach provides an alternative for accurate evaluation of blend uniformity with detailed insights into the effects of process design and operating parameters. Blend uniformity is a critical quality attribute for tablet manufacturing and its accurate evaluation can avoid non-conformity of the final product.In conclusion, the proposed work focuses on incorporation of detailed particle-scale information obtained from high-fidelity simulations within flowsheet models using hybrid data-driven approaches. This increases the mechanistic understanding of powder systems while improving the predictive ability of flowsheet simulations. The integrated approach presented in this work for powder blending systems, can be extended for other unit operations for the construction of robust flowsheet models and allow the implementation of QbD for continuous pharmaceutical process development. Reference: [1] L.X. Yu, G. Amidon, M.A. Khan, S.W. Hoag, J. Polli, G.K. Raju, et al., Understanding Pharmaceutical Quality by Design, Aaps J. 16 (2014) 771–783.[2] J.D. Anderson, J. Wendt, Computational fluid dynamics, Springer Berlin Heidelberg, Berlin, Heidelberg, 1995.[3] P.A. Cundall, O.D.L. Strack, A discrete numerical model for granular assemblies, Géotechnique. 30 (1980) 331–336.[4] M. von Stosch, R. Oliveria, J. Peres, S.F. de Azevedo, Hybrid semi-parametric modeling in process systems engineering: Past, present and future, Computers & Chemical Engineering. 60 (2014) 86–101.[5] N. Metta, M. Verstraeten, M. Ghijs, A. Kumar, E. Schafer, R. Singh, et al., Model development and prediction of particle size distribution, density and friability of a comilling operation in a continuous pharmaceutical manufacturing process, International Journal of Pharmaceutics. 549 (2018) 271–282.[6] D. Barrasso, A. Tamrakar, R. Ramachandran, Model Order Reduction of a Multi-scale PBM-DEM Description of a Wet Granulation Process via ANN, Procedia Engineering. 102 (2015) 1295–1304.[7] Z. Wang, M.S. Escotet-Espinoza, M. Ierapetritou, Process Analysis and optimization of continuous pharmaceutical manufacturing using flowsheet models, Computers and Chemical Engineering. 107 (2017) 77–91.[8] P.M. Portillo, M. Ierapetritou, F.J. Muzzio, Characterization of continuous convective powder mixing processes, Powder Technology. 182 (2008) 368–378.[9] A.U. Vanarase, F.J. Muzzio, Effect of operating conditions and design parameters in a continuous powder mixer, 208 (2011) 26–36.",56,1.0
"Traditionally, batch-wise operation has accounted for a large portion of small- and large-scale industrial pharmaceutical processes. However, the development of newer technologies through intensified unit operations for pharmaceutical manufacturing, as well as more precise modeling and online control, has accelerated over the past 10 – 20 years. In addition to these advancements, interest in implementing more continuous pharmaceutical manufacturing techniques in industry has also increased. With the adoption of new unit operations and operational modes, quality guarantees through initiatives such as Quality by Design [1] (QbD) and Quality by Control (QbC) must be maintained to ensure both consumer and operational safety through good manufacturing and modeling practices. In this work, we present a case study utilizing a start-to-finish framework for the full analysis of a high value, low volume pharmaceutical drug pathway with simulation-based alternative flowsheet comparisons. Our group is developing an end-to-end simulation and optimization framework for generating and comparing flowsheet alternatives for pharmaceutical drug manufacturing. The framework includes a custom model library and supports estimation of model parameters from experimental data.In this presentation, the pharmaceutical drug is analyzed under various operational modes, i.e. batch and continuous for various unit operations. Process development was performed starting with parameter estimation from experiments. For example, crystallization kinetic parameters were estimated with metastable width zone determination at the lab scale in batch crystallizers [2] and nucleation monitoring in microfluidic nitrogen-segmented droplets [3]. We simultaneously developed a digital twin of the process using gPROMS FormulatedProducts to analyze the response of critical quality attributes to process disturbances and better characterize process dynamics under uncertainty. Subsequently, flowsheet simulation and dynamic process analysis via the digital twin was performed. The framework utilizes many computational platforms for analysis by exploiting strengths of different software environments. We utilize Python for parameter estimation and flowsheet simulation, Pyomo for unit operation and flowsheet optimization, and gPROMS FormulateProducts for the digital twin of the process. Thus, the fidelity between gPROMS FormulatedProducts and the custom model library was analyzed as well. The start-to- finish framework is shown to be robust in analyzing different operational modes and conditions in pharmaceutical processing. To demonstrate this capability, we analyze in detail one batch flowsheet, one continuous flowsheet, and one hybrid flowsheet. Here, we compare these flowsheet alternatives manually for proof-of-concept; however, this framework is ultimately to become an end-to-end optimization framework which will algorithmically generate and compare flowsheet alternatives, specifically for the comparison of batch, continuous, and hybrid operational modes.  Food and Drug Administration. Pharmaceutical cGMPs for the 21st Century—A Risk-Based Approach; Technical Report; U.S. Department of Health and Human Services, Food and Drug Administration, Center for Drug Evaluation and Research (CDER): Rockville, MD, USA, 2004.Nagy, ZK; Fujiwara, M; Woo, XY; Braatz, RD; Determination of the kinetic parameters for the crystallization of paracetamol from water using metastable zone width experiments. Industrial and Engineering Chemistry Research. 2008, 47 (4), 1245-1252.Lu, J; Litster, JD; Nagy, ZK. Nucleation studies of active pharmaceutical ingredients in an air-segmented microfluidic drop-based crystallizer. Crystal Growth & Design. 2015, 15 (8). 3645-3651",56,2.0
"Digital twins built using mechanistic models are playing an increasingly significant role in helping pharmaceutical industries develop robust and more economically efficient manufacturing processes. Building a digital twin for an end-to-end drug manufacturing process allows exploration of the individual and combined effects of numerous process parameters during the active ingredient and drug product manufacturing stages on the final drug product performance. This allows for the development of a more robust drug manufacturing process and greater assurance of product quality while reducing process development timelines and resources.This work will outline the quantitative analysis performed on the manufacturing process for mefenamic acid by building the end-to-end mechanistic flowsheet model Each element of the drug substance production step, including synthesis step, crystallization and separation are validated using process data individually. Once each unit operation step in the production step is validated, a systems model was configured to provide a quantitative representation of the end-to-end production process including drug substance, drug product and product performance elements. This end-to-end model was subsequently utilized to develop a quantitative understanding of the effect of process disturbances, raw material variability, process parameters, formulation parameters and model uncertainty on CQAs (e.g. tablet properties) and manufacturability. Along with the analysis, the relative influence of those factors (sensitivity indices) on the desired CQAs will also be presented. Furthermore, the impact of batch and continuous operational elements of the process will also be probed. This work helps to identify any existing gaps in the scientific understanding of the effects of some of the API attributes on the desired KPIs during this end-to-end flowsheet analysis will also be discussed.  ",56,3.0
"Digital twins built using mechanistic models are playing an increasingly significant role in helping pharmaceutical industries develop robust and more economically efficient manufacturing processes. Building a digital twin for an end-to-end drug manufacturing process allows exploration of individual and combined effects of numerous process parameters and material attributes within the end-to-end process. The end-to-end process includes the active pharmaceutical ingredient (API) and drug product manufacturing stages as well as the final drug product performance. This can allow for the development of a more robust drug manufacturing process and greater assurance of product quality while reducing process development timelines and resources. This work details an actual application and analysis performed on an existing UCB drug product manufacturing process by building the end-to-end flowsheet model in gPROMS FormulatedProducts, shown in Figure 1. This analysis includes qualitative understanding of the effect of process disturbances, API variability, process parameters, formulation parameters and model uncertainty on tablet properties, and manufacturability key performance indicators (KPIs). Along with this analysis, the relative influence of those factors (sensitivity indices) on the desired tablet properties are presented. Critical gaps were identified in the scientific understanding of the effects of some of the API and granule attributes on the desired tablet properties and KPIs (e.g. mechanistic link of the particle size distribution to the bulk density of the powder impacting on cohesive forces of powder during roller compaction operation, critical fine fraction generated during the particle forming steps leading to segregation of the powder during powder charge operation and impact on content uniformity, etc.). The importance of data integrity, breaking silos across departments and ensuring correct operating ranges used in the analysis are some of the lessons learnt when developing a digital twin. This work acts as a starting point for UCB to begin utilizing the end-to-end approach with gPROMS FormulatedProducts as a platform for future drug development pipelines. It also opens up new avenues for scientific understanding in the mechanistic models employed to address the gaps identified during the project (e.g. availability of critical material characterization information for fully exploiting the early adoption of these digital twins in a model risk assessment approach and analysis the feasibility and manufacturability of the drug product production processes). As regards gaps in science, these will be addressed either through improving mechanistic understanding or incorporating data driven elements in the overall mechanistic end-to-end process model.",56,4.0
"Motivated by aims to improve R&D efficiency, reduce time to market, and develop robust drug products, the pharmaceutical industry has seen increased application of mechanistic-based system models. Such approach establishes science-based understanding of relationships between various process parameters and Quality Attributes (QAs) with reduced experimentation requirements.The primary objective of this study is to demonstrate system model of a dry granulation-based drug product manufacturing process with in-vitro dissolution. This approach is used to determine the effect of drug product manufacturing process on tablet (API) dissolution. The system model, which was developed in gPROMS FormulatedProducts, couples multiple unit operations together, namely roller compaction, screen milling, lubrication, tablet compression, and in vitro dissolution. The roller compactor and the tablet press were calibrated by estimating the respective compaction-related model parameters against experimental data for ribbon and tablet properties. In order to capture the effect of the disintegration of the tablet solid matrix on the delayed API dissolution, a hierarchical description of the tablet and a porosity-dependent disintegration scheme was adopted. The in-vitro dissolution was calibrated by estimating the tablet and granule disintegration kinetic parameters in order to capture the dissolution profile of the API.Following model development activities, the system model (Figure 1) was used to assess the impact of the drug product manufacturing process parameters on API dissolution. Monte Carlo and variance-based analyses were performed using gPROMS FormulatedProducts’ Global System Analysis framework to understand the relationship between drug product process parameters and API dissolution. Such framework can be effectively used in the workflow of pharmaceutical product development to define the input vs output relationships between process parameters and product performance. Furthermore, it has the potential to significantly contribute to QbD framework.Figure: An integrated drug product and in-vitro dissolution model in gPROMS FormulatedProducts",56,5.0
"Due to the improvement of manufacturing technologies and publicised industrial cases realising the potential value of adopting continuous manufacturing in pharmaceutical production, much of the research focus has been realigned to the understanding of integrating batch-wise operations into a single production line. However, there is little regulatory instruction and good model-based principles to follow. From a modelling point of view there is a lack of coherence in approach due to the profound lack of understanding of the process dynamics. Despite a significant adoption of particulate system modelling approaches, it is apparent that very few existing, science-based models from academia are fully explored and implemented for industrial practice. Furthermore, while a generic modelling platform for integrated modelling of an industrial process could be an enabling technology, it can be formidable for an industrial practitioner to apply such digital tools.This paper aims to develop a systematic framework for approaching digital design of a continuous manufacturing process by transferring state-of-the-art innovations from the modelling world into industrial practice through a mechanistic model-based digitalisation platform. The digitalisation framework is built upon gPROMS using a defined case explored in the DiPP (Diamond Pilot Plant), a continuous powder processing in the University of Sheffield, to demonstrate the usability and expandability of the mechanistic model approach. The DiPP of a ConsigmaTM-25 manufacturing line from powder to tablet consists of twin screw granulation, fluid bed dryer, cone mill, vertical blender and tableting compaction. Global System Analysis of the DiPP with sound engineering principles defined in underlying mechanistic models is carried out to enable a comprehensive exploration of the systematic responses and the development of a robust operating space. The systematic modelling framework will enable the pharmaceutical industry to effectively and efficiently design and implement continuous pharmaceutical manufacturing processes, leading to safer and faster design and operating decisions.",56,6.0
"Drug nanoparticle-laden microparticles (nanocomposites) and amorphous solid dispersions (ASDs) have been commonly used to deliver poorly soluble drugs and enhance their solubility, dissolution rate, and bioavailability. A major shortcoming of drug nanocomposites as compared with drug ASDs is their limited supersaturation capability in dissolution, whereas ASDs have various issues about physical stability of the metastable form of the drug (amorphous). Here, we prepared drug hybrid nanocrystal–amorphous solid dispersions (HyNASDs) and compared their performance to ASDs. A wet-milled griseofulvin (GF, model poorly soluble drug) nanosuspension and a GF solution, both containing the same dissolved polymer–surfactant (SDS: sodium dodecyl sulfate) with 1:1, 1:3, and 1:5 GF:polymer mass ratios, were spray-dried. Hydroxypropyl cellulose (HPC) and Soluplus (Sol) were used as matrix-forming polymers. XRPD, DSC, and Raman spectroscopy reveal that ASDs were formed upon spray-drying the solution-based feed, whereas nanocomposites and nanocomposites with >10% amorphous content, HyNASDs, were formed with the nanosuspension-based feed. Sol provided higher GF relative supersaturation in the dissolution tests than HPC owing to Sol’s stronger intermolecular interactions and miscibility with GF and its recrystallization inhibition. Besides the higher kinetic solubility of GF in Sol, presence of GF nanoparticles vs. micron-sized particles in the nanocomposites enabled fast supersaturation. SDS provided enhanced wettability, allowing for fast supersaturation from Soluplus-based HyNASDs (~300% within 20 min), while higher Soluplus loading led to higher supersaturation. This study demonstrates successful preparation of fast supersaturating HyNASDs, which renders nanoparticle formulations competitive to ASDs in bioavailability enhancement of poorly soluble drugs.",57,0.0
"Leveraging hollow fibre membranes and high boiling point organic solvents in the production of enteric amorphous solid dispersionsRute Mota, Rui C. Silva, João VicenteIn recent years up to 90 % of the drugs in the discovery pipeline are poorly soluble molecules which poses challenges for bioavailability and dosage form development. Furthermore, the physical and chemical properties of these candidate compounds result in poor solubility in aqueous and volatile organic solvents which hinders the development of amorphous solid dispersion (ASD) using spray drying (SD), while at the same time possessing melting points too high to use hot-melt extrusion (HME): such compounds are typically designated as “brick dust” [1].In the literature several technologies can be found to deal with poorly soluble APIs: nanomilling or complexation with cyclodextrins fall within the non-ASD alternatives; co-precipitation; supercritical CO2 and KinetiSol [2], as an alternative ASD technology. One such strategy has been developed where nano or micro particles of API-polymer composite precipitate are produced via co-precipitation process using microfluidization. This technology offers the advantage of precise control of particle morphology and generates particles with surface areas that can be 10 times greater than those obtained using HME or SD [3]. Although this technology circumvents the low solubility aspect of the “brick dust” molecules it has some the following drawbacks: a) a suspension with very low solids load and, thus, longer drying process times; b) with “brick dusts” finding a solvent for both API and polymer in amounts for economically viable throughput usually means using organic polar solvents with high boiling points, such as dimethylacetamide (DMA) or dimethylformamide (DMF) or dimethyl sulfoxide (DMSO) which poses challenges to spray dry [3]. These drawbacks amount to longer process times with low throughput which may not be financially appealing in early stage development.The work presented herein consists on a co-precipitation process with the inclusion of a cross-filtration hollow fibre membranes unit as a straightforward and scalable technology focusing on producing amorphous solid dispersions with “brick dust” compounds. The cross-filtration operation consists in a two-step approach: 1) the first step, concentration of the suspension, the removal of excess antisolvent to increase the solids load; 2) the second step, a solvent swap, where the presence of high boiling point organic solvents in the suspension is minimized by replacing it with an aqueous based solvent. This approach widens the co-amorphous technology to include high boiling points organic solvents best suited for “brick dust” while circumventing the need for secondary drying and reducing cycle time.References[1] Temtem M, The Coming of Age of Amorphous Solid Dispersions, Pharma's Almanac, 26 October, 2018.[2] Jermain SV, Brough C, Williams RO 3rd, Amorphous Solid Dispersions and Nanocrystal Technologies for Poorly Water-Soluble Drug Delivery – An Update, Int J Pharm. 2018 Jan 15; 535 (1-2): 379-392.[3] Duarte Í, Corvo ML, Serôdio P, Vicente J, Pinto JF, Temtem M, Production of nano-solid dispersions using a novel solvent-controlled precipitation process – benchmarking their in vivo performance with an amorphous micro-sized solid dispersion produced by spray drying, Eur J Pharm Sci. 2016 Oct 10;93:203-14.Figure 1 – Co-Precipitation Process Schematics with cross-filtration membranes setup",57,1.0
"IntroductionFormulation of Biopharmaceutics Classification System (BCS) class II and IV drugs into amorphous solid dispersions (ASDs) is a promising strategy to increase both the apparent aqueous solubility and bioavailability of these drugs. Although drug release from ASDs is still poorly understood, it has been suggested that drug release from ASDs is polymer-controlled at low drug loadings, whereby both drug and polymer release at the same rate (congruently) [1]. However, in contrast, when a higher drug loading is reached, which differs for different drug-polymer combinations, an abrupt decrease in drug release rate may be observed which may be explained by a switch to drug-controlled release, whereby release of drug and polymer is incongruent [2].In the present work, it is intended to further understand the dissolution performance of different ASD formulations by following the dissolution rate of ASD particles using a laser-diffraction methodology. Furthermore, it will be also studied the possible drug enrichment on the partially dissolved ASD particles by Raman. Intrinsic dissolution was also applied to measure the dissolution rate of the API from the ASD matrix. For this study several ASDs were prepared with two grades of HPMC-AS having different ratios of succinoyl:acetyl groups (L and M) by spray-drying. A BCS class II drug will be used as a model drug.Materials and methodsProduction of ASDsAmorphous solid dispersions were produced by spray-drying using a mixture of methanol and dichloromethane at 1:4 ratio (w/w), respectively, as solvents. The ASDs were produced at different Drug:HPMC-AS grades (L and M) and at different drug loads (15% and 35 % w/w). For these experiments, a Buchi Mini Spray Dryer B-290 was used with a two-fluid nozzle in closed loop. The spray-dried samples were further dried to remove any residual solvents in a vacuum drying oven at 40 ºC. Physical characterization of ASDAmorphous solid dispersions were characterized by X-Ray Powder Diffraction (XRPD) to confirm their amorphous nature. Morphology was studied by scanning electron microscopy (SEM) and particle size distribution was evaluated by Laser Diffraction (Sympatec). ASDs and API dissolution performance To get an insight into ASDs disintegration/dissolution kinetics, these ASDs particles were added to the dispersion unit of Mastersizer 2000 containing 150 mL of FaSSIF (biorelevant dissolution media). 86 mg and 200 mg of each ASD with 35 wt.% and 15 wt.% drug loads, respectively, were added to this dispersion unit to obtain 0.2 mg/mL as a drug target concentration. Particle size of ASDs was monitored for 4 hours. The reduction of particle size of ASDs during dissolution and colloidal formation was compared with the dissolution rate of the API released from these ASD particles. The drug concentration over time was measured by HPLC.ResultsASD prepared by spray-drying showed an amorphous nature with no diffraction peaks from crystalline molecule. The particle morphology of the produced ASD was also analyzed by SEM. All the ASDs presented a shriveled morphology (raisin-like particles) which is characteristic of slower spray-drying kinetics. The particle size distribution was similar between different amorphous solid dispersions.In order to monitor the dissolution performance of the four ASD, the different formulations were suspended in FaSSIF (pH 6.5) in the dispersion unit of the Mastersizer 2000.In all ASD formulations it was possible to see the appearance of a colloidal population with a submicron size (< 600 nm) after the 240 minutes (Figure 1, A). ASD dissolution showed to be slower in formulations containing higher API loading, since after 60 minutes it is still possible to observe ASD particles in suspension. It’s worth mentioning that for formulation - API:HPMC-AS M (35:65) - a full disintegration of ASD did not occur even after 240 minutes.Raman imaging was performed to the API:HPMC-AS M (35:65) ASD before and after being submitted to the experiment in the Mastersizer equipment. Since part of this sample was not dissolved after 240 minutes, an aliquot was taken from the dispersion unit and centrifuged prior Raman analysis. The intensity of one specific Raman band of the API (1605 cm-1) and the Polymer (2943 cm-1) were monitored in ASD particles before and after the experiment. Results showed that the ratio of 2943 cm-1/1605 cm-1 bands is above 1 (red spectra, Figure 1, B) in this ASD particles before being suspended into FaSSIF. However, this ratio showed to invert after the experiment, i.e, the polymer specific band at 2943 cm-1 seems to reduce faster than API specific band suggesting an absence of a concomitant dissolution of both elements when this ASD is suspended in FaSSIF. This is consistent with a surface drug enrichment phenomenon [2], suggesting faster dissolution of the polymer in relation to the API.Intrinsic dissolution confirms that the dissolution rate of ASDs is aligned with the dissolution of ASDs particles observed in laser-diffraction experiment. It is possible to note that formulations containing 35% of API present slower dissolution rates than the ones containing 15% of API. Again, API:HPMC-AS M (35:65) formulation showed to have the lowest performance (Figure 1, C), which seem to be related with a drug enrichment of ASD particles in this particular formulation.ConclusionsThe results presented in this work indicate that dissolution of ASD particles (by laser-diffraction) give an important insight on the API dissolution rate. In detail, it was interesting to note that formulations which showed to disintegrate/dissolve faster leading to a stable colloidal suspension in laser-diffraction test were also the formulations with the highest and fastest API dissolution rate. This proves that their improved performance is related with the ASD dissolution mechanism. The faster dissolution of ASD particles in 15% drug load formulations suggests possible polymer-controlled dissolution which is associated with low drug loadings formulations [2]. The Raman analysis of API:HPMC-AS M (35:65) particles after being suspended/stirred in FaSSIF for 240 min suggested a faster dissolution of the polymer in comparison to the API, leading to particles enriched in API. This explains the slowest dissolution rate observed in this formulation. Therefore, it can be concluded that API dissolution performance is impacted not only by the drug load but also by the HPMC-AS grade. As a future work, the different hydrogen bonding between the API and the 2 different grades of HPMC-AS with varying ratios of succinoyl:acetyl groups will be explored. This study also shows that laser diffraction and Raman spectroscopy pose as orthogonal techniques to understand the dissolution mechanisms of ASDs.REFERENCESTres F, Posada MM, Hall SD, Mohutsky MA, Taylor LS. Mechanistic understanding of the phase behavior of supersaturated solutions of poorly water-soluble drugs. Int J Pharm. 543(1-2): 2018Saboo S, Kestur US, Flaherty DP, Taylor LS. Congruent release of drug and polymer from amorphous solid dispersions: insights into the role of drug-polymer hydrogen bonding, surface crystallization and glass transition. Mol Pharm. 17(4): 2020.",57,2.0
"We have previously demonstrated the application of Focused Beam Reflectance Measurement (FBRM) for analysis during immediate-release tablet disintegration to evaluate changes in particle size and count. This approach could distinguish changes in tablet particle size during disintegration due to varying properties, such as tablet hardness. Here we present a proof-of-concept study demonstrating the generation of a predictive statistical model of a drug product design of experiments (DoE) using parameterization of FBRM measurements taken during tablet disintegration. The FBRM was used to monitor the disintegration of tablets generated in an 18-run DoE evaluating five variables at three levels. The DoE evaluated API properties as well as dry granulation and tableting conditions. Statistical models were generated using the DoE input variables fit to predict FBRM measurement parameters. Robust, multi-factor models identified that several FBRM measures such as total particle count, time of maximum particle count, mean chord (particle) length, or bounded particle size ranges were dependent on several DoE inputs. Furthermore, these factors identified via FBRM disintegration corresponded with the significant factors of dissolution prediction models. In contrast, a USP disintegration method failed to provide a predictive model with multiple significant factors. The FBRM approach to monitoring disintegration allows for a more comprehensive understanding of tablet performance and provides greater insight into the impact of upstream material properties or processing parameters. Material properties and process parameters ultimately impacting dissolution could be predicted earlier and in a material-sparing way. It further offers an orthogonal measurement to gain a better understanding of tablet dissolution explained in a framework of underlying particle behavior.",57,3.0
"For a solid dosage form, medicine is dispensed based on the dissolving of the active ingredient in the gastrointestinal tract. The significance of the dissolution rate on drug clinical performance has been recognized for a long time. One of the most critical quality attributes was assessed depends on its dissolution behavior. Unfortunately, significant numbers of FDA approved drugs and developmental drugs are hydrophobic and poorly soluble. It has been documented that approximately 40% of approved drugs and 90% of drugs under development are categorized as molecules of low water-solubility and low bioavailability. Due to discovering new drugs is increasing difficulty and become a non-effective way to approach therapeutic requirements, enhancing the solubility of these available drugs is considerably investigated. The prevalent approach for dissolution improvement can be ionizing the drug molecules, producing salt from, micronizing API particles, and using solid dispersion techniques. However, these techniques may require complex unit operations, create unstable solid forms, or generate APIs of poor manufacturability. In this work, we will propose a novel approach in which a small amount of surfactant (< 10% of API weight) is coated on the surface of API particles by applying shearing and heat simultaneously. In this way, we are able to enhance the dissolution of hydrophobic and insoluble API without changing its original solid state.In our work, three poorly soluble APIs (Ibuprofen, Carbamazepine, and Fenofibrate) and two low-melting-point surfactants (Poloxamer407 and Cetylpyridinium Chloride) are selected. Three specific case studies are performed. In each case study, a twin-screw extruder is used to perform melt-coating because it can apply shearing and heat simultaneously. At a certain temperature, the surfactant can partially melt, then smears and coats on the surface of API particles. Then, the hydrophobic APIs presents a new hydrophilic surface. The particle morphology is photographed using SEM, which shows that the surfactant successfully coats on API particles.Then, the dissolution test is performed for both treated powder and finished product (tablet and capsule) and compared with the physical mixture and no-surfactant case. Results show that the melt-coating of the surfactant considerably enhances the dissolution rate. The treated product can release drugs 2-5 times faster than the physically mixed product. In contrast, the physical blending surfactant has a marginal effect on dissolution in comparison to the no-surfactant case.",57,4.0
"In this work, we have designed several new cocrystals and solvates of a pharmaceutical molecule apremilast. Out of these new multicomponent forms, the cocrystal with benzoic acid is particularly promising due to its improved intrinsic dissolution rate (approximately 400%) and comparable thermal properties to marketed apremilast B. The crystal structure of this cocrystal was solved using single-crystal X-ray diffraction to provide further insight into the cocrystal formation. Cocrystallization process was explored to simplify further scale up. Utilizing the ternary phase diagram, constructed by the combination of specific experiments and a solid-liquid equilibria model, we were able to significantly decrease the experimental efforts compared to traditional approaches such as discontinuous isoperibolic thermal analysis (DITA). Methylehtyl ketone was chosen as a solvent for the cocrystallization experiment. This was performed in a 100ml reactor, were we investigated the impact of process parameters included cooling rate, stirring speed, impeller, concentration and seeding on the size and morphology of the apremilast-benzoic acid cocrystal. Obtained results confirmed strong impact of operating conditions on the crystals sizes and shapes. These crystals were consequently tested during dissolution and it was observed that significant improvements in the dissolution rate can be achieved by varying the crystal sizes and well as shapes.",57,5.0
,58,0.0
,59,0.0
"Wearable sensors have received a major recent attention owing to their considerable promise for monitoring the wearer’s health and wellness. The medical interest for wearable systems arises from the need for monitoring patients over long periods of time. These devices have the potential to continuously collect vital health information from a person’s body and provide this information to them or their healthcare provider in a timely fashion. Such sensing platforms provide new avenues to continuously and non invasively monitor individuals and can thus tender crucial real-time information regarding a wearer’s health. Our research over the past decade has aimed toward filling the gaps toward providing biochemical information, beyond that given by common wrist-watch mobility trackers. This presentation will discuss recent developments in the field of wearable electrochemical sensors integrated directly on the epidermis or within the mouth for various non-invasive biomedical monitoring applications. Particular attention will be given to non-invasive and minimally-invasive monitoring of metabolites and electrolytes using skin-worn and microneedle amperometric and potentiometric sensors, respectively, along with related materials, energy and integration considerations. The preparation and characterization of such wearable electrochemical sensors will be described, along with their current status and future prospects and challenges.",59,1.0
"IntroductionThe current manuscript outlines the fundamentals of a methodological framework for precision exposure assessment and gives concrete examples of its application in different European cities to assess the health risk associated with exposure to fine and ultra fine particles of chemical and biological origin. The work described herein involves a multi-city personal sensor campaign aiming to refine PM exposure using low-cost portable sensor data, exposure and human respiratory tract deposition modelling to estimate the biologically effective internal daily intake on an individual basis and agent-based modelling to estimate exposure and consequent health risk to the community.MethodologyA prototype monitoring device (IcaruSense) was developed for measuring three size fractions of PM (1, 2.5 and 10 μm), enabling direct assessment of personal exposure. The device is based on an Arduino microcontroller where miniaturized electrochemical sensor-modules are connected. In order to improve sensor performance, correction factors were used to account for diverse environmental conditions and they were calibrated with the help of a well-validated light scattering optical sensor. Study participants wore a physical activity wristband (Garmin Vivosmart 3) that records steps, distance, type of activity, heartbeat and sleeping patterns. Finally, participant positions were recorded using a GPS sensor, integrated into the PM sensor kit. After calibration and validation, the IcaruSense devices were used to capture daily variability of PM exposure. Exposure was further refined by estimating inhalation adjusted intake rate from the continuous time-activity data collected from the physical activity and the GPS sensors. Using the inhalation adjusted intake rate PM deposition across the human respiratory tract (HRT) was readily estimated using the Multiple Path Particle Deposition (MPPD) model.A city scale agent-based model (ABM) was developed to support this work by expanding the precision exposure estimates from the individual to the community level. The model feeds into population-based exposure assessment without imposing prior bias, basing its estimations onto emergent properties of the behaviour of the computerised autonomous decision makers (agents) that compose the city-system. Population statistics, road and buildings network data were transformed into human, road and building agents, respectively. Informed by literature-extracted relationships between Socio-Economic Status (SES) and human behavioural patterns, the established ABM simulates societal dynamics. Time-use survey outputs were associated with human agent behavioural rules, aiming to model representative to real-world routines. Moreover, real time-geography of exposure data, extracted from the precision sensors campaigns in nine European cities were used to parametrise and enhance the model.Mobility behaviour and eventually exposure to pollutants change as a result of a human agent-specific decision making. Virtual individuals of different sociodemographic backgrounds use different means of transportation and follow different sequence and types of activities. Behaviours that were not explicitly programmed, arise through the human agents’ interactions, enabling the examination of emergent behaviours from the bottom-up. Spatiotemporal trajectories are coupled with spatially resolved Particulate Matter (PM) levels, enabling the assessment of personal and inhalation adjusted exposure, based on type of location and intensity of enrolled activities.Results and DiscussionThe above methodology was applied in the personal sensors campaign of the HORIZON2020 EU Project ICARUS, where exposure and intake to PM of almost 100 individuals in each out of nine European cities were monitored. The integrated hybrid approach outlined above, which couples multi-sensor datasets on an individual basis with advanced respiratory tract modeling, allowed us to significantly differentiate the actual intake of the participants, highlighting larger differences than the ones attributed to the spatial differentiation of air pollution based on fixed stations (difference of ambient PM levels of 50% were translated in intake differences up to 110%). These differences are the result of the differences in PM size fractions that are captured by the sensors and the capability of the HRT model to translate these differences in PM size distribution accounting for variation in respiratory tract physiology among the participants, which is ultimately reflected in intake estimates.ABM findings indicated that inhalation adjusted exposure differences between 2 individuals living in the same neighbourhood can vary by as much as 87%, due to radically different spatiotemporal behaviours. This major difference was observed for cases where one human agent has a full-time job and often exercises in outdoor courts whereas his/her neighbour is a retired homemaker. For the same reason, ABM runs confirmed that even people who reside in the same dwelling might not necessarily be exposed to similar levels of pollution. Model results indicated that PM inhalation adjusted exposure between flatmates can differ by 61%. Evidence was, also, provided for vulnerable subgroups of population that are disproportionately subjected to high levels of exposure. ABM-retrieved population exposure findings indicated that subgroup median exposure for children, adult males with a full-time job and low income and human agents of a lower educational level was notably higher than the total population median value. The validated model can assess exposure for the entire sociodemographic spectrum of a simulated region, without the constant need for repeated individual monitoring which is often not feasible due to high costs, ethical constraints and complex organisation that is associated with such measurements. By modelling the heterogeneous routines of human agents, the ABM produces detailed information related to the societal system examined and generates data that could be used to fill in the gaps that exist in traditional datasets.ConclusionsThis study presents a new methodological approach to estimate the external exposome, which encompasses the totality of human environmental exposures at an individual and community level. This method helps to integrate substantive considerations of individual status into long-term urban planning and management; it would also facilitate the identification of both physically and socially practical means for reducing life-threatening exposure levels. The methodological paradigm for precision assessment of exposure at the individual and community levels laid out in this work can be used for evaluating the impacts of different public health policies prior to implementation.",59,2.0
,59,3.0
"A remaining challenge in the pursuit of rational design of novel and optimized electrolytes is understanding and ultimately predicting the reaction cascade responsible for the creation of a functional liquid-anode solid-electrolyte interfaces (SEI). We here present an overview of the challenge, in different energy storage systems and advocate that a data-driven approach coupled with a high-throughput quantum chemistry computational infrastructure1  can address the complexity of such reaction cascade. To this end, we have constructed a chemical reaction network containing over 50,000 elementary reactions which allow for bond breaking/formation as well as oxidation/reduction reactions. The reaction energetics are obtained from our computational framework which automate geometry optimization and vibrational frequency calculations for molecules, including radical, charged, metal-coordinated, and solvated species. Our framework, using select levels of theory and a suite of on-the-fly error handlers, is able to successfully converge to a minimum energy structure and calculate the molecular thermodynamic properties in over 97% of cases. Individual calculations can then be combined to perform complex workflows in a fully automated fashion. To date, we have applied this framework to over 25,000 unique molecules relevant to the formation of solid-electrolyte interphases in Li-ion and Mg-ion batteries. To analyze possible reaction pathways, we apply a graph representation to our reaction network, leveraging an iterative reweighting strategy that solves for all prerequisite branching such that traditional pathfinding algorithms correctly capture coordinated pathways. Machine-learning algorithms operating on bond-formation and breaking energetics aid in rapid evaluation of highly reactive processes. We show that without any apriori chemical intuition, our automated framework recovers the most favorable reaction paths to form key SEI components, which were carefully identified over the past two decades. Thus, our data-driven approach and infrastructure show promise to accelerate the understanding of chemical reactivity in complex environments, in the aid of novel electrolyte design.",60,0.0
"In this talk, I will describe our Robotic-Accelerated Perovskite Investigation and Discovery (RAPID) system. The first generation of RAPID uses inverse temperature crystallization (ITC) to grow halide perovskite single crystals for x-ray structure determination and bulk characterization using commercial liquid handling robots. The second generation extends this to antisolvent vapor diffusion experiments. Experiment plans for the syntheses are contributed remotely, by both human scientists and algorithms trained on the reaction data, facilitating by our ESCALATE (Experiment Specification, Capture and Laboratory Automation Technology) data management system. Incoming data collected by ESCALATE is used to automatically train machine learning models, evaluate model performance and feature influence, and quantify reproducibility. A live web dashboard communicates these insights to the scientist and management in visual form. I will conclude by describing case studies about new scientific insights extracted from the comprehensive RAPID dataset that have been enabled by this comprehensive dataset, and discuss ongoing deployments of ESCALATE to perovskite thin film and vapor diffusion synthesis experiments. ",60,1.0
"This talk will cover recent progress in the application of data science and machine learning techniques to problems in synthetic chemistry as they relate to molecular discovery. The typical discovery paradigm is an iterative process of designing candidate compounds, synthesizing those compounds, and testing their performance, where each repeat of this cycle can require weeks or months. Time and cost constraints may necessitate selecting compounds on the basis of what are perceived as fast to synthesize, rather than what are most informative to assay. Data science and statistical learning offer unprecedented opportunities to systematize and streamline the process by which new functional small molecules are designed and synthesized. I will describe how historical reaction data can be used to inform decision-making in small molecule pathway design, for both retrosynthesis and forward reaction prediction, and how these techniques can be integrated with de novo molecular design to triage candidate compounds for testing.",60,2.0
"The adoption of data-science methods within chemical engineering is primed to revolutionize materials discovery and to accelerate developments in molecular understanding. Yet, the movement is largely dominated by data and information originating from molecular modelling and simulation, or that is acquired from legacy sources (e.g. databases, literature mining) accumulating data from decades of careful experimental work. In order to advance data-driven materials developments well into the future, high-throughput experimentation (HTE) and automation throughout the complete laboratory workflow must also be developed and widely adopted to accelerate rates of experimental data production. In this talk we outline several examples and experiences showcasing how our research group is developing and adapting hardware and software infrastructure to accelerate the pace of molecular discovery in soft-matter systems for applications in health care, clean energy and materials synthesis. The talk will highlight recent research examples related to the implementation of HTE for electrolyte discovery and colloidal formulation/synthesis. We will also highlight significant challenges that have emerged from transitioning an established ‘wet-laboratory’ practice to HTE. These relate to adapting routine experimental methods to achieve HTE, developing new skills within the research workforce, the adoption of new data stewardship practices, needs for autonomous data sorting/classification, algorithms for automatic modeling and analysis and many others. Conversely, we will also highlight the numerous opportunities that emerge for enhancing virtual collaboration, enabling open data/hardware/software sharing, tackling challenging irreducible problems (e.g. optimization of complex formulations), and improving the outlook for the implementation of 'self-driving' laboratories.",60,3.0
"Deep learning is revolutionizing many areas of science and technology, particularly in natural language processing, speech recognition and computer vision. In this talk, we will provide an overview into latest developments of machine learning and AI methods and application to the problem of drug discovery and development at Isayev’s Lab at CMU. We identify several areas where existing methods have the potential to accelerate pharmaceutical research and disrupt more traditional approaches. First we will present a deep learning model that approximate solution of Schrodinger equation. Focusing on parametrization for drug-like organic molecules and proteins, we have developed a single ‘universal’ model which is highly accurate compared to reference quantum mechanical calculations at speeds 10^6 faster. Second, we proposed a novel computational method for de-novo design of molecular compounds with desired biological and physical properties.",60,4.0
"Chromatographic operations are the workhorse for protein purification in the biopharmaceutical industry due to their unparalleled separation efficiency and capacity. Typically, chromatographic processes consist of multiple polishing steps, at least one of which is operated in a bind and elute mode. Although bind and elute chromatography is very effective at removing product variants, it suffers from limited capacity and long processing times. In this work, we developed efficient cation exchange chromatography (CEX) flowthrough steps for mAbs using a combination of high throughput screens and lab scale experiments. CEX resins were screened across parameter space using high throughput screening techniques. Conditions which improved process performance as compared to a bind and elute operation were identified. Following high throughput screening, this process was evaluated at the lab scale for further development and refinement. Comparable product quality and process consistency between the Tecan and the lab scale experiments suggest an efficient and reliable high-throughput process development workflow for CEX flowthrough operations.",61,0.0
"Tangential flow filtration (TFF) is a pressure and shear driven separation process that uses membranes to separate components in a liquid solution based primarily on size differences. TFF processes such as microfiltration and ultrafiltration are widely used in the pharmaceutical industry for concentration and buffer exchange in biologics. Due to the nature of TFF membrane materials, these unit operations have been primarily limited to aqueous or low-solvent processes. Example TFF applications include monoclonal antibodies and microbially derived proteins and peptides.Recent advances in membrane technology have enabled the fabrication of membranes with smaller pore sizes and composed of materials with improved compatibility to organic solvents. For larger and more complex synthetic peptides, ultrafiltration may provide a superior alternative to distillation and/or chromatography for concentration and solvent exchange.This presentation will highlight the use of ultrafiltration as part of a synthetic peptide process to concentrate purified peptide solution and to perform a solvent exchange in preparation for precipitation of the pure API. This work represents a novel transfer of TFF technology from the biologic/microbial platform to the synthetic peptide purification platform.",61,1.0
"Bio-active peptides are the next generation of pharmaceuticals which lead to a significant increase in demand over the last years [1]. Remarkable improvements in the upstream processing of peptides has been achieved, leading to a shift in the bottleneck to downstream purification [2]. Recently, crystallisation has emerged as a novel technology to replace expensive downstream purification techniques in the production line of bio-active peptides in order to increase economic efficiency [3, 4]. Due to their high molecular weight, low diffusivity, and soft and fragile nature, peptides require a gentle, but intensive mixing in order to achieve the same level of crystallisation reproducibility at larger scales than microlitres. The lack of suitable mixing strategies for large-scale peptide crystallisation, while guaranteeing high crystallisation throughput and reproducibility of the crystal qualities, has been identified as one of the main limitations that has to be overcome in order to apply crystallisation as a sustainable downstream purification techniqueIn this study, the importance of mixing on the controllability and reproducibility of the crystallisation process in large-scale batch crystallisers will be discussed. Instead of commonly used mixing strategies, such as over-head stirring, that usually leads to a more uncontrolled crystallisation with a poor crystallisation reproducibility, the impact of a gentler mixing strategy by using mechanical, oscillatory shaking will be reported. Furthermore, a newly developed image-analysis tool specific to peptide crystals will be presented. It is used to determine crystal sizes and distribution in-situ by processing images of crystals to evaluate the impact of mixing. A simple model peptide with a low solubility and molecular weight, which is known to crystallise, is used to investigate the importance of mechanical mixing on achieving a reproducible peptide crystallisation process. A tubular shaped batch crystalliser with volumes from the millilitres to the tens of millilitres along with different geometries was utilised. The crystallisation condition, such as pH, concentration, shaker speed, templating additives, and the orientation of the crystalliser to the shaking plane have been optimised. The impact on the crystallisation process is evaluated by means of measuring induction time, crystal yield, crystal sizes, and size distribution.The induction time was determined from peptide concentration by UV-vis spectroscopy every 20-30 minutes and compared to optical microscopy images. These images were analysed using a novel automated evaluation-routine we developed to evaluate the crystallisation behaviour and crystal properties. This new computer-based image evaluation-routine enables the autonomous processing of hundreds of peptide crystal images. Thereby, commonly applied image processing steps, such as background noise filtering and image-gradient based edge determination for obtaining the crystal contours, have been utilised to determine the macro-scale crystal properties, such as crystal size distribution. This novel evaluation-routine is less expensive, able to calculate the macro-scale crystal properties within a couple of minutes, and only requires tens of microlitres of the crystal suspension instead of tens of milligrams of solid crystals, which is required in commonly used analytical techniques such as dynamic light scattering. Furthermore, it enables the evaluation of hundreds of crystals with sizes from the micrometre to the tens of centimetres scale. This makes the evaluation-routine suitable for studying macro-scale crystal properties for expensive and poorly soluble peptides in-situ, allowing to optimise crystallisation parameters during the crystallisation process. Comparison of crystal size distribution obtained with the new routine and manually obtained crystal sizes was used to optimise and identify problems within the image processing steps.Monitoring the peptide’s concentration gradient in crystallisers with volumes of up to 15 millilitres found that an increase in oscillatory shaking speed from 50 to 150 rpm leads to a reproduceable higher homogeneity of the peptide concentration within the crystalliser resulting in a higher consistency of the crystallisation processes. Additionally, higher shaking speed lead to a faster decrease in peptide concentration implying a reduce in induction time and increase in crystal growth rate. These results can be supported by investigation of the crystal size distribution, obtained with the evaluation-routine, showing that a shaking speed of 150 rpm results in a more uniform crystal size distribution with a smaller span whereas a shaking speed of 50 rpm results in a wider spread size distribution. Furthermore, the reproducibility of controlling nucleation with templating additives, such as amino acids, can be enhanced significantly when a faster shaking speed is utilised, leading to a more homogeneous distribution of the additives. Mounting the tubular crystalliser in different orientations to the mixing plane found that if the axial direction of the crystalliser is aligned to the mixing plane, higher mixing length scales can be achieved resulting in an increase of mixing efficiency while keeping the energy input constant.This study demonstrates the importance of mixing to achieve a high reproducibility of the crystallisation process in crystalliser volumes up to the tens of millilitres for peptides with low solubilities and molecular weights. Higher shaker speed enhances the mass transfer leading to a faster crystallisation process and a narrower crystal size distribution. To study the impact of mixing on the macro-scale crystal properties in-situ, hundreds of crystal images in combination with a novel developed evaluation-routine were evaluated. Bibliography Kastin, A.J., Handbook of Biologically Active Peptides. 2 ed. 2013: Academic Press. 2032. Guillemot-Potelle, C., et al., Cost of Goods Modeling and Quality by Design for Developing Cost-Effective Processes. BioPharm International, 2010. 23(6): p. 26-35. Yang, H., et al., Development and Workflow of a Continuous Protein Crystallization Process: A Case of Lysozyme. Crystal Growth & Design, 2019. 19(2): p. 983-991. Roque, A.C.A., et al., Anything but Conventional Chromatography Approaches in Bioseparation.Biotechnology Journal, 2020.",61,2.0
"Tangential flow filtration (TFF) process such as microfiltration and ultrafiltration has been widely used in the pharmaceutical industry for biologics to remove smaller molecular weight process waste from the proteins. Due to the nature of these membranes, this unit operation has been limited to aqueous systems and larger molecular weight assets. Recent advances in membrane technology have enabled the fabrication of membranes with significantly smaller pore sizes and composed of materials with markedly improved compatibility to organic solvents. These changes make TFF systems a useful alternative to precipitation, distillation and solvent exchange operations in synthetic chemistry steps in small molecules and peptide processes, and in particular for amorphous or thermally labile materials.This presentation will highlight the use of nanofiltration as part of a synthetic peptide process to remove small molecular weight by-products, to concentrate reaction solution at low temperatures and to perform a solvent exchange between chemical transformation steps.",61,3.0
"Purpose: Concurrent visualization of liquid transport, drug mobilization and release process through complex polymer structures is of great scientific significance and challenge in the development of advanced small molecule formulations. Such elucidation of the local, non-linear dynamics of drug release not only can support formulation design and optimization, but also reveals potential risk factors for drug stability that is influenced by local transient phenomena. This work discusses the advancement of quantitative multi-nuclear (1H and 19F) magnetic resonance imaging (MRI) and spectroscopic techniques, applied to revealing drug dissolution and mobilization dynamics inside a long-acting implant formulation in order to support formulation design and risk mitigation.Methods: Long-acting implant samples, consisted of an active drug, MK-A, and ethylene-vinyl acetate (EVA) copolymer, were placed in dissolution media under controlled conditions and imaged periodically with a 400 MHz NMR Bruker Avance spectrometer. At each time point, 1H NMR imaging (at 20 mm pixel resolution) and spectroscopy were used to observe water concentration and mobility;19F pulsed-field gradient (PFG) NMR spectroscopy was also used to observe drug mass transport, via its molecular self-diffusion (Dself), within the polymer matrix.Results: Clear signals of the dissolution media were observed via 1H MRI, showing the ingress of the media into the implant via a porous polymer structure created by dissolution of the drug. Analysis of the 1H images reveal a dissolution medium transport diffusivity, DT, of around 10-13 m2s-1.The pores are initially created as the drug dissolves into the ingressing dissolution media and thereafter provides a pathway for the drug to leave the implant. Concurrently, 19F NMR spectroscopy reveals that the solid drug dissolves into the media with a square root time dependence within the pores. Dself of the dissolution media inside the pores was found to be 2 orders of magnitude higher than the transport diffusivity, DT. 19F PFG-NMR measurements of the dissolved drug molecules within the porous structure exhibit a similar self-diffusivity as the dissolution media, suggesting the concurrent diffusion of these mobile species in the porous system. We tentatively assert that both the transport diffusivity, DT, of the dissolution media ingress into the polymer matrix and the self-diffusivity, Dself, of the drug are key descriptors that govern the overall drug release rate into the surrounding dissolution medium. The characterization of such drug mobility provides a mechanistic understanding of the slow drug release as a design feature of such implants. In addition, changes in the 19F signal from within the implant with extended media exposure are indicative of recrystallization of the drug, pointing to the potential stability risks that such a slow drug release can carry.Conclusions: The concurrent elucidation of the dynamics of both active drug and the dissolution media, made possible by advanced multi-nuclear NMR micro-imaging technology, provides unique perspectives of the local release dynamics inside the implant formulation, and greatly empowers formulation scientists to optimize the formulation design and manufacturing process based on detailed mechanistic understandings.",62,0.0
"The Pharmacy on Demand (PoD) project enables an end-to-end continuous manufacturing (CM) of API using four refrigerator-sized, portable units; synthesis, purification, crystallization and drug product. High-dose, poorly flowing API always represents a challenge for developing a robust continuous manufacturing (CM) process for solid dosage forms. We explored the nature of powder flow of Ciprofloxacin HCl and investigated multiple techniques in order to maximizing flow properties. Multiple feeders were studied, and an optimum design was selected to facilitate formulation of the targeted batch size of 1000 doses per day. Since our goal is to produce ciprofloxacin using a continuous process on a mobile platform, a simple formulation was implemented. It consisted of a filler, disintegrant, glidant and a lubricant. We investigated the effect of excipient types and levels on the formulation manufacturability and performance. Disintegration, in-vitro dissolution, tablet mass variability, hardness and assay were used as performance measures which are required to pass the United States Pharmacopeia (USP) standards. Moreover, the dissolution profiles for the tested formulations were compared to the reference listed drug. The chosen formulation passed the USP standards criteria for tablets friability <1%, % label claim (90-110%), and an in-vitro dissolution profile reaching 85% dissolved within 30 minutes. The selected formulation robustness and design space were tested by varying the input ciprofloxacin properties in terms of water content and particle size distribution (PSD) and evaluated by testing drug product (DP) performance. The selected formulation showed robustness across all the tested levels of variables and passed appearance and depicted great control of the module over online tablet masses within less than ±5% standard deviation of the target tablet mass (500 mg). In the near future, the developed formulation will be manufactured in a GMP facility for a clinical bioequivalence study required for an abbreviated New Drug Application (aNDA) filling.",62,1.0
"IntroductionThe search for new active pharmaceutical ingredients (API) has gone through an important development over the last few decades. Especially by using technologies like high throughput screening and combinatorial chemistry, improving solubility and thereby oral bioavailability of drugs in development has become a main challenge for pharmaceutical development [1]. About 40 % of the drugs in the market are considered to be poorly soluble, but regarding the substances in development, the percentage of BCS class II and IV drugs is expected to be much higher [2]. Therefore it is necessary to develop new formulation techniques to enhance solubility and thereby bioavailability of these drug substances.In this work submicron API particles are produced with ultrasonic atomization technique in a specially designed aerosol generator. As nanoparticular aerosols create a hazard to the environment, and frequently show reduced wettability, the generated particles are charged and deflected into a carrier melt in the melt electrostatic precipitator (MESP) [3]. Thereby the hazardous material is handed safely on the one hand. On the other hand the drug particles are embedded in a water soluble carrier and show an enhanced dissolution in water. The concept of the MESP and first experiments have already been discussed in previous work [3]. The aim of the present work is to characterize the MESP with a new designed aerosol generator in order to obtain a product with a higher drug load than it has been achieved before with a spray drying plant with aerosol conditioning [4]. Materials and MethodsMaterialsPhenytoin (Recordati Pharma GmbH, Ulm, Germany) was dissolved in acetone (Merck KGaA, Darmstadt, Germany) for the spray drying process. The sugar alcohol xylitol (Xylisorb 300, Roquette Pharma, Lestrem, France) was chosen as carrier matrix. Drug loads were determined by UV-Vis spectroscopy in a mixture of isopropyl alcohol (Carl Roth GmbH & Co. KG, Karlsruhe, Germany) and water (1:1). MethodsParticle size and shape were investigated with scanning electron microscopy (SEM) (H-S4500 FEG, Hitachi, Krefeld, Germany) and laser diffraction measurement (Spraytec, Malvern Panalytical, Malvern, UK) right after spray drying.The drug load in the xylitol melt was analyzed. The chilled and recrystallized melt was dissolved in a mixture of water and isopropyl alcohol (1:1). The phenytoin content was measured with UV-Vis (Jenway 7305, Stone, UK) at a wavelength of 212 nm. The loading time was kept constant at 5 min, while the applied voltage was varied above the corona inception voltage and below the corona breakdown voltage.Dissolution experiments were conducted in United States Pharmacopeia (USP) Apparatus 2 (DT 6, Erweka, Heusenstamm, Germany) in demineralized water of 37 °C at a stirrer speed of 50 rpm. The drug-laden xylitol was crushed and sieved with a mesh size of 400 µm. For comparison a physical mixture of xylitol and phenytoin was prepared using a turbula mixer (Turbula T10B, W.A. Bachofen AG, Muttenz, Switzerland).ResultsThe ProcessA spray drying plant for the production of submicron particles and the use of organic solvents was designed. A piezo crystal (WHQ 3005/1530-12N, Siansonic Technology, Bejing, China) operating at a high frequency of 3 MHz, was used to produce an aerosol of small, uniform droplets. The drug solution was pumped constantly out of a feed tank into the aerosol chamber in order to cover the piezo ceramic. The liquid level was kept constant throughout the nebulization process. To separate larger droplets out by a vortex and to transport small, gas-borne droplets upwards, carbon dioxide was let in tangentially. A particularly low volume flow was chosen, as the droplet size is independent of the volume flow, but increases the separation efficiency in the melt electrostatic precipitator (MESP). The droplets were transported upwards through a dip pipe into a drying stage, where the solvent was evaporated and removed from the aerosol by condensation. The particles were collected on a glass surface for investigation in scanning electron microscope (SEM) (Figure 1).The particles were transported into the separation stage, the MESP. The concept of the MESP has been described in a previous study [3]. It consists of two stages. In the first stage the entering particles were charged by applying high voltage of 5 to 9 kV in order to cause a corona discharge. In the second stage the particles were separated out of the gas stream by deflecting them towards the collecting electrode. The collecting electrode was covered with a melt of xylitol, so that drug particles were embedded (Figure 1) [3].Particles Size and ShapeThe spray dried API particles were characterized by scanning electron microscope (SEM) and laser diffraction measurement. One representative SEM picture is shown in Figure 2. The particles are rectangular or rhombic shaped and thereby indicate a crystalline state [5,6] (Figure 2a). Laser Diffraction measurements were conducted in three measurements and no relevant differences were seen. The particle size distribution is narrow, with a d50 of 300 nm (Figure 2b).Electrostatic PrecipitationThe drug load increased with increasing applied voltage hence the particles are charged more and can be separated more efficiently [7]. The drug load could be increased 25 times compared to a previous study [3] (Figure 3). The volume flow of the transporting gas was set at 3.6 L/h compared to 120 L/h. This led to a longer residence time of the particles in the loading area of the MESP and thereby to more efficient precipitation. The aim to increase the drug load could be achieved by the introduction of a new spray drying plant. Nevertheless the drug load is still rather low. During the process a particle layer on the inner walls of the MESP was built up. It can be explained by the phenomenon of back corona. Therefore, an optimization of the MESP is required.In vitro dissolutionIn vitro dissolution experiments were conducted with a solid dispersion with a high drug load and a physical mixture. The mean dissolution times (MDT) were calculated and indicated a significant (α=0.05) shorter mean dissolution time of the solid dispersion (MDT= 125 ± 12.5 s) compared to the physical mixture (MDT= 143 ± 6.9 s). Nevertheless a faster dissolution was expected, so that dissolution experiments with solid dispersions with a low drug load were conducted. A significant reduction of the mean dissolution time (MDT= 92 ± 7.9 s) was observed (Figure 4). It is assumed that agglomeration of the drug particles is a reason for this observation, since agglomeration tendency increases with a higher particle load.ConclusionIn this study a spray drying plant for the production of submicron particles was built and tested. The design was adapted to the requirements of a combined use with a previously designed Melt Electrostatic Precipitator. The gas volume flow could be set as low as it was needed for an efficient electrostatic separation without influencing the small size of the generated droplets. With this adaption of the spray drying section the aim to obtain higher drug loads in the product was achieved.The in vitro dissolution behavior was improved, nevertheless an influence of the drug load was observed. An agglomeration tendency with increasing drug load is assumed. Thereby further improvements concerning the MESP design are required in order to reduce particle agglomeration during the process.AcknowledgementsThe authors thank Volker Brandt (TU Dortmund University) for taking the SEM pictures and are grateful for the material support from Roquette Pharma. ReferencesLipinski, C. Drug-like properties and the causes of poor solubility and poor permeability. Journal of Pharmacological and Toxicological Methods 2000, 44, 235-249, doi:10.1016/S1056-8719(00)00107-6.Ku, M.; Dulin, W. A biopharmaceutical classification-based Right-First-Time formulation approach to reduce human pharmacokinetic variability and project cycle time from First-In-Human to clinical Proof-Of-Concept. Pharmaceutical Development and Technology 2012, 17, 285-302, doi:10.3109/10837450.2010.535826.Dobrowolski, A.; Pieloth, D.; Wiggers, H.; Thommes, M. Electrostatic Precipitation of Submicron Particles in a Molten Carrier. Pharmaceutics 2019, 11, doi:10.3390/pharmaceutics11060276.Strob, R.; Dobrowolski, A.; Schaldach, G.; Walzel, P.; Thommes, M. Preparation of spray dried submicron particles: Part A - Particle generation by aerosol conditioning. International Journal of Pharmaceutics 2018, 548, 423-430, doi:10.1016/j.ijpharm.2018.06.067.Muhrer, G.; Meier, U.; Fusaro, F.; Albano, S.; Mazzotti, M. Use of compressed gas precipitation to enhance the dissolution behavior of a poorly water-soluble drug: Generation of drug microparticles and drug-polymer solid dispersions. International Journal of Pharmaceutics 2006, 308, 69-83, doi:10.1016/j.ijpharm.2005.10.026.Nokhodchi, A.; Bolourtchian, N.; Dinarvand, R. Crystal modification of phenytoin using different solvents and crystallization conditions. International Journal of Pharmaceutics 2003, 250, 85-97, doi:10.1016/S0378-5173(02)00488-X.White, H.J. Particle Charging in Electrostatic Precipitation. Transactions of the American Institute of Electrical Engineers 1951, 70, 1186-1191, doi:<b _ngcontent-c10=""""> 10.1109/T-AIEE.1951.5060545.",62,2.0
"Despite numerous studies on drug delivery to the brain there still remain some questions on how can blood brain barrier be overcome in the process of drug transport, and how can drug be targeted and confine within a particular portion of the brain. Drug loaded paramagnetic particle seems to have potential to eliminate the challenges encountered during drug delivery to the central nervous system. Paramagnetic particles, under the effect of external magnetic field, can be used as carriers to effectively transport drugs through cerebrospinal fluid and deliver to the targets without being disturbed by the blood brain barrier and brownian motion. Since the in vivo experiments are expensive and difficult to conduct for such systems, a mechanistic model that can predict the dynamics and distribution of paramagnetic particles is necessary for the optimal design of drug delivery systems. The existing models are applicable to dilute solutions of paramagnetic particles and cannot be extended to concentrated solution, which is generally created near the magnetic targets. Here we present a framework to model magnetophoretic diffusion and convection of paramagnetic particles in a concentrated solution. An analytical solution to the magnetic field-driven transport of particles in 1-D and 3D were developed with application to drug delivery through cerebrospinal fluid. This solution gives rise to concentration profiles of the paramagnetic nanoparticle. The effect of activity coefficient was investigated. Invitro experimental study of paramagnetic nanoparticle transport in a tube was used to back up our model prediction. The result obtained from our model and experiment show that paramagnetic nanoparticle can be confined in a particular domain of the central nervous system (CNS) which will aid the delivery and targeting of drug to the domain.",62,3.0
"Intrathecal (IT) drug delivery can efficiently target regions in the central nervous system without hindrance of the blood brain barrier. This delivery mode is especially promising for novel therapeutics such as enzyme replacement, gene or antisense oligonucleotide (ASO) therapies for the treatment of neurodegenerative diseases including Alzheimer’s, Huntington’s disease, and stroke. However, the development of dosing guidelines for IT delivery requires intricate knowledge of the interplay between cerebrospinal fluid (CSF) dynamics, patient anatomy and pharmacokinetics of drug molecules. Fluid mechanical and chemical principles governing IT delivery are difficult to observe in vivo, and experimentation in small animals (mice, rats) does not translate to human applications. In order to fill the knowledge gap concerning IT biodispersion, we propose to use an anatomically accurate replica of the entire human central nervous system. This model faithfully matches spinal and cranial CSF spaces, and also operates with oscillatory CSF motion similar to physiological CSF flow conditions obtained with in vivo cine phase contrast MRI. The in vitro human CNS replica served to study CSF motions and mixing patterns that occur in acute and continuous infusion via a drug pump. The relationships between infusion volumes, injection impulse, catheter choices, and pump settings were systematically studied. Moreover, experimental data informed a novel distributed pharmacokinetic model to predict drug biodispersion for different infusion modes, drug properties, and subject specific physiology. A 1-D mathematical model was developed to replicate both the deformation and dispersion processes as experienced in the experiment, by solving continuity equation (for deformation) and transport equation (for dispersion). This mathematical model gives us a scalable method to predict the biodispersion of drug in the human spine with computer simulation. The presentation demonstrates a workflow to determine optimal infusion guidelines for novel drug leads based on anatomically accurate experimentation, optimal integration of animal data, and computer simulations using the developed mathematical model. The proposed experimental method avoids unnecessary animal experimentation, cuts costs and time effort needed to develop dosing guidelines for new drug leads, which potentially shortens time to market for new therapies. The proposed mathematical model enables a predictive method to quantify the effect of deformation and diffusion on the distribution of drugs.",62,4.0
"In pharmaceutical formulation development, microstructures, such as porosity and the distribution of active ingredient particles, play an increasingly important role. Three-dimensional microscopic imaging, specifically X-Ray Microscopy (XRM) and focused ion beam scanning electron microscopy (FIB-SEM), provides insight into these microstructures at unprecedented resolution. However, acquiring such images can be challenged by cost, time, and the availability of high-end imaging facilities and required expertise. Even when the images are acquired at desirable quality, there can be uncertainty that the acquired data is a representative elementary volume (REV).An image-based microstructure modeling framework will be presented in this talk. XRM images, FIB-SEM images, or correlative images using both, are processed with an artificial intelligence-based image analysis engine using supervised machine learning and generative adversarial networks. A matrix of quantitative descriptors such as volume fractions, particle size, particle dispersion, porosity, and transport properties are computed. A computer aided drug formulation (CADF) model can be numerically generated that matches these image-based descriptors. From the CADF model, full 3D microstructures corresponding to variants of the formulation can be numerically generated.Figure 1 demonstrates one example CADF model for an amorphous drug formulation. Two dimensional SEM images, Figure 1a, were acquired and segmented with a hybrid supervised machine learning and deep learning method (Figure 1b). Mathematical descriptors are extracted and divided into three groups. The first group is used as the generator input in the GAN model, and the second group as the discriminator input in the GAN model. The third group serves as the validation control group to drive the GAN model iteratively toward a microstructure model with desirable properties, Figure 1c. Once the model is validated, three dimensional microstructures of new formulations can be generated numerically. For example, Figure 1d-1f illustrates five new formulations with porosity 10%, 20%, 30%, 40% and 50%. A broader porosity range, at desirable intervals, can be covered. All quantitative descriptors can be computed and correlated with release behavior without making a new sample.The CADF model provides a numerical framework to study the sensitivity of these quantitative descriptors to drug formulation parameters (such as drug loading, particle size of the active pharmaceutical ingredients, milling conditions, particle orientation alignment, and more) and under various processing conditions (such as hot melt extrusion temperature, polymer properties, compaction force, and more). The numerical model allows a formulation scientist to rapidly traverse multi-variable parameter space, and narrow down to lead formulation parameters and optimal processing conditions. When in vitro release test or dissolution data is available, release simulations can then be conducted on the CADF model to validate, before applying the CADF model to iteratively predict release profiles under different parameters.The proposed framework has broad applications in formulation development and solid dosage forms, and in material science in general, where microstructures play a critical role.",62,5.0
"Oral solid dosage formulations require first to be dissolved in the gastrointestinal (GI) tract before being absorbed through the epithelial cell membrane. In vivo drug dissolution depends on the physiological variables in the GI tract, pH, residence time, intestinal motility, and drug properties. The pH-dependent dissolution profile of an ionizable drug may benefit from the manipulation of in vivo variables such as the environmental pH using pH-modifying agents incorporated into the dosage form. The dissolution of dipyridamole, a BCS class II (high permeability, low solubility) weak-base drug formulation in the presence of pH-modifier (tartaric acid) was investigated in this study. A comprehensive mass-transport model was developed that takes into account the effect of physical and chemical properties of drug and pH modifiers such as pKa, solubility, particle size distribution. Besides, the model considered dissolution media conditions such as the hydrodynamical parameters in the dissolution media (e.g., shear rate, convection, confinement, and diffusion) and the dynamic changes in the bulk pH due to the drug dissolution. Good agreement between the mass-transport analysis prediction and in vivo clinical data lends the credibility to apply this model for predicting the performance of a range of weak-base drugs with different solubilities, pKa, and particle size by considering the effect of gastric emptying rate and pH under different conditions. Furthermore, sensitivity analysis quantifies the impact of physiological parameters and drug-pH modifier properties. This analysis provides an improved procedure for formulation design using the pH-modifiers by minimizing the experimental iterations.",62,6.0
"Hydrates are co-crystalline materials containing water as one of the molecules in the crystal lattice. The incorporation of water into the crystal lattice produces a unit cell different from that of the anhydrate and, consequently, the physical properties of the hydrate can differ significantly from those of the anhydrate. The existence and stability of hydrates is an important consideration in the development of pharmaceutical products: the prevalence of water during manufacturing and storage can mean that neat forms of an active pharmaceutical ingredient can undergo a phase transition to hydrate form, impacting the effectiveness of the drug. Crystal structure prediction (CSP) methods can in principle be useful in identifying likely hydrates, by undertaking searches for all polymorphs of water and one or more given compounds for a given co-crystal stoichiometry. Minimal information is needed, typically just the chemical connectivity diagram1, to search for the low lattice energy arrangements of the constituent atoms in space.Applications of CSP to hydrates have resulted in mixed success so far. In the fifth blind test2 organised by Cambridge Crystallographic Data Centre, one of the targets was a hydrate but none of the 10 groups that attempted to predict its structure put forward the correct structure within their shortlist. In the sixth blind test3, only 8 groups submitted predicted structures for the hydrate target, and only one group generated the experimental structure within their shortlist.In order to gain a better understanding of the challenges that make CSP for hydrates difficult, we present a systematic evaluation of a CSP state-of-the-art method for organic hydrates, in which the lattice energy is partitioned into intramolecular and intermolecular contributions. Intramolecular interactions are modelled via quantum mechanical calculations4, and intermolecular interactions are divided into electrostatics, modelled using ab initio derived distributed multipoles5,6,7, and repulsion/dispersion interactions modelled using a semi empirical potential. A total number of 107 hydrates extracted from the Crystal Structure Database are minimized locally using the CrystalOptimizer algorithm8 with six models of different levels of sophistication (functional, basis set, use of continuum polarizability). The geometric differences between experimental structures and the corresponding minimization outputs are compared in terms of root mean-squared deviation and a CPU time required. Five of the six models are found to give a good degree of accuracy in more than 95% of cases, but with varying computational costs. A further assessment of the proposed models is undertaken by determining relative energy rankings, which are critical in generating reliable polymorphic energy landscapes. References:(1) Pantelides, C. C.; Adjiman, C. S.; Kazantsev, A. V. General Computational Algorithms for Ab Initio Crystal Structure Prediction for Organic Molecules. In Prediction and Calculation of Crystal Structures Methods and Applications. Topics in Current Chemistry; Atahan-Evrenk, S.; Aspuru-Guzik, A., Eds.; Springer, 2014; Vol. 345, pp. 25–58.(2) Bardwell, David A., Claire S. Adjiman, Yelena A. Arnautova, Ekaterina Bartashevich, Stephan XM Boerrigter, Doris E. Braun, Aurora J. Cruz-Cabeza et al. Towards crystal structure prediction of complex organic compounds–a report on the fifth blind test. Acta Crystallographica Sect. B: Struct. Sci. Cryst. Eng. Mater. 2011, 535-551.(3) Reilly, A. M.; Cooper, R. I.; Adjiman, C. S.; Bhattacharya, S.; Boese, A. D.; Brandenburg, J. G.; Bygrave, P. J.; Bylsma, R.; Campbell, J. E.; Car, R.; et al. Report on the Sixth Blind Test of Organic Crystal Structure Prediction Methods. Acta Crystallogr. Sect. B Struct. Sci. Cryst. Eng. Mater. 2016, 72, 439–459.(4) Gaussian 09, Revision A.02, M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A. Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A. Petersson, H. Nakatsuji, X. Li, M. Caricato, A. Marenich, J. Bloino, B. G. Janesko, R. Gomperts, B. Mennucci, H. P. Hratchian, J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-Young, F. Ding, F. Lipparini, F. Egidi, J. Goings, B. Peng, A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski, J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada, M. Ehara, K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima, Y. Honda, O. Kitao, H. Nakai, T. Vreven, K. Throssell, J. A. Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. Bearpark, J. J. Heyd, E. Brothers, K. N. Kudin, V. N. Staroverov, T. Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. Rendell, J. C. Burant, S. S. Iyengar, J. Tomasi, M. Cossi, J. M. Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L. Martin, K. Morokuma, O. Farkas, J. B. Foresman, and D. J. Fox, Gaussian, Inc., Wallingford CT, 2016.(5) Nyman, J.; Day, G. M. Static and Lattice Vibrational Energy Differences between Polymorphs. CrystEngComm 2015, 17, 5154–5165.(6) Price, S. L.; Leslie, M.; Welch, G. W. A.; Habgood, M.; Price, L. S.; Karamertzanis, P. G.; Day, G. M. Modelling Organic Crystal Structures Using Distributed Multipole and Polarizability-Based Model Intermolecular Potentials. Phys. Chem. Chem. Phys. 2010, 12, 8466.(7) Stone, A. J. Distributed Multipole Analysis, or How to Describe a Molecular Charge Distribution. Chem. Phys. Lett. 1981, 83, 233–239.(8) Kazantsev, A. V.; P. G. Karamertzanis; C. S. Adjiman; C. C. Pantelides. Efficient handling of molecular flexibility in lattice energy minimization of organic crystals. J. Chem. Theory Comput. 2011, 7, 1998-2016.",63,0.0
"Studies have demonstrated that inorganic salts can drive polymorphic selectivity of glycine from the metastable α-polymorph to the stable γ-polymorph. Here, we explore the effect of NaCl and CaCl2 together with volume reduction on glycine nucleation. The salts were added to supersaturated aqueous solutions of glycine at a 2:1 Gly:NaCl/CaCl2 ratio at volumes from 1000 μL to 50 μL. Analysis showed that a primary γ-glycine nucleation event occurs for Gly:NaCl systems, with observed increase in induction times as volume was decreased. However, β-glycine primary nucleation was also detected for Gly:NaCl systems. Interestingly, it was observed that β-glycine nucleation is preferred over the salt-directed γ-glycine nucleation at a certain volume. The increase in induction times combined with a decrease in probability for γ-glycine nucleation may have allowed preferred nucleation of the β-polymorph. On the other hand, no nucleation events were observed for Gly:CaCl2 systems at a time span of 48 hours. These results provide insight on how polymorph modifiers act in confined environments.",63,1.0
"Crystallization of organic materials as thin films is an area of interest in multiple fields, such as in the pharmaceutical industry, organic electronics and for the multiple applications afforded by metal organic frameworks (MOFs). Over the past decade, significant work has been accomplished in terms of understanding how polymorphism of organic molecules can be enhanced through crystallization as a thin film, through innovative techniques such as confinement, polymer additives, and control of deposition conditions such as the use of electric fields. A deeper understanding of how these polymorphs are formed is critically missing from literature.In this work, we use a mix of experimental and computational work to study the formation of thin film polymorphs of pharmaceutical compounds and organic semiconductors. The use of interface engineering can influence both the morphology and the polymorphism of acetaminophen, a pharmaceutical compound, and TIPS-pentacene, an organic semiconductor. We also show that changing the flow coating parameters can tune the nucleation and growth rates of organic semiconductors, which then impact the molecule polymorph, crystal size, and crystal orientation. In-situ X-ray diffraction data shows the changes in crystal growth conditions due to changing the fluid dynamics conditions and provides valuable insight in the mechanism of crystal nucleation. Controlling the interface precisely to obtain the desired polymorphs and crystal morphologies will be discussed as well.",63,2.0
"Spray coating and antisolvent addition have emerged as viable techniques that for use in the continuous processing of pharmaceutical molecules. Through controlling supersaturation during crystallization, these techniques are able to create particle distributions. In this work, meniscus guided flow coating is explored as an alternative to spray coating that creates high surface area crystals for use in pharmaceutical formulations. We have observed the formation of film morphology similar to that obtained using a spray coating technique, and unique from what has been previously described using a flow coating method. Acetaminophen is used as a model compound in this study, where films were fabricated and characterized using polarized optical microscopy and grazing incidence x-ray diffraction (GIXD) to determine particle sizes and crystal orientation on the substrate. Results suggest that the coating regime deviates from traditional evaporative or Landau-Levich coating regimes described in literature. Here we present our current hypothesis for the mechanism of crystallization that produces the observed particulate morphology.",63,3.0
"The thermodynamic stability of creatine phosphate sodium (CPS) in water–methanol system was investigated by the determination of CPS–water–methanol ternary phase diagram. The involved transformation behaviors were explored in solid-state, solutions, and vapor environment by VT-PXRD, Raman, FTIR spectroscopy and DVS. Results show that CPS can exist in three hydrate forms, namely, heptahydrates, tetrahydrates, and dihydrates, in the ternary phase diagram. These three crystal forms become the stable form in turn as the water content of the solvent mixture increases, with no anhydrate form appearing, which was seldom found in current reports. The transition points move towards higher water activity as the temperature increases. Besides, solution-mediated phase transformation could happen between tetrahydrates and dihydrates, which is influenced by solvent composition, temperature, and solid loading. While solid-state phase transformation of them couldn’t occur when heating. Heptahydrates and tetrahydrates could transform into each other through adsorption or desorption of the water vapor. The study provides a guide for the control of CPS hydrate forms in theory and in the production to obtain the commercial crystal form with high purity.",63,4.0
"Emerging digitization technologies within Industry 4.0 framework are bringing revolutionary transformations in the chemical manufacturing sector, including pharmaceutical manufacturing. Industry 4.0 in pharmaceutical manufacturing comprises efficient data collection and implementation strategies through the use of process digitization, Internet of Things (IoT), advanced data analytics using big data, machine learning, cyber-physical systems and cloud computing. These data-intensive strategies highlighted in Industry 4.0 are recently being applied towards the development of a digital twin framework [1,2]. For continuous pharmaceutical manufacturing, a data-centric digital-twin framework consists of a physical manufacturing plant inter-connected with its virtual, model-based representation, allowing seamless communication between the two entities. Within this framework, real-time data pertaining to process operation are recorded from the physical plant and sent to update the virtual model for advanced system analyses, and to update the plant operation when needed. For the efficient implementation of digital-twin framework, real-time modeling strategies concerning virtual model and plant updates need to be developed. The presented work focuses on developing these strategies, where the major aspects involve: 1) model maintenance based on real-time data obtained from plant operation; 2) algorithms for real-time system analyses; 3) determination of frequency criterion for model and plant updates. Model maintenance of the virtual plant is performed using model adaptation strategies, including recursive adaptation and ensemble-based methods [3,4]. Unlike commonly used methods where models are updated and maintained off-line, these strategies allow real-time model updates in the virtual plant during manufacturing plant operation. After developing real-time model updating strategies within the digital twin framework, the model updates are used for dynamic system analyses, like sensitivity and feasibility analysis [5-7], providing uninterrupted monitoring of process performance. Based on these analyses, corrective process improvements are suggested for real plant operation, in situations where, the current process settings can potentially lead to violations of drug’s quality specifications. Along with this capability of real-time update of virtual and real plant, frequency criterion for these updates need to be investigated for an efficient update strategy in the digital twin framework. These updates are generally performed at the same frequency associated with plant measurements [8] but can be modified based on a user-defined plant-model mismatch criterion [9] to reduce required computational resources. In this presentation, the results of such a framework for a continuous direct compaction line, together with Industry 4.0 will be illustrated. This will allow for a computationally efficient and robust implementation of digital-twin framework in pharmaceutical manufacturing for scientific, risk-based regulation compliant process operation. Reference:[1] M. Grieves, J. Vickers, Digital Twin: Mitigating Unpredictable, Undesirable Emergent Behavior in Complex Systems, in: Transdisciplinary Perspectives on Complex Systems, Springer, Cham, 2017: pp. 85–113.[2] A. Belhadi, K. Zkik, A. Cherrafi, S.M. Yusof, S. El Fezazi, Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies, Computers & Industrial Engineering. 137 (2019).[3] P. Kadlec, R. Grbic, B. Gabrys, Review of adaptation mechanisms for data-driven soft sensors, Computers & Chemical Engineering. 35 (2011) 1–24.[4] K. Hazama, M. Kano, Covariance-based locally weighted partial least squares for high-performance adaptive modeling, Chemometrics and Intelligent Laboratory Systems. 146 (2015) 55–62.[5] G. Bano, Z. Wang, P. Facco, F. Bezzo, M. Barolo, M. Ierapetritou, A novel and systematic approach to identify the design space of pharmaceutical processes, Computers & Chemical Engineering. 115 (2018) 309–322.[6] Z. Wang, M.S. Escotet-Espinoza, M. Ierapetritou, Process Analysis and optimization of continuous pharmaceutical manufacturing using flowsheet models, Computers and Chemical Engineering. 107 (2017) 77–91.[7] F. Boukouvala, V. Niotis, R. Ramachandran, F.J. Muzzio, M. Ierapetritou, An integrated approach for dynamic flowsheet modeling and sensitivity analysis of a continuous tablet manufacturing process, Computers & Chemical Engineering. 42 (2012) 30–47.[8] G. Bano, P. Facco, M. Ierapetritou, F. Bezzo, M. Barolo, Design space maintenance by online model adaptation in pharmaceutical manufacturing, Computers & Chemical Engineering. 127 (2019) 254–271.[9] A.S. Badwe, R.D. Gudi, R.S. Patwardhan, S.L. Shah, S.C. Patwardhan, Detection of model-plant mismatch in MPC applications, Journal of Process Control. 19 (2009) 1305–1313.",64,0.0
"The Unit Operations Laboratory at Rose-Hulman Institute of Technology recently underwent its digital transformation and the facilities are now being used in multiple courses as well as research projects. The fundamental shift to allowing open and efficient access to data has allowed this rich resource to influence not only the traditional Unit Operations lab course, but to supplement other core courses, serve as the backbone for our new Process Analytics course, and act as a hub for multidisciplinary research projects.",64,1.0
"This work provides an overview on the opportunities created when a developed Industry 4.0 infrastructure is applied to chemical engineering processes. Industry 4.0 defines a technology framework designed to revolutionize the world process industries. In this context, research groups have been exploring such a revolution by bringing to this environment improvements to link chemical engineering research, applications and teaching1,2,3. In particular, in this presentation, the cooperation between research groups at West Virginia University (WVU, USA) and Federal University of Campina Grande (UFCG, Brazil) in this area is described. Such cooperation aims to lead to a complete IIoT (Industrial Internet of Things) infrastructure connected to several dynamic process simulators (e.g., Aspen, CHEMCAD, SimCentral). With data availability from this infrastructure (at different data request points for a simulated process and in real time), Industry 4.0 aspects can be explored, such as: digital twin, machine learning, on-line data analysis, event frames and notifications, as well as topics directly linked to automatic and statistical process control, process optimization and economic analysis.The current infrastructure envisioned for analysis involves the use of a simulation platform that represents a chemical or manufacturing plant, data processing/storage and data request points from client devices and applications. The particularly investigated simulations include a subcritical coal-fired power plant model (from WVU), an industrial process from a large Latin America Refining Company (from UFCG), as well as chemical processes from the literature. Central to this infrastructure is a database to store real-time simulation data that represent plant measurements. The database used corresponds to the OSIsoft PI historian (which is part of the PI System)1. A Supervisory Control and Data Acquisition (SCADA) system is employed to remotely monitor and control the studied simulation through Open Platform Communications (OPC). In an analogy between the developed infrastructure and the Automation Pyramid, levels 0 (sensors and actuators) and 1 (unit automation) are represented by the dynamic simulations, level 2 (SCADA) provides the remote access to process variables and specifications by customized HMIs (Human Machine Interfaces), and level 3 (Plant Information Management System – PIMS) contextualizes the data from an asset standpoint allowing the management of analytics, event frames, notifications and system integrations with a tree view of the plant structure. The PIMS makes the data available for custom applications, thus interpreting the information and adding value to process related techniques such as predictive process control, optimization and economics analysis. Therefore, this infrastructure can provide a more realistic environment for investigating process automation and implementations under development, using desktop or web-based custom reports and dashboards for decision making at all different levels (operators, engineers or executives).This work thus explores aspects of Industry 4.0 such as Digital Twins, Simulation, System Integration and Data Analytics using a developed infrastructure at WVU and UFCG that can be used for operator training, research and engineering classes. Additionally, it corresponds to the starting point for future research on more robust data analytics, machine learning, big data and business intelligence studies.References1 E. Ruiz-Ramos, J. M. Romero-García, F. Espínola, I. Romero, V. Hernández and E. Castro, “Learning and researching based on local experience and simulation software for graduate and undergraduate courses in chemical and environmental engineering”, Education for Chemical Engineers, 21, pp 50 – 61, 2017;2 J. Uhlemann, R. Costa and J. Cl. Charpentier, “Product design and engineering — past, present, future trends in teaching, research and practices: academic and industry points of view”, Current Opinion in Chemical Engineering, 27, pp 10–21, 2020;3 M. Teles dos Santos, A. S. Vianna Jr. and G. A. C. Le Roux, “Programming skills in the industry 4.0: are chemical engineering students able to face new problems?”, Education for Chemical Engineers, 22, pp 69–76, 2018.4 PI Server. (2018 SP3). San Leandro, CA, USA. OSIsoft.",64,2.0
"Effective monitoring and control of a process require online measurements of all state variables of the process. Measurement of all state variables are typically not available due to the lack of reliable online sensors and/or high costs of online sensors. These unmeasured variables are often directly related to product properties, which are important to monitor and control. Their estimates are usually obtained using state estimators. As a result of the need for and the importance of state estimation, the development of accurate state estimators that are robust with respect to model uncertainties and unmeasured inputs, has been the subject of research for decades [1-5]. There have been a few reports on the design of large-scale state estimators [6-11]. Although the implementation of centralized estimators for large-scale systems seems to be an optimal approach, these estimators are not scaleable to complex, large-scale systems, as they suffer from the curse of dimensionality.In this paper, we address the problem of large-scale robust state-estimate prediction; that is, the prediction of future values of state estimates robustly at every time instant in large-scale processes. To this end, we decompose a large-scale state-estimate prediction problem into a set of smaller-scale state-estimate prediction problems so that the resulting set of the smaller-scale state-estimate predictors are more robust, easier to design and implement, and more computationally efficient. We also study tuning of the smaller-scale state-estimate predictors to maximize the robustness of the predictors set while ensuring the stability of the error dynamics of the entire set. In addition, we address the problem of implementing the smaller-scale state-estimate predictors in parallel (using a computer with parallel processors) to improve computational efficiency of the state estimate prediction while preserving the accuracy and robustness of state-estimate predictions.References[1] J.M. Ali, N.H. Hoang, M.A. Hussain, D. Dochain, Review and classification of recent observers applied in chemical process systems, Computers & Chemical Engineering, 76 (2015) 27-41.[2] A.K. Jana, A nonlinear exponential observer for a batch distillation, in: 2010 11th International Conference on Control Automation Robotics & Vision, IEEE, 2010, pp. 1393-1396.[3] M. Soroush, State and parameter estimations and their applications in process control, Computers & Chemical Engineering, 23 (1998) 229-245.[4] S. Tatiraju, M. Soroush, Nonlinear state estimation in a polymerization reactor, Industrial & engineering chemistry research, 36 (1997) 2679-2690.[5] N. Zambare, M. Soroush, B.A. Ogunnaike, Robustness improvement in multi-rate state estimation, in: Proceedings of the 2001 American Control Conference.(Cat. No. 01CH37148), IEEE, 2001, pp. 993-998.[6] N. Abdel-Jabbar, C. Kravaris, B. Carnahan, A partially decentralized state observer and its parallel computer implementation, Industrial & engineering chemistry research, 37 (1998) 2741-2760.[7] N. Abdel-Jabbar, C. Kravaris, B. Carnahan, Structural analysis and partitioning of dynamic process models for parallel state estimation, in: Proceedings of the 1998 American Control Conference. ACC (IEEE Cat. No. 98CH36207), IEEE, 1998, pp. 3170-3176.[8] J.B. Carvalho, F.M. Barbosa, Parallel and distributed processing in state estimation of power system energy, in: MELECON'98. 9th Mediterranean Electrotechnical Conference. Proceedings (Cat. No. 98CH36056), IEEE, 1998, pp. 969-973.[9] R. Ebrahimian, R. Baldick, State estimation distributed processing [for power systems], IEEE Transactions on Power Systems, 15 (2000) 1240-1246.[10] D.M. Falcao, F.F. Wu, L. Murphy, Parallel and distributed state estimation, IEEE Transactions on Power Systems, 10 (1995) 724-730.[11] D.B. Pourkargar, M. Moharir, A. Almansoori, P. Daoutidis, Distributed estimation and nonlinear model predictive control using community detection, Industrial & Engineering Chemistry Research, 58 (2019) 13495-13507.",64,3.0
"ICH introduced risk based approaches and enhanced scientific understanding as an opportunity to encourage continuous process improvement facilitate the introduction of newer technology. In addition, Quality by Design (QbD) promised to improve confidence in quality through the lifecycle of pharmaceutical products. A primary incentive for industry remains the prospect of global regulatory concordance for new applications and post approval changes, and ICHQ12 holds much promise. However, during the last decade, the industry has experienced a proliferation of regulatory divergence regarding the interpretation of ICH guidelines (and control strategies) across geographic regions, including “anxiety” for adoption and implementing ICHQ12. This presentation will illustrate some examples of divergence in control strategies while exploring opportunities for improved harmonization.",65,0.0
"There is significant pressure on the CMC team to complete the product development process in ever shorter timeframes. Several new molecules have had rapid clinical development programs that have shortened Phase 3 or have had no Phase 3 in the case of oncology therapeutics. Gone are the days when a multiyear Phase 3 clinical program allowed for the CMC team to complete the drug development cycle in a standard timeframe. CMC activities are being pushed forward in the development cycle. The CMC team is more reliant on company platform technology and Quality by Design (QbD) principles that can allow the CMC team to complete development faster and create a robust control strategy.The regulatory dossier must capture elements of the development process, it must show how our clinical drug product links to the commercial drug product and it must outline our control strategies for commercial manufacturing. This presentation will highlight the successes and challenges of rapid drug development and how our global regulatory dossier has evolved to deal with these complexities.  Kevin Fitzpatrick is an employee of AbbVie and may own AbbVie stock. AbbVie sponsored and funded the study; contributed to the design; participated in the collection, analysis, and interpretation of data, and in writing, reviewing, and approval of the final publication.",65,1.0
"Quality by Design (QbD) elements are an integral component of product development and regulatory submissions, enabling continual improvement throughout the product lifecycle. QbD can provide a systematic approach in which prior knowledge from platform technologies is leveraged to focus resources on understanding variability critical to a specific product and/ or process. Companies decide on the extent to which to invest in QbD based on the stage of development and product and/or process risk and expand the use of QbD principles during the development process. Design spaces can be established for processes initially identified as high risk as that knowledge is used to establish appropriate process and input material controls for commercial operations. Raw material monitoring plans that support continual improvement can support expanded material attribute ranges based on a robust understanding of their interaction and impact on product quality. Process analytical technology (PAT) is often deployed primarily during development to rapidly understand sensitivity to variability and establish appropriate controls that are subsequently used during production, however PAT has been successfully implemented globally for release, in limited cases. Regulatory flexibility as a result of QbD development and registrations has not been fully realized, however with continuous manufacturing this is expected to change. Design spaces across integrated unit operations can be established on commercial equipment without requiring extensive input materials and time. This provides a comprehensive understanding of the process sensitivities, the appropriateness of the control strategy and the justification for regulatory and operational flexibility. Furthermore, the shift towards performance-based approaches for analytical methods and models where the focus is on control of the output rather than the input can provide additional flexibility.The objective of this presentation is to provide examples of how QbD elements and PAT are used to develop a robust process and how continuous manufacturing can fully utilize QbD in development and commercial manufacture allowing for potential regulatory flexibility.",65,2.0
"QbD is based on a fundamental understanding of process performance and product quality, which can be derived from first principles modeling methods. Model development and use routinely suffers from peer-to-peer variability that undermines the credibility of model-based evidence. In this talk we discuss Amgen’s approach to model quality and modeling lifecycle management to enable model-informed decisions and promote model-based filing activities. Several case studies will be presented.",65,3.0
"This talk focuses on the lessons learned in implementing the QbD approach in DS regulatory filings for the Vertex CF portfolio ranging from Kalydeco (2012) to Trikafta (2019). Case studies are used to address the evolution of approaches taken for risk assessments, experiment design and analysis, nonlinear modeling, design space limits and other areas. These lessons learned are fueling further development as we prepare for future regulatory filings. Further discussion during the panel forum will be welcome.",65,4.0
"Quality by Design strategies in the area of API production largely focus on definition of design space for reactions for development and commercial processes followed by traditional isolation, sampling and off-line release. PAT monitoring utilized in development has generally been removed when processes enter the commercial manufacturing facilities or is used “for information only” purposes. Continuous, on-demand production strategies require the PAT monitoring and quality control of intermediate as well as crude API solutions in order to release materials for purification. In addition PAT strategies can be envisioned which could eventually remove some end-product testing for API, thus allowing on-demand release to accompany the on-demand production. The talk will focus on potential PAT and control strategy opportunities that will facilitate the eventual move toward RTRT of CQAs for API.",65,5.0
"Quality by Design can be defined as systematic approach to development that begins with predefined objectives and emphasizes product/process understanding and process control, based on sound science and quality risk management. Implementation of QbD approach leads to improved robustness of a firm’s quality system and better assurance of product quality. The US Food and Drug Administration (FDA) encourages risk-based approaches and the adoption of QbD principles in drug product development and manufacturing. This presentation will address impact of QbD primarily as an enabler that supports patient focused drug development, allows leveraging of flexible regulatory filing strategies based on enhanced product/process understanding and supports faster speed to market of novel products and/or manufacturing techniques. There will also be a discussion of how CDER is implementing a science and risk based approach for CMC assessment of regulatory submissions and for site inspections.",66,0.0
,66,1.0
